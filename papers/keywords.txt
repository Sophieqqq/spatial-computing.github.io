Automatically and Accurately Conflating  
Orthoimagery and Street map 

Ching-Chien Chen, Craig A. Knoblock, Cyrus Shahabi, Yao-Yi Chiang, and Snehal Thakkar 
University of Southern California 
Department of Computer Science and Information Sciences Institute 
Los Angeles, CA 90089-0781 
[chingchc, knoblock, shahabi, yaoyichi, snehalth] @usc.edu 

ABSTRACT 
Recent growth of the geospatial information on the web has made it possible to easily access various map and orthoimagery. By integrating these map and imagery, we can create intelligent images that combine the visual appeal and accuracy of imagery with the detailed attribution information often contained in diverse map. However, accurately integrating map and imagery from different data sources remains a challenging task. This is because spatial data obtained from various data sources may have different projections and different accuracy levels. Most of the existing algorithms only deal with vector to vector spatial data integration or require human intervention to accomplish imagery to map conflation. In this paper, we describe an information integration approach that utilizes common vector datasets as "glue" to automatically conflate imagery with street map. We present efficient techniques to automatically extract road intersections from imagery and map as control points. We also describe a specialized point pattern matching algorithm to align the two point sets and conflation techniques to align the imagery with map. We show that these automatic conflation techniques can automatically and accurately align map with images of the same area. In particular,  the approach described in this paper, our system automatically aligns a set of TIGER map for an area in El Segundo, CA to the corresponding orthoimagery with an average error of 8.35 meters per pixel. This is a significant improvement considering that simply combining the TIGER map with the corresponding imagery based on geographic coordinates provided by the sources results in error of 27 meters per pixel. 
Categories and Subject Descriptors 
H.2.8 [Database Management]: Database Applications . 
Spatial Databases and GIS 


General Terms 
Algorithms, Design 
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. To copy 
otherwise, or republish, to post on servers or to redistribute to lists, 
requires prior specific permission and/or a fee. 
GIS’04, November 12–13, 2004, Washington, DC., USA. 
Copyright 2004 ACM 1-58113-979-9/04/0011…$5.00. 


Keywords 
Conflation, orthoimagery, street map, point pattern matching 

1. INTRODUCTION 
There is a wide variety of geospatial data available on the Internet, including a number of data sources that provide imagery and map of various regions. The National Map1, ESRI Map Service2, MapQuest3, University of Texas Map Library4, Microsoft TerraService5, and Space Imaging6 are good examples of map or imagery repositories. In addition, a wide variety of map are available from various government agencies, such as property survey map and map of oil and natural gas fields. Satellite imagery and aerial photography have been utilized to enhance real estate listings, military intelligence applications, and other applications. Road vector data covering all of the United States is available from the U.S. Census Bureau7. By integrating these spatial datasets, one can support a rich set of queries that could not have been answered given any of these datasets in isolation. Furthermore, this integration would result in cost savings for many applications, such as county, city, and state planning, or integration of diverse datasets for emergency response. However, accurately integrating these geospatial data from different data sources remains a challenging task.  This is because spatial data obtained from various data sources may have different projections and different accuracy levels. If the geographic projections of these datasets are known, then they can be converted to the same geographic projections.  However, the geographic projection for a wide variety of geospatial data available on the Internet is not known. Consider the integration of imagery and map. Most online imagery (such as satellite imagery and aerial imagery) has been orthorectified (called orthoimagery,  this imagery is altered from original photos so that it has the geometric properties of a map). Moreover, online map are routinely revised  
1 http://seamless.usgs.gov 2 http://arcweb.esri.com/sc/viewer/index.html 3 http://www.mapquest.com 4 http://www.lib.utexas.edu/map/index.html 5 http://terraserver-usa.com/ 6 http://www.spaceimaging.com/ 7 http://www.census.gov/geo/www/tiger/ 


a) TIGER street map b) Satellite image c) Imagery with superimposed map ( the roads on imagery are represented as white lines) 
 1: The map-imagery integration without alignment 




 2: The map-imagery integration with alignment 
satellite imagery or aerial photographs these days. However, these map might align to imagery of a particular resolution and misalign to imagery that has different resolutions or different orthorectification parameters. The fact that many of the online map sources do not provide the geo-coordinates of the map makes the integration even more complicated. In previous work, we developed an approach to automatically conflating road vector data with imagery [7]. In this paper we describe how we address the even more challenging problem of automatically conflating street map ( map showing roads) with imagery. 
 1 shows an example of integrating a street map (geo.referenced US Census TIGER map with the scale 1:4269 which is queried from the TIGER Map Server8) and an image (geo.referenced USGS DOQ images with 2-meter per pixel resolution which is queried from Microsoft TerraService). The map is made semi-transparent with the underlying image. We can see that there are certain geospatial inconsistencies between the map and imagery. In this paper, we describe our approach to automatically and accurately aligning orthoimagery with the various online 
8 http://tiger.census.gov/cgi-bin/mapurfer 
street map to alleviate these inconsistencies. In addition, we can take map that have not been geo-referenced and automatically determine the geo-coordinates. By properly aligning imagery with map, we can annotate objects on imagery, such as roads, streets and parks, with the map. Consider the example shown in  
2. The user sees the imagery of unknown area nearby and notices a park in the imagery. However, the imagery does not provide street names, so the user cannot determine how to reach the park.  the techniques described in this paper, user can easily obtain an integrated view of the imagery with the map of the area, which would guide the user on how to reach the park. 
The traditional approach to aligning these various geospatial products is to use a technique called conflation [21], which requires identifying an appropriate set of counterpart features (termed control points) on the two data sources to be integrated. Other points will be moved according to the correspondence between the control point pairs. Various GIS researchers and computer vision researchers have shown that the intersection points on the road networks provide an accurate set of control point pairs [7, 11, 12, 14]. In addition, road networks are commonly illustrated on diverse map. The identification of these control points is often performed manually, which is a tedious and time-consuming process that is made even harder by the fact that many of the online sources do not even provide the coordinates of the corner points of the map.  We have developed an approach to automatically identify a set of control point pairs by combining different sources of information from each of the sources to be integrated. In particular, we utilize common vector datasets as “glue” to integrate imagery with map. We first identify feature points on imagery by utilizing some information inferred from vector dataset, and then we detect the same sort of feature points on map. Finally, we compute the alignment between the two point sets. Now that we have a set of control point pairs for the map and imagery, we can use the conflation technique described in [21] to align the map with the imagery. Our proposed approach facilitates the close integration of vector datasets, imagery and map, thus allowing the creation of intelligent images that combine the visual appeal and accuracy of imagery with the detailed attribution information often contained in diverse map. 

The remainder of this paper is organized as follows. Section 2 reviews our previous work on automatically detecting road intersection points in imagery. Section 3 illustrates the techniques to automatically find road intersection points in street map. Section 4 presents a specialized point pattern matching algorithm for finding the mapping between the layout (with relative distances) of the intersection points on the imagery and the map, respectively, to generate a set of control point pairs. Section 5 describes the idea of conflating map with imagery based on the detected control point pairs. Section 6 provides experimental results. Section 7 discusses the related work and Section 8 concludes the paper by discussing our future plans. 

2. 	PREVIOUS WORK: IDENTIFYING INTERSECTIONS ON IMAGERY 
Automatic extraction of road intersection points from imagery as feature points is a difficult task due to the complexity that characterizes natural scenes [1].  In order to efficiently and accurately detect road intersection points on imagery, we utilize existing road network vector databases as part of the prior knowledge. In general, integrating existing vector data as part of the spatial object recognition scheme is an effective approach. The vector data represents the existing prior knowledge about the data, thus reducing the uncertainty in identifying the spatial objects, such as road segments, in imagery. 
In [6, 7], we described several techniques for automatic conflation of road vector data with imagery.  The most effective technique we found exploits a combination of the knowledge of the road network with image processing in a technique that we call localized image processing.  With this approach, we first find road intersection points from the road vector dataset. For each intersection point, we then perform image processing in a localized area around the intersection point to find the corresponding point in the image. The running time for this approach is dramatically lower than traditional image processing techniques due to performing image processing on localized areas. Furthermore, exploiting the road direction and width information improves both the accuracy and efficiency of detecting edges in the image. An issue that arises is that the localized image processing may still identify incorrect intersection points, which introduces noise into the set of control point pairs. To address this issue, we utilized a filtering technique termed Vector-Median Filter [7] to eliminate inaccurate control point pairs. Once the system has identified an accurate set of control point pairs, we utilize rubber-sheeting techniques described in [21] to align the vector data with the imagery. With our test sets as described in [7], this approach produced an accurate alignment of the vector data with the imagery. 
More details about our previous work on vector-imagery conflation is provided in [7]. As a result the conflated intersection points on the road network can be aligned with the intersection points on the imagery. We can then use the conflated intersection points as intersection points on the imagery.  3 shows an example illustrating the detected intersection points on an image, before and after conflating the image with a road network. 


3. 	IDENTIFYING INTERSECTION POINTS ON STREET map 
Since there are few online street map with known geo.coordinates, we cannot apply the same localized image processing, described in Section 2, to find intersection points on map. This is because we cannot find the corresponding vector data for the map, since the map geo-coordinates are unknown. Hence, for those map whose geo-coordinates are unknown in advance, we utilize automatic map processing and pattern recognition algorithms described below to identify the intersection points on map. 
Ideally, intersection points on street map could be extracted by simply detecting road lines. However, due to the varying thickness 


a) Imagery with road network, before conflation b) Detected intersection points on imagery, after conflation 
 3: Intersection points automatically detected on imagery 

a) MapQuest map b) Detected intersection points 
 4: Intersection points detected on a map 

a) A map with some detected intersections b) An image (with detected intersections) covers left map 
 5: Intersection points detected on a map and an image 
of lines on diverse map, accurate extraction of intersection points from map is difficult [19, 23]. In addition, there is often noisy information, such as symbols and alphanumeric characters on the map, which make it even harder to accurately identify the intersection points. To overcome these problems, we adapted the automatic map processing algorithm described in [19] to skeletonize the map for extracting intersection points. The basic idea is to detect intersection points only on the map that has been pre-processed by line thinning algorithms and noise-removal procedures. In particular, the process can be divided into the following subtasks: (1) isolate map data by a threshold, (2) decrease line width by thinning algorithms, such as [3], (3) recognize intersection points by crossing number (CN), the number of lines emanating from an intersection point [3], (4) remove misidentified intersections caused by noisy information (such as symbols and text). The details of the line intersections detection algorithm are discussed in [19]. However, this algorithm assumes that the roads are illustrated as multiple single-lined segments on the map. Therefore, it is not appropriate for the map where roads are depicted as double lines. In particular, from our experiments with diverse single line online street map, this algorithm achieved 65% to 95% precision in identifying road intersections, while it worked poorly (with 20% to 30% precision) for double line street map.  
To alleviate this problem, instead of  “crossing number(CN)” (for an intersection, its CN must be greater than two) to detect intersections, we utilize feature-detection functions implemented in OpenCV9 to detect promising points, such as 
9 http://sourceforge.net/projects/opencvlibrary 
corners and distinct points. Then, a verification process is conducted to check whether there is any linear structure around each detected corner point. If so, the detected point will be characterized as an intersection point. We found that our revised approach can achieve 76% precision (on average) on our tested street map. 
 4 shows an example illustrating the detected intersection points on a map queried from MapQuest. Although our algorithm can significantly reduce the rate of misidentified intersection points on the map, it is still possible that both noisy points are detected as intersection points and some intersections be missed. For example, the point near the lower right corner (the “E” in “E Grand Ave”) was mistaken for a road. However, our point matching algorithm (described next) can tolerate the existence of misidentified intersection points. 


4. POINT PATTERN MATCHING 
So far we have identified a set of intersections on both the street map and the imagery.  5 shows an example of the two point sets on a map and an image, respectively. The remaining problem is to find the mapping between these points in order to generate a set of control point pairs. The problem of point pattern matching has at its core a geometric point sets matching problem. The basic idea is to find the transformation T between the layout (with relative distances) of the intersection point set M on the map and the intersection point set S on the imagery. The key computation of matching the two sets of points is calculating a proper transformation T, which is a 2D rigid motion (rotation and translation) with scaling. Because the majority of map and imagery are oriented such that north is up, we only compute the translation transformation with scaling. Without loss of generality, we consider how to compute the transformation where we map from a fraction . of the points on map to the points on imagery. The reason why only a fraction . of the points on the map is considered is that there are misidentified points arising from the processes of image recognition ( identifying intersection points on map). Moreover, there may be some missing intersection points on the imagery as well. 

The transformation T brings at least a fraction . of the points of M (on the map) into a subset of S (on the imagery). Symbolically, this implies:  
. T and M’ . M , such that T(M’) . S , where | M’ | ≥.| M | and T(M’) denotes the set of points that results from applying T to the points of M’. Or equivalently,  
for a 2D point (x, y) in the point set M’ . M, . T in the matrix case running time of O(|M|3 |S|2 log|S|). The advantage of this approach is that we can find a mapping (if the mapping exists) with a proper threshold ., even in the presence of very noisy data. However, it suffers from high computation time. One way to improve the efficiency of the algorithm is to utilize randomization in choosing the pair of points from M as proposed in [17], thus achieving the running time of  O(|S|2 |M| log|S|). However, their approach is not appropriate for our datasets because the extracted intersection points from map could include a number of misidentified intersection points. 
Assuming that map-scales are provided, we improve the (brute.force) point matching algorithm by exploiting information on direction and relative distances available from the vector sets and map. The information on direction and distance is used as prior knowledge to prune the search space of the possible mapping between the two datasets. More precisely, given a point pair (x1, 
y1) and (x2, y2) on M, we need  to only consider pairs (lon1, lat1) 
and (lon2, lat2) in S, such that the ground distance between (x1, y1) 

Sx 00
... ..
.

form
(Sx and Sy are scale factors along x and y
.. ..

0 Sy 0 Tx 
and (x2, y2) is close to the ground distance between (lon1, lat1) and 

Ty 1 
(lon2, lat2). The ground distance between (x1, y1) and (x2, y2) is 
calculated by multiplying their Euclidean distance by map scale.direction, respectively, while Tx and Ty are translation factors 

along x and y directions,  respectively), such that Furthermore, the orientations of  (x1, y1) and (x2, y2) should also 
be close to the orientations of (lon1, lat1) and (lon2, lat2). This enhanced algorithm runs in O(|M|3 |S|1.3 log|S|). 

.
Sx 00
.
[x, y, 1] *
= [longitude, latitude, 1] , where 

Tx | M’ | ≥.| M | and the 2D point (longitude, latitude) belongs to 
. . ..
. . ..

0 Sy 0 We can further improve the performance by transforming the 
point patterns on map and imagery to a 2D Euclidean space, 
Ty 1 
where the distance measurement is ground distance. The real 
world distance is used between points in the transformed space. 
Therefore, we only consider translation transformation without 
scaling in such space. In particular, the process (as shown in 
 6) can be divided into the following subtasks: (1) Consider 
the points on the map: choose one point P as origin (0,0), then 
determine the coordinates of other points Qi (Xi, Yi) as follows. Xi 
is the ground distance between P and Qi in east-west orientation, 
while Yi is the ground distance between P and Qi in north-south 
orientation. Note Xi is negative, if Qi is west to P. Yi is negative, if 
Qi  is south to P. (2) Repeat the similar transformation to the the intersection point set S on the imagery.  With this setting, we 

do not expect point coordinates to match exactly because of finite-
precision computation or small errors in the datasets. Therefore, 
when checking whether a 2D point p belongs to the point set S, 
we declare that p . S, if there exists a point in S that is within 
Euclidean distance . of p for a small fixed positive constant ., 
which controls the degree of inaccuracy. The minimum . such 
that there is a match of M’ into S is called Hausdorff distance. 
Different computations of the minimum Hausdorff distance have 
been studied in great depth in the computational geometry 
community [8]. We do not seek to minimize . but rather adopt an points on imagery. (3) Compare the two point patterns from these acceptable threshold for .. The threshold is small compared to the two transformed spaces: we now only consider the translation 
inter-point distances in S. In fact, this sort of problem was categorized as “Nearly Exact” point matching problem in [5]. 
Given the parameters . and ., to obtain a proper transformation T, we need to compute the values of the four unknown parameters Sx, Sy, Tx and Ty. This implies that at least four different equations are required. A straight forward (brute-force) method is first choosing a point pair (x1, y1) and (x2, y2) from M, Then, for every pair of distinct points (lon1, lat1) and (lon2, lat2) in S, the transformation T’ that map the point pair on M to the point pair on S is computed by solving the following four equations: 
Sx* x1 + Tx = lon1 Sy* y1 + Ty = lat1 
Sx* x2 + Tx = lon2 Sy* y2 + Ty = lat2 
Each transformation T’ thus generated is applied to the entire points in M to check whether there are more than .|M| points that can be aligned with some points on S within the threshold .. The above-mentioned process is repeated for each possible point pair from M, which implies that it could require examining O(|M|2) pairs in the worst case. Since for each such pair, we spend O(|S|2 |M| log|S|) time searching for a match, this method has a worst transformation T between the two transformed point patterns. The revised algorithm runs in O(|M|2 |S| log|S|) and works well in our experiments (see Section 6) even in the presence of very noisy data.  

5. IMAGE AND MAP CONFLATION  
Now that we have a set of control point pairs for the map and imagery, we can deform one of the datasets (the source image) to align the other (the target image) utilizing these identified control point pairs. Without loss of the generality, we assume that the map is the source image, while the orthoimage is the target image. 
To achieve overall alignment of an image and a map, the system must locally adjust the map to conform to the image. It is reasonable to align the two datasets based on local adjustments, because small changes in one area should not affect geometry at long distance.  To accomplish local adjustments, the system partitions the domain space into small pieces. Then, we apply local adjustments on each single piece.  Triangulation is an effective strategy to partition the domain space to define local adjustments. There are different triangulations for the control 


Nort
h 


T
r
ans
f
o
r
m
 poin
t
s 
t
o
 anot
her 
s
p
ace b
a
s
e
d
 on
 res
ol
u
t
i
o
n 




T
r
ans
f
orm
 p
o
int
s
 t
o
 a
n
ot
her 
s
p
ac
e
 bas
e
d on 
m
a
p-
s
c
ale
s 




 6: Enhanced point pattern matching process 

 7: Delaunay triangulation on imagery and a map,  identified intersections as control points 
points. One particular triangulation, the Delaunay triangulation, is especially suited for the conflation purpose [21]. A Delaunay triangulation is a triangulation of the point set with the property that no point falls in the interior of the circumcircle of any triangle (the circle passing through the three triangle vertices). The Delaunay triangulation maximizes the minimum angle of all the angles in the triangulation, thus avoiding triangles with extremely small angles. We perform the Delaunay triangulation with the set of control points on the map, and make a set of equivalent triangles with corresponding control points on the imagery. The details of the triangulation algorithms can be found in [4, 16]. 
Imagine stretching a map as if it was made of rubber. We deform the map algorithmically, forcing registration of control points on the map with their corresponding points on the imagery. This technique is called “Rubber sheeting” [25]. There are two steps for rubber sheeting. First, the transformation coefficients to map each Delaunay triangle on the map onto its corresponding triangle on the imagery are calculated. Second, for each pixel in each triangle on the imagery, we replace it semi-transparently with the corresponding pixel on the map by  the computed transformation coefficients. 
 7 shows an example of Delaunay triangulation, and the arrow illustrates that the pixel of the triangle on the imagery would be (semi-transparently) overlaid by the corresponding pixel on the map ( rubber-sheeting). In practice, if the conflation area ( the convex hull formed by control points ) of the source image is much larger than that of the target image, the rubber-sheeting results will be distorted because the sampling frequency is insufficient. We solve this problem by rescaling the conflation area on the map and imagery to identical sizes before applying triangulation and rubber-sheeting. 
 8 shows the overall approach for conflating imagery and map as described in Sections 2 through 5. First, we automatically conflate the road vector data with the orthoimagery to find the intersections in the image. Next, we find the road intersection points on the street map. Then, we utilize a specialized point pattern matching algorithm to align the two point sets and conflation techniques to align the imagery with map. 


6. EXPERIMENTS 
We utilized a set of online street map and imagery to evaluate our approach. The purpose of the integration experiment was to evaluate the utility of our algorithms in integrating real world data. We are interested in measuring the accuracy of the integration of map and imagery  our techniques. To that end, we performed several experiments to validate the hypothesis that  our techniques we can automatically and accurately align map and imagery. 


 8: Overall approach to align orthoimagery and street map Table 1: Tested datasets used in experiment 
Test data set 1 ( El Segundo, CA)  Test data set 2 (St. Louis, MO)  
Imagery  Geo-referenced USGS DOQ ortho.images with multiple resolutions  Geo-referenced USGS high resolution color ortho-images with 0.3m/pixel resolution  
map (with various sizes/scales)  15 MapQuest map, 15 TIGER map, 5 ESRI map  7 ESRI map, 6 MapQuest map, 5 Yahoo map10, 5 TIGER map, 4 Missouri census geographic base map11  
Vector data  U.S. Census TIGER/Lines12  NAVTEQ NAVSTREETS13  
Area covered  Latitude:33.9164 to 33.9301 Longitude:-118.4351 to -118.3702 Width: 5.2km; Height: 1.6km  Latitude: 38.5808 to 38.5951 Longitude: -90.4222 to -90.3883 Width: 3km; Height: 2km  

6.1 Experimental Setup 
Table 1 summarizes the datasets and test sites used for our experiments. For each type of map, the threshold ., a fixed constant used in point pattern matching routine, was determined by trying a few values and examining the results. We plan to determine the threshold by an iterative process in the future, which will dynamically change the threshold and try to find an acceptable threshold that can carry most of points on one dataset to the points on the other dataset. 
The experiment platform is Pentium III 700MHz processor with 256MB memory on Windows 2000 Professional (with .NET framework installed). We conducted the experiments as follows. We first obtained online orthoimages covering the experimental area and identified road intersection points on the image by utilizing some information inferred from vector dataset (as 
10 http://map.yahoo.com/ 
11 http://mcdc-map.missouri.edu/ (population density map with streets) 
12 http://www.census.gov/geo/www/tiger/ 
13 http://www.navteq.com/ 

described in Section 2 and the performance evaluation was illustrated in [7]). Then, we randomly downloaded various street map (with diverse sizes and map-scales) within this area from the Internet and extracted an intersection point set for each map. Finally, we computed the alignments between the point set on each map with the point set on the image and acquired the results as described in the following sub-section. 


6.2 Experimental Result 
We identified 281 intersection points on the image of test data set 1 (El Segundo, CA) and 240 intersections on the image of test data set 2 (St. Louis, MO). Because the tested map are in diverse sizes and scales, the number of points detected on each map is different. On average, there are about 60 points on each map and we achieved 76% precision (on average) for identifying road intersections on different map. Since the running time of our techniques is mainly dominated by the point matching routine, we used the running time of the point matching routine as the overall execution time (the query time for retrieving online images or map was not included). In addition, the running time of the point matching algorithm mainly depends on the number of road intersections on the map, not on the map sizes or map scales. 

Table 2: Percentage of the tested map whose point pattern aligns with the corresponding point pattern on the imagery 
Test data set 1 (El Segundo, CA)  Test data set 2 (St. Louis, MO)  
MapQuest map  TIGER map  ESRI map  MapQuest map  TIGER map  ESRI map  Yahoo map  MO census geographic base map  
Percentage  93.3%  86.7%  80%  83.3%  80%  71.4%  100%  100%  


 9: MapQuest map to imagery conflation (semi- 10: TIGER map to imagery conflation ( semi.transparent map) for El Segundo, CA transparent image) for El Segundo, CA 


We found that the average execution time for conflating a map with 60 detected intersection points (possibly with some misidentified points)  our geospatial point matching routine is about two minutes. 
For each category of map, the percentage of the tested map whose point pattern aligned with the corresponding point pattern on the imagery is shown in Table 2. On average, 87.1% of our tested map accurately aligned their intersection point set with the corresponding point pattern on the image. 12.9% of the map mis.aligned with the image. We noticed that this is because the roads on some of these mis-aligned map are in grid shape with similar block distances and some map cover a smaller area compared with other map. For example, the map available on MapQuest are in fixed dimensions. The covered area becomes smaller whenever one zooms in the area of his interest. Hence, there is no unique pattern in the points of such large-scale, small map. We can achieve higher accuracy by foc on larger map where there is more likely to be a unique pattern of points. 
We generated an accurate control point pair set for each map. Then, we used these control points to conflate the map with imagery. To demonstrate the accuracy of our conflation techniques, some results are shown in  9 to 12. As shown in these aligned images, we can annotate spatial objects (e.g., streets) on imagery with the attribution information contained in map. 
In addition, we also conducted a quantitative analysis to our conflation results. Towards that end, we randomly selected a set of TIGER map and imagery from both our test data sets. These selected map and imagery cover 14% of our tested area in El Segundo, CA and 50% of our tested area in St. Louis, MO, respectively. Furthermore, after applying our point pattern matching routine against the tested TIGER map and imagery, we accurately obtained aligned control point sets. The reason why we chose TIGER map is that the geographic coordinates are provided by the data source. Therefore, we can simply combine the TIGER map with the corresponding imagery based on geographic coordinates provided. The integration results were then compared with the conflation results by utilizing our approach. Our evaluation used all the road intersections in the map and measured the displacement of the road intersections to the corresponding road intersections in the imagery. The mean of the point displacements are used to evaluate the accuracy of the algorithms.  
The experimental results are listed in Table 3 and the displacement distributions of the intersections on map are shown 

Table 3: Comparison of the integration accuracy of conflated map with the original map 
Dataset  Original TIGER-map  Our conflated map  
Mean point displacement (meters) for test data set 1  27  8.35  
Mean point displacement (meters) for test data set 2  24  10.9  

in  13.  The X-axis of this  depicts the displacement between intersection on the map and the equivalent intersection on the image.  The displacement values are grouped every 5 meters.  The Y-axis shows the percentage of intersections that are within the displacement range represented by the X-axis. For example, as shown in  13(a), when utilizing our imagery-map conflation approach to the first test data set, 84% of the road intersections on our conflated map have less than 5 meters displacement from the corresponding imagery points. When simply combining original TIGER-map with imagery, we obtained 1.3% points within 5 meters displacement. Furthermore, original TIGER-map have about 93% points with more than 10 meters displacement, while our conflated map only have 2.8% points with larger than 10 meters displacement. In sum, as shown in Table 3, for the first test data set, we aligned the TIGER-map with an average error of 8.35 meters, which is three times better than the original TIGER-map. For the second data set, we improved the error 2.2 times over the original TIGER-map on high resolution imagery. 


7. RELATED WORK 
Geospatial data fusion has been one of central issues in GIS [24]. Geospatial data fusion requires that the various datasets be integrated (without any spatial inconsistencies), resulting in a single composite dataset from the integrated elements. Towards automatic geospatial data fusion, a vital step is automated geospatial data conflation to align multiple geospatial datasets. There have been a number of efforts to automatically accomplish vector to vector conflation [9, 21, 26] and vector to imagery (or map) conflation [2, 11, 15]. Our work significantly differs from the previous work in terms of our approach to conflate vector data with imagery. These differences are described in detail in [7]. Furthermore, there has been relatively little work on automatically conflating map with imagery. In [22], the authors describe how an edge detection process can be used to determine a set of features that can be used to conflate two image data sets. However, their work requires that the coordinates of both image data sets be known in advance.  Our work does not assume that coordinates for the map are known in advance, although we do assume that we know the general region. Dare and Dowman [10] proposed a feature-based registration technique to integrate two images. However, their approach requires users to manually select some initial control points. Some commercial GIS products, such as Able R2V14 and Intergraph I/RASC15 provide the functionality of conflating imagery and map ( raster to raster registration)  different types of transformation methods. However, these products do not provide automatic conflation, so users need to manually pick control points for conflation. 
14 http://www.ablesw.com/r2v/ 
15. http://imgs.intergraph.com/irasc/ 
Our automatic map to imagery conflation approach utilizes a specialized point pattern matching algorithm to find the corresponding control point pairs on both datasets. The geometric point set matching in two or higher dimensions is a well-studied family of problems with application to different areas such as computer vision, biology, and astronomy [8, 17]. Furthermore, the space partition and deformation techniques (e.g., triangulation and rubber-sheeting) are also used for image warping [13, 20]. 



8. CONCLUSION AND FUTURE WORK 
Given the huge amount of geospatial data now available, our ultimate goal is to be able to automatically integrate this data  the limited information available about each of the data sources. The main contribution of this paper is the design and implementation of a novel data fusion approach to automatically conflate street map with orthoimagery. We use common vector data as “glue” to integrate imagery with map. In particular, our approach utilizes the road intersections automatically identified on imagery and map (whose geo-coordinates are unknown in advance), and applies a specialized point matching algorithm to compute the alignment between the two point sets. Experimental results on the city of El Segundo, CA and the county of St. Louis, MO demonstrate that our approach leads to remarkably accurate alignments of map and imagery. The aligned map and imagery can then be used to make inferences that could not have been made from either the map or the imagery individually. 

We intend to extend our approach in several ways. First, we plan to further improve our geospatial point pattern matching, since we have noticed that there is a natural similarity between point pattern matching and string pattern matching, which is the problem of finding a match between a given pattern string and a test string. The main issue is how to efficiently convert the 2D geospatial points to 1D points without the impact of noisy points (e.g.,  Hilbert curve [18]). We also plan to enhance our intersection detection techniques used on map. We intend to use OCR-related techniques to extract textual information from the map in order to reduce the impact of these alphanumeric characters. In addition, these pre-extracted textual information (e.g., road names) can be used to label the detected intersections. Therefore, we can even further prune the search space of possible point pattern matchings by  these labeled intersections. An interesting direction with respect to integrating map is to be able to take arbitrary map with unknown geo-coordinates and determine their location anywhere within a city, state, country, or even the world. We already have road vector data covering most of the world, so the real challenge is developing a hierarchical approach to the point matching to make such a search tractable. 
9. ACKNOWLEDGEMENT 
This research has been funded in part by NSF grants EEC.9529152 (IMSC ERC), IIS-0238560 (CAREER), and IIS.0324955, in part by the Air Force Office of Scientific Research under grant numbers F49620-01-1-0053 and FA9550-04-1-0105, in part by a gift from the Microsoft Corporation., and in part by a grant from the US Geological Survey (USGS). 
10. REFERENCES 
[1] Aculair-Fortier, M.-F., Ziou, D., Armenakis, C., and Wang, S. Survey of Work on Road Extraction in Aerial and Satellite Images, Technical Report, Universite de Sherbrooke, 2000. 
[2] Agouris, P., Stefanidis, A., and Gyftakis, S. Differential Snakes for Change Detection in Road Segments, Photogrammetric Engineering & Remote Sensing, 67(12): p. 1391-1399, 2001. 
[3] Arcelli, C. and Sanniti di Baja, G. A width-independent fast thinning algorithm, IEEE Transaction on Pattern Analysis and Machine Intelligence: p. 463-474, 1985. 
[4] Berg, M.d., Kreveld, M.v., Overmars, M., and Schwarzkopf, O. Computational Geometry: Algorithms and Applications, Springer-Verlag, 1997. 
[5] Cardoze, D.E. and Schulman, L.J. Pattern Matching for Spatial Point Sets, In Proceedings of the IEEE Symposium on Foundations of Computer Science, 1998. 
[6] Chen, C.-C., Shahabi, C., and Knoblock, C.A. Utilizing Road Network Data for Automatic Identification of Road Intersections from High Resolution Color Orthoimagery, In Proceedings of the Second Workshop on Spatio-Temporal Database Management(STDBM'04), colocated with VLDB, Toronto, Canada, 2004. 
[7] Chen, C.-C., Thakkar, S., Knoblok, C.A., and Shahabi, C. Automatically Annotating and Integrating Spatial Datasets, In 
Proceedings of the International Symposium on Spatial and 
Temporal Databases, Santorini Island, Greece, 2003. 

[8] Chew, L.P., Goodrich, M.T., Huttenlocher, D.P., Kedem, K., Kleinberg, J.M., and Kravets, D. Geometric pattern matching under Euclidean motion, In Proceedings of the Fifth Canadian Conference on Computational Geometry, 1993. 
[9] Cobb, M., Chung, M.J., Miller, V., Foley, H.I., Petry, F.E., and Shaw, 
K.B. A Rule-Based Approach for the Conflation of Attributed Vector Data, GeoInformatica, 2(1): p. 7-35, 1998. 
[10] DARE, P. and DOWMAN, I. A new approach to automatic feature based registration of SAR and SPOT images, International Archives of Photogrammetry and Remote Sensing, 33(B2): p. 125-130, 2000. 
[11] Filin, S. and Doytsher, Y. A Linear Conflation Approach for the Integration of Photogrammetric Information and GIS Data, International archives of photogrammetry and remote sensing, 33: p. 282-288, 2000. 
[12] Flavie, M., Fortier, A., Ziou, D., Armenakis, C., and Wang, S. Automated Updating of Road Information from  Aerial Images, In Proceedings of American Society Photogrammetry and Remote Sensing Conference, 2000. 
[13] Goshtasby, A. Piecewise Linear Mapping Functions for Image Registration, Pattern Recognition, 19(6): p., 1986. 
[14] Habib, A., Uebbing, R., Asmamaw, A. Automatic Extraction of Road Intersections from Raster map, Technical Report, The Center for Mapping, The Ohio State University., 1999. 
[15] Hild, H. and Fritsch, D. Integration of vector data and satellite imagery for geocoding, International Archives of Photogrammetry and Remote Sensing, 32(Part 4): p. 246-251, 1998. 
[16] Hwang, J.-R., Oh, J.-H., and Li, K.-J. Query Transformation Method by Delaunay Triangulation for Multi-Source Distributed Spatial Database Systems, In Proceedings of the 9th ACM Symposium on Advances in Geographic Information Systems, Atlanta, GA, 2001. 
[17] Irani, S. and Raghavan, P. Combinatorial and experimental results for randomized point matching algorithms, Computational Geometry, 12(1-2): p. 17-31, 1999. 
[18] Mokbel, M.F., Aref, W.G., and Kamel, I. Performance of Multi-Dimensional Space-filling Curves, In Proceedings of the 10th ACM Symposium on Advances in Geographic Information Systems, McLean, VA, 2002. 
[19] Musavi, M.T., Shirvaikar, M.V., Ramanathan, E., and Nekovei, A.R. A Vision Based Method to Automate Map Processing, Pattern Recognition, 21(4): p. 319-326, 1988. 
[20] Ruprecht, D. and Muller, H. Deformed Cross-Dissolves for Image Interpolation in Scientific Visualization, The Journal of Visualization and Computer Animation: p., 1994. 
[21] Saalfeld, A. Conflation: Automated Map Compilation, Ph.D. Dissertation, Computer Vision Laboratory, Center for Automation Research, University of Maryland, 1993. 
[22] Sato, T., Sadahiro, Y., and Okabe, A. A Computational Procedure for Making Seamless Map Sheets, Center for Spatial Information Sciences, University of Tokyo, 2001. 
[23] Sebok, T.J., Roemer, L.E., and Malindzak, J., G.S. An Algorithm for Line Intersection Identification, Pattern Recognition, 13(2): p. 159.166, 1981. 
[24] Usery, E.L., Finn, M.P., and Starbuck, M. Data Integration of Layers and Features for The National Map, In Proceedings of American Congress on Surveying and Mapping, Phoenix, AZ, 2003. 
[25] White, M.S. and Griffin, P. Piecewise Linear Rubber-Sheet Map Transformation, The American Cartographer, 12(2): p.123-131,1985. 
[26] Yuan, S. and Tao, C. Development of Conflation Components., In Proceedings of Geoinformatics, Ann Arbor, Michigan, USA, 1999. 




Automatic Extraction of Road Intersections from Raster map 
Yao-Yi Chiang, Craig A. Knoblock, and Ching-Chien Chen 
University of Southern California 
Department of Computer Science and Information Sciences Institute 
Los Angeles, CA 90089-0781 

[yaoyichi, knoblock] @isi.edu, chingchc@usc.edu 
ABSTRACT 
Numerous raster map are available on the Internet, but the geographic coordinates of the map are often unknown. In order to determine the precise location of a raster map, we exploit the fact that the layout of the road intersections within a certain area can be used to determine the map’s lo.cation. In this paper, we describe an approach to automat.ically extract road intersections from arbitrary raster map. Identifying the road intersections is di.cult because raster map typically contain multiple layers that represent roads, buildings, symbols, street names, or even contour lines, and the road layer needs to be automatically separated from other layers before road intersections can be extracted. We combine a variety of image processing and graphics recogni.tion methods to automatically eliminate the otherlayers and then extract the roadintersectionpoints. Duringthe extrac.tionprocess, wedetermine theintersection connectivity( number of roads that meet at an intersection) and the road orientations. This information helps in matching the ex.tracted intersections with intersections from known sources (e.g., vector data or satellite imagery). For the problem of road intersection extraction, we applied the techniques to a set of48 randomly selected raster mapfrom various sources and achieved over90%precisionwith over75% recall. These results are su.cient to automatically align raster map with other geographic sources, which makes it possible to deter.mine the precise coverage and scale of the raster map. 
Categories and Subject Descriptors 
H.2.8[Database Management]: DatabaseApplications— 
Spatial Databases and GIS 

General Terms 
Algorithms, Design 

Keywords 
Raster map, road extraction, road intersection, imagery, conﬂation 
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. GIS’05 November 4-5th, 2005, Bremen, Germany Copyright 2005 ACM 0-12345-67-8/90/01 ...$5.00. 
1. INTRODUCTION 
Due to the popularity of Geographic Information System (GIS) and high quality scanners, we can now obtain more and more raster map from various sources on the Inter.net, such as digitally scanned USGS Topographic map on Microsoft Terraserver,1 or computer generated TIGER/Line mapfromU.SCensusBureau,2 ESRImap,3 Yahoomap,4 MapQuestmap,5 etc. To utilize these raster map, we need toknowthegeospatial coordinates of the map. Thefactis, however, only a few map sources provide this information. In addition, among the sources that provide the geospatial coordinates of the map, only a few embed the information in the raster map  the geo-ti. format, while others include it on the companion web pages or in separate ﬁles, which may result in the loss of this information if the raster map and thegeospatial coordinates areprovided separately. 
If the map sources do not provide geospatial coordinates or the information is missing, we need an alternative way to identify the coordinate of raster map. Considering the fact that the road layers are commonly used on various raster map andthe road networks are usuallydistinguishablefrom each other, we can use the road intersection points as the “ﬁngerprint” of the raster map. By matching a set of road intersection points of a raster map covering an unknown area to another set of road intersection points for which the geospatial coordinates are known, we can identify the coverage of the unknown raster map. 
Inourpreviouswork[5], wedescribed anautomatic and accurate map conﬂation method to integrate raster map with orthoimagery. We combine the information on raster map with accurate, up-to-date imagery by matching the correspondingfeatures( roadintersectionpoint sets)be.tween raster map and imagery. In that work, we developed a simple approach todetectintersectionsfrom simpler raster map, and we only used thepositions of the roadintersection pointsinthematchingprocessduringtheconﬂation. Inthis paper, wepresentamoregeneral approach tohandlediverse and more complicated map(e.g., USGSTopographicmap, Thomas Brother map). We achieve higher precision/recall and also e.ectively compute the intersection connectivity and the road orientations to help a conﬂation system to prune the search space during the matching process. 
1 http://terraserver-usa.com/ 2 http://tiger.census.gov/cgi-bin/mapurfer 3 http://arcweb.esri.com/sc/viewer/index.html 4 http://map.yahoo.com 5 http://www.mapquest.com 


(a)USGS Topographic Map (b)TIGER/Line Map 
 1: The raster map that use double lines and single lines to represent roadlayers(ElSegundo,CA) 
Raster map in di.erent scales are composed from multi.ple layers which typically contain information about roads, buildings, symbols, and characters. In some cases, usually for computergenerated raster map, roadlayers canbe sep.arated by extracting pixel with user speciﬁed colors or col.ors learned from the legend information. There are still a huge number of raster map, however, for which we cannot separate road layers  speciﬁed color thresholds (e.g., low-quality scanned map, USGS Topographic map). To overcome this problem, we ﬁrst utilize an automatic seg.mentation algorithm to remove background pixel based on the di.erence in the luminosity level. After we obtain the foregroundpixel, which contain the entireinformation layer of the original raster map, we separate the road layer from other layers to extract the road intersection points. 
Roadlayers are usuallypresentedin single-line ordouble.line format depending on map sources as shown in  1. The double-line format provides us more information to ex.tracttheroadlayersthanthesingle-lineformat, whichisim.portantif theinput raster maphas otherlayers that contain mostly linear structures, such asgridlines, rivers orcontour lines. We automatically examine the format of the road layer on the input raster map, and detect the road width if the road layer is in double-line format to trace the parallel pattern of the road lines. Then we apply text/graphics sep.aration algorithms with morphological operators to remove noise and rebuild the road layer. 
With the extracted road layer, we detect salient points as the road intersection candidates based on the variation of luminosity level around each road layer pixel. The connec.tivity of each salientpointisthen computedby checking the neighborhoodlinearstructuresontheroadlayer( road lines) to determine if it is an actual road intersection point. We also compute the orientation of the roads intersecting at each road intersection point as a by-product. 
The remainder of this paper is organized as follows. Sec.tion 2 describes our approach to extract road intersections. Section 3 reports on our experimental results. Section 4 dis.cusses the related work andSection5presents the conclusion and future work. 

2. 	AUTOMATIC ROAD INTERSECTION DETECTION 
The overall approach we aretaking inthispaperis shown in  2. The input can be any raster map regardless of resolution from various sources and without any prior knowledge such as color of layers, vector data, legend types orgazetteerdata[6]. The outputs arethepositions of road intersection points as well as the connectivity and the ori.entation of each intersected road. 
Raster map usually contain many objects, such as char.acters,buildings, streets, rivers or even contourlinesin topo.graphic map, anditisimportant thatwe candistinguishbe.tween these objects and the background. We classify the in.put raster map into two major categories depending on the way they were generated. The ﬁrst category includes com.puter generated raster map from vector data, such as the TIGER/Line map, and the other includes scanned raster map, such as USGS Topographic map. 
Computer generated raster map sources usually use dif.ferent colors to represent di.erent information layers, espe.cially road layers. Thus road layers can be extracted  a speciﬁed color threshold. Di.erent raster map sources, how-ever,requiredi.erent colorthresholds, and evenrastermap fromthesame sourcemay havedi.erent colorthresholdsfor di.erent scales. The scale or the source of the input raster map are unknown in our automatic approach, and we are not able to use only a color threshold to separate the road layers. On the other hand, scanned raster map su.er from quantization errors resulting from the manual scan process 
[11] and the color of each layer may vary from tile to tile. For example, in the USGS Topographic map some roads are composed of brown, white and black pixel and others are composed of pink, brown, and black pixel. 
Since the color threshold is not a reliable property to ex.tract the road layer, we use the di.erences in luminosity level to remove background pixel in the ﬁrst module, and use the geometry properties of road lines to separate road layers from others among foreground pixel in the second module. The last module detects salient points and deter.mines which salientpoints shouldbeidentiﬁed as roadinter.section points by counting the connectivity of each salient point along with the orientation of each intersected road. 
2.1 Automatic Segmentation 
In order to automatically separate the foreground with.out introducing additional noise, we use a common tech.nique called segmentation. We ﬁrst discard color informa.tion by converting the original input raster map to 8 bit grayscale with 256 color levels. Then we use the luminos.ity as a clue to automatically generate a threshold by the Triangle method proposed by Zack et al.[17]. The segmen.tation uses the threshold to segment the foreground pixel and background pixel. The grayscale and binary images are shown in  3.a and  3.b. 


2.2 Pre-Processing -Extracting Road Layers 
This module receives the binary raster map that contains multiple information layers as input and outputs the road layer. Road layers on raster map typically have two distin.guishable geometric properties from other layers: 
1. 
Road lines are straight within a small distance ( several meters in a street block). 

2. 
Unlikelabellayersorbuildinglayers,which couldhave many small connected objects; roadlines are connected to each other as road networks and road layers usually havefew connected objects or even only onehuge con.nected object -the whole road layer. 


Some map sources use double lines to represent roads, like Yahoo map, ESRI map, or USGS Topographic map, while others use single lines. Double-line format is com.




Binary Map Images 










Road Intersection Points with Connectivity and Orientation 
 2: The overall approach to extract road intersections 


(b)Binary map 

 3: Raster map before and after automatic segmen-tation(USGSTopographicMap,St. Louis,MO) 
monly used when the resolution is high or the map con.tain other linear objects such as contour lines. After au.tomatically checking the format of the road layer, we use parallel-pattern tracing to eliminate linear structures other than road lines if the map uses double-line format. The text/graphics separation program then removes small con.nected objects with part of the road lines that touch the re.moved objects, and the morphological operators reconnect the broken road lines. 

V  


H  C  H  


V  



H  C  H  


(a)RW =3 (b)RW =4 
 4: Double-line format checking and parallel-pattern tracing(Cisthetargetforegroundpixel. Visthepixel at the vertical direction and H is at the horizontal direction. Black cells are foreground pixel.) 

2.2.1 	Double-Line Format Checking and Parallel-Pattern Tracing 
To determine whether a target foreground pixel is on a double-line road layer with a road width of RW pixel, we search for the corresponding foreground pixel at a distance of RW in horizontal and vertical directions. If the target pixel is on a horizontal or vertical road line, we can ﬁnd two foreground pixel along the orientation of the road line within a distance of RW and at least another foreground pixel onthe correspondingparallel roadlinein adistance of RW, as shown in  4.a. If the orientation of the road line is neither horizontal nor vertical, we can ﬁnd one fore.groundpixel on each of thehorizontal and verticaldirection on the correspondingparallel roadlines at adistance ofRW, as shown in  4.b. 
There are some exceptions,however, as shownin5; foreground pixel from 1 to 8 are the example pixel which have the aboveproperties(graypixel are the corresponding pixel in horizontal/vertical direction or on the correspond.ing parallel road line of these pixel) and foreground pixel fromAtoDare the examplepixel thatbelong to thedouble road line layer but do not have the above properties. After the parallel-pattern tracing, pixel A to D will be removed resulting in small gaps between line segments. These gaps 5: The exceptionsin double-lineformat checking and parallel-pattern tracing(white cells arebackgroundpixel) 

A B 3 2 1  3 3 2 3 2 1 2 1  4  3  4 4  4  5  C  D  5 5  6  6 6  6  7  8 8  7 7  8  



Remaining Foreground pixel Ratio
1 
0.8 
0.6 
0.4 
0.2 0 

(a)Full-size view withinterestareahighlightedby the black rectangle 

(a) The raster map sources with double-line format road layers 


(b)Detail view 

1 2 3 4 5 6 7 8 910 
7: USGSTopographicMap before and afterparallel-
Width (pixel) 
pattern tracing(El Segundo, CA) 

(b) The raster map sources with single-line format road layers 
 6: Double-line format checking 
will be ﬁxed later  the morphological operators. Al.though we use single pixel-wide road lines in  4 and 5 for simpliﬁcation, road lines which are multiple pixel wide are also suitable for parallel-pattern tracing. To utilize the parallel-pattern tracing component, we need to know the format of the input road layer and the road width (RW). We check theroadlayerformatby applyingparallel-pattern tracing on the input raster map varying the road width from 0 to 10 pixel and remove foreground pixel which do not have the properties to be a road line pixel for a given road width. Then we compute the ratio of the remaining foreground pixel divided by the original foreground pixel for each road width as shown in  6. 
At the beginning of double-line format checking process, we set the road width as 0 pixel and no foreground pixel is removed. After increasing road width, the ratio starts to decline. This is because foreground pixel tend to be near each other, and it is easier to ﬁnd corresponding pixel even if theroad widthis not correct oritisnot adouble-linemap when the given road width is small. If the input raster map has a double-line road layer with the correct road width, thereis apeak ontheline of the chartin6.abecause the majority of foreground pixel on the double-line road layer have corresponding foreground pixel. ESRI map, MapQuest map, Yahoo map in high resolution and all of the USGS Topographic map are double-line map that have a peak on the line as shown in  6.a. ESRI map and MapQuest map, which are not high resolution, and all of the TIGER/Line map are all single-line map, which do not have any peak as shown in  6.b.  this method, we can detect double-line map automatically and also obtain the road width by searching for the peak. For example, from  6.a, we know the USGS Topographic Map is adouble-line map with road width equal to4pixel. Hence, we apply the parallel tracing algorithm setting RW to 4 pixel. The resulting image of this step is shown in 7 and theremainingpixelaremainly roadlineswith some broken characters. The contour lines and other linear structures are all removed. 

2.2.2 Text/Graphics separation 
After we usetheparallel-pattern tracingalgorithm to elim.inate other layers which contain linear structures, the re.(a)Binary TIGER/Line map 



(b)After text/graphics separation 
 8: TIGER/Line map before and after text/graphics separation(St. Louis,MO) 
maining major sources of noise are the small connected ob.jects, e.g. buildings, symbols, characters, etc. The small connected objects tend to be near each other on the raster map, such ascharactersthat areclosetoeach othertoform a string, andbuildings that are close to each other on a street block. The text/graphics separation algorithms in pattern recognition[2,3,7,9,15,16] are very suitableforgrouping these small connected objects. These text/graphics sepa.ration algorithms start by identifying every small connected foreground object, and then use various algorithms to search neighborhood objectsin ordertobuild an objectgroup[15]. We apply the algorithm described in Cao et al.[3] and the result is shown in  8. The broken road lines are in.evitable after the removal of those objects touching thelines, and we can reconnect them later  the morphological operators described next. 

2.2.3 	Morphological Operators: Generalized Dila.tion, Generalized Erosion and Thinning 
Morphological operators are implemented  hit-or.miss transformations [10], and the hit-or-miss transforma.tion is performed in our approach as follows: We use 3-by-3 binary masks to scan over the input binary images. If the masks match the underneath pixel, it is a “hit,” and if the masksdoesnotmatch theunderneathpixel,itisa “miss.” Each of the operators uses di.erent masks to perform hit.or-miss transformations and performs di.erent actions as a result of a “hit” or “miss.” We brieﬂy describe each oper.ator in the following paragraphs and the resulting images after each operator are shown in  9. 

(a)After generalized dilation operator 


 9: The resulting images from morphological opera.tors. The input is shown in  8.b 
The e.ect of a generalized dilation operator is expanding the region of foreground pixel [10]. We use it to thicken the road lines and reconnect the neighbor pixel. As shown in  10.a, if a background pixel has a foreground pixel in any ofits eight neighborpixel( a “hit”),it willbe ﬁlled up as aforegroundpixel(theaction resulting from the “hit”). The resulting image after performing three iter.ations of the generalized dilation operator on  8.b is shown in  9.a. The number of iterations determines the maximum size of gaps we want to ﬁx. The gaps smaller than6pixel are now reconnected and roadlines are thicker. 
Theidea of ageneralized erosion operatoristo reducethe region offoregroundpixel[10]. We useit to thin the road lines and maintain the orientation similar to the original ori.entation prior to applying the morphological operators. If a foreground pixel has a background pixel in any of its eight neighbor pixel ( a “hit”), it will be erased as a back.(a)The generalized dilation operator 


2  3  5  
1  4  


 11: The salient points (black cells are foreground 




pixel) 


(b)The generalized erosion operator 

(c)The thinning operator 

10:Themorphological operators(black cellsarefore.ground pixel) 
ground pixel ( the action resulting from the “hit”) as shown in  10.b. The resulting image after performing two iterations of the generalized erosion operator on Fig.ure 9.a is shown in  9.b. The road lines are thinner and the orientation is similar to the original. 
After applying the generalized dilation and erosion oper.ators, we have road layers composed from road lines with di.erent width, but we need the road lines to have exactly onepixel width todetect salientpoints and theconnectivity in the next module. The thinning operator can produce the one pixel width results as shown in  10.c. The idea of  the generalized erosion operator before the thinning operatorisbecausethegeneralized erosion operatorhasthe opposite e.ect to the generalized dilation operator, which can prevent the orientation of road lines from being dis.torted by the thinning operator. The thinning operators are conditional erosion operators which have an extra conﬁrma.tion step. After we mark all possible foreground pixel to beconverted tobackgroundpixelinthe ﬁrststep,thecon.ﬁrmation step utilizes the conditional masks to determine whichpixel among the candidatepixel shouldbe converted tobackgroundpixelto ensurethe conversion will not com.promise the basic structure of the original objects. The re.sulting image with the extracted road layers after applying the thinning operator on  9.b is shown in  9.c. 


2.3 	Detection of Road Intersection Candidates 
After eliminating the layers other than the road layer, we need to locate possible road intersection points. A salient point is a point at which more than one line segment meets with di.erent tangents, which is the basic requirement of a road intersection point. Among the image processing oper.ators, the interest operator is most suitable to detect the salient points such as corners or intersections on the input 

(a) 2-lineconnectivity,not (b) 3-line connectivity, an an intersection point intersection point 
 12: Use 11-by-11 rectangles to get connectivity for road intersection ﬁltering. C is the road intersection candi.datedetectedby theinterest operator(black cells arefore.ground pixel). 
road layer. We use the interest operator proposed by Shi andTomasi[13] andimplementedinOpenCV6 to ﬁnd the salient points as the road intersection candidates. 
The interest operator checks the color variation around every foreground pixel to identify salient points, and it as.signs a quality value to each salient point. If one salient point lies within the predeﬁned radius R of some salient points with higher quality value, it will be discarded. As shown in  11, pixel 1 to pixel 5 are all salient points, with the radius R of 5 pixel. Salient point 2 is too close to salient point 1, which has a higher quality value. We discard salient point 2, while salient point 1 will become a road intersection candidate. We also discard salient point 4 because it lies within the 5 pixel radius of salient point 
3. Salient point 5 is considered as a road intersection point candidate, however, since it does not lie within any other salient points with higher quality value. These road inter.section candidates are then passed to the next module for the determination of actual road intersections. 
2.4 	Filtering Intersections, Extracting Inter.section Connectivity and Road Orienta.tion 
The deﬁnition of intersection connectivity is the number of line segments intersecting at an intersection point. Every road intersection point should be crossed by more than one road, which is more than two line segments. The connec.tivity is the main criteria to distinguish road intersection points from salient points. 
We assume roads on raster map are straight within a small distance ( several meters within a street block). Foreach salientpointdetectedby theinterest operator( GoodFeaturesToTrackfunctioninOpenCV),wedraw a rect.angle around it as shown in  12. The size of the rect.
6 http://sourceforge.net/projects/opencvlibrary 


angle is based on the maximum length in our assumption that the road lines are straight. In  12, we use an 11.by-11 rectangle on the raster map with resolution 2m/pixel, which means we assume the road lines are straight within 5 pixel(e.g., onthehorizontaldirection, aline oflength11 pixelisdivided as5pixel to theleft, one centerpixel and5 pixel to the right), 10 meters. Although the rectangle size can vary with various raster map ofdi.erent resolutions, we use a small rectangle to assure even with the raster map of lower resolution, the assumption that road lines within the rectangle are straight is still tenable. 
The connectivity of the salient point is the number of foregroundpixelthatintersect with this rectangle sincethe roadlines are all singlepixel width. Ifthe connectivityisless thanthree, wediscard thepoint; otherwiseitisidentiﬁed as a road intersection point. Subsequently, we link the salient point to the intersected foreground pixel on the rectangle to computetheslope( orientation) oftheroadlines as shown in  13. 
In this module, we skip the step to trace the pixel be.tween the centerpixel and theintersectedpixel on the rect.angle. This could introduce errors if the intersected pixel arefrom other roadlines whichdo notintersect on the center pixel or the road lines within the rectangle are not straight. This usually happens in low-resolution map, however, in thegeneral case,the rectangleis much smallerthanthe size of a street block, and it is unlikely to have other road lines intersect or have non-straight road lines. Moreover, we save signiﬁcant computation time by avoiding the tracing of ev.ery pixel between the center and the rectangle box. 


3. EXPERIMENTS 
We experimented with six sources of raster map, ESRI Map,MapQuestMap,YahooMap,TIGER/LineMap,USGS Topographic Map and Thomas Brothers Los Angeles 2003 Map, with di.erent map scales as shown in Table 1. USGS Topographic Map and Thomas Brothers Map are scanned map while the others are computer generated from vector data. These raster map are randomly selected within the areas covering El Segundo, CA and St. Louis, MO. 
3.1 Experimental Setup 
Since we assume that we do not have any information about the input map, we use a set of default thresholds for all the input raster map. The size of small connected objects to be removed in text/graphics separation program 


(b)4.17m/pixel 
14: Roadintersection extraction(TIGER/LineMap, St. Louis, MO) 
is set to 20-by-20 pixel, which means any object smaller than this size will be removed. The number of iterations for thegeneralizeddilation operatoris3 andforthegeneralized erosion operator is 2 ( a gap smaller than 6 pixel can be ﬁxed). In the ﬁltering intersection and extracting con.nectivity and orientation module, we used a 21-by-21 pixel rectangle box (10 pixel to the left, 10 pixel to the right plus the center pixel). 
These thresholds are based on practical experiences and may not have the best results for all raster map sources, but the results are good enough to generate a set of road intersectionpointstoidentifythe raster map[4]. We can optimize them for one particular source to get the best pre.cision/recall if we know the source of the input raster map. 


3.2 Experimental Results 
The resulting images from our experiments are shown in  14 and  15. In these ﬁgures, an “X” means one roadintersectionpoint extractedby our system, and the number next to each “X” is shown for the users to examine the matched result after the conﬂation. The statistics are in Table 1 and Table 2. The precision is deﬁned as the num.ber ofcorrectly extracted roadintersectionpointsdividedby thenumberof extracted roadintersectionpoints. Therecall is deﬁned as the number of correctly extracted road inter.Table1:Experimental results,P/R/A(Precision/Recall/PositionalAccuracy),with respecttorastermap source,reso.lution andtype of the roadlayer(ElSegundo,CA andSt. Louis,MO) 

Map Source  Map Type  P/R/A(pixel) by Source  Type of the Road Layers  P/R/A(pixel) by Type of the Road Layers  Resolution (m/pixel)  P/R by Resolution (m/pixel)  Number ofTested map  
ESRI Map  Computer generated  0.96/0.64/0.43  Double line  0.96/0.64/0.43  N/A*  0.96/0.64  10  
MapQuest Map  Computer generated  0.90/0.61/0.84  Double line  1.00/0.88/0.57  2.00  1.00/0.85  1  
2.17  1.00/0.89  2  
Single line  0.87/0.52/0.93  4.84  0.92/0.75  3  
5.17  0.95/0.65  3  
11.11  0.80/0.20  2  
11.67  0.59/0.09  1  
TIGER/Line Map  Computer generated  0.94/0.74/0.57  Single line  0.94/0.74/0.57  1.85  1.00/1.00  1  
2.90  0.98/0.72  1  
3.82  0.92/0.67  4  
4.17  0.97/0.85  2  
7.65  0.84/0.38  1  
7.99  0.95/0.84  1  
USGSTopo.graphic Map  Scanned  0.84/0.74/0.80  Double line  0.84/0.74/0.8  2.00  0.84/0.74  10  
Yahoo Map  Computer generated  0.86/0.64/0.11  Double line  0.96/0.81/0.07  1.20  1.00/0.96  1  
1.22  0.83/0.90  2  
Single line  0.34/0.07/0.13  4.26  0.99/0.76  7  
14.08  0.34/0.07  2  
Thomas Brothers Map  Scanned  0.94/0.66/0.01  Single line  0.94/0.66/0.01  N/A**  0.94/0.66  2  

* We deliberately chose the ERSI map service that does not provide the resolution. ** We randomly selected two scanned Thomas Brothers Map without the knowledge of the scanned resolution 
Table2: Experimental results with respectto resolution(El Segundo, CA and St. Louis, MO) 
Resolution  Precision  Recall  
Higher then 7m/pixel (48 map)  0.92  0.77  
Lower then 7m/pixel (8 map)  0.66  0.27  

section points divided by the number of road intersections on the raster map. The positional accuracy is deﬁned as the distance in pixel between the correctly extracted road intersection points and the corresponding actual road inter.sections. Correctly extracted road intersection points are deﬁned as follows: if we can ﬁnd a road intersection on the original raster map within a 5 pixel radius of the extracted roadintersectionpoint,itisconsidered acorrectly extracted road intersection point. Road intersections on the original map are deﬁned as the intersection points of any two roads if it is a single-line map or the intersection areas where any two roads intersect if it is a double-line map. 
Thelow-resolution map( resolutionslowerthan7m/ pixel) have below average precision and low recall as shown inTable2. Thisisbecausethecharactersandsymbolstouch the lines more frequently as shown in  16. In the preprocessingstep, we use text/graphics separationprogram to removethecharacters andlabels, andit will removemost of the road lines in a low-resolution map. Also, the size of street blocks on the low-resolution map is usually smaller than the window size we use in the intersection ﬁlter, which leads to inaccurate identiﬁcation of road orientations. 
Except for the low-resolution map in our experiments, the USGS Topographic map have the lowest precision and recall. This is because a USGS Topographic Map contains more information layers than other map sources and the quality of scanned map is not as good as computer gener.ated map. 



We also report the positional accuracy because the mor.phological operators may cause the extracted road layers to shift from the original position. The average positional accuracy is lower than 1 pixel in our experiments. This means the average distance between the intersection points we found and the actual intersection points are less than 1 pixel. Thus providing a good set of features for a conﬂation systemtoprecisely aligntherastermapwith othersources. 
The computation time mainly depends onhow manyfore.ground pixel are in the raster map. Raster map which contain moreinformation need moretime than others. USGS Topographic map are the most informative raster map in our experiments; it took less than one minute to extract the road intersections from an 800 x 600 topographic map with resolution 2m/pixel on an Intel Xeon 1.8 GHZ Dual Proces.sors serverwith1GB memory. Othersourcesneedlessthan 20 seconds on images smaller than 500 x 400. 


4. RELATED WORK 
There is a variety of research on extracting information ( road intersection extraction, buildingrecognition, con.tour line extraction) from raster map [8, 9, 11, 12] and satelliteimagery[1]. Theproblem of extracting information from satellite imagery is more di.cult than for raster map, thusthetechniques(e.g., roadtracking andgrouping[14]) used are more computationally intensive. Since our tech.nique deals only with raster map, we focus our comparison on related work in extracting information from raster map. 
The approachesin[8,9,11,12] to exploittheinput raster map rely on a variety of prior knowledge. The main di.er.encebetween our approach and theprevious work is that we assume a more general situation where we do not have any prior knowledge about how to separate the road layers from otherlayersin theinput raster map, such as the color of the road lines, legend information, etc., and the road layers on raster map have not been extracted manually before road intersection extraction. 
Salvatore andGuitton[11] use color classiﬁcation to sep.arate contour lines from other objects on the topographic map and apply image processing algorithms with global topology information to reconstruct the broken lines, which requirespriorknowledge and experimentstogenerate aproper set of color thresholds to separate the contour lines from other objects. With our approach, the system does not have prior knowledge of the road line color in the input raster map. We use an automatically generated threshold to separate the foreground pixel, which include road lines, and utilize the text/graphics separation algorithm with mor.phological operators to extract road lines from foreground pixel. Moreover, for the road-layer extraction step, in the previous workthegoalis to ensure that the resulting contour lineshaveacontinuity closetotheoriginal, which makesthe problem hard to solve and the time complexity higher com.paredtotheuseof morphological operatorsinourapproach. We focus on the road lines close to each intersection point, andignore the accuracy of the entire roadlayer to save com.putation time. The drawback of morphological operators is that theydo notguarantee that the roadlineshave the same continuity as before the text/graphics separation. This does not cause a problem in our approach as we are only inter.ested in segments around each intersection point and the broken lines usually occur in the middle of road lines and not around the intersection points. 
Habib et al.[8] utilize several image processing methods to automatically extract primitives on raster map. They detectthe cornerpoints( salientpointsin ourpaper) of the foreground objects by determining the magnitude and orientation array  an edge detector and an interest op.erator. However, they require the input raster map contain nocharactersorlabels, and therearemajordrawbacks this methodin our automatic approach when there are other layers in the input raster map. First, the edge detector is sensitive to noise, which makes it di.cult to determine the threshold automatically in raster map with many objects and more than one color in the background. Second, the edge detector usually makes the resulting characters fatter than the original ones. Fat characters touch more road lines and they are harder to remove. 
Samet et al.[12] use the legend layer in a learning process to identify labels on the raster map. Meyers et al.[9] use a veriﬁcation based approach to extract data on raster map, which require map speciﬁcations and legends. These ap.proaches all need prior knowledge of the input raster map, such as the color of objects that need to be extracted or the legend information. 


5. CONCLUSION AND FUTURE WORK 
The main contribution of thispaperis toprovide aframe.work to automatically and e.ciently extract road inter.sections from arbitrary raster map by combining several well-studied image processing and graphic recognition algo.rithms. Ourapproachachieves92%precisionand77% recall when automatically extracting roadintersectionpointswith no prior information on the input raster map (resolution higher than 7m/pixel). The resulting point sets provide ac.curate features for identifying the input raster map. For example, a conﬂation system [4] can utilize this technique todiscovergeospatialdata(such asother rastermap,im.agery, and vectordata) with the same coverage asthegiven raster map. In addition, we apply our technique on ran.domly returned map from image search engines and suc.cessfully extract the road intersection points for conﬂation systems to identify the geocoordinate[6]. 
We have made two general assumptions about the input raster map. Firstly, the background pixel must be sepa.rable  the di.erence of luminosity level from the fore.ground pixel, which contain road, building, symbol, and character layers as well as any other notations. This means thatthebackgroundpixel musthavethedominant colorin the raster map. On certain raster map that contain nu.merous objects and the number offoregroundpixelislarger than that of the background pixel, the information layers overlap each other, which makes the automatic processing nearly impossible. Even if we can remove the background pixel on these raster map, removingnoisy objects touching roadlines will break the road layerinto smallpieces whichis hard to reconnect. Secondly, although our approach works with no prior knowledge of the map scales, low-resolution raster map may lead to low precision and recall. 

We intend to extend our work in several ways. Firstly, we want to handle other types of raster map, such as low.qualityscanned map with more than oneluminositylevel on theirbackgroundpixel. In this case, we needtoimprove the automatic segmentation component with histogram analy.sis to generate the threshold in order to separate the fore.groundpixelfrom thebackgroundpixel. Secondly, weplan to add di.erent morphological operators to raise the recall of our approach. There are other morphological operators whichhave similar e.ect as the onesin our current approach. For example, the skeletonizing operator produces one pixel width results similar to those of the thinning operator but the resulting shapes of the road layers will be di.erent. The di.erences will impact the precision and recall of the ﬁnal result, and userscanchoosetoraiseeitherprecision orrecall by  other morphological operators. 

6. ACKNOWLEDGMENTS 
This researchisbased upon work supportedinpartby the NationalScienceFoundationunderAwardNo. IIS-0324955, andinpartby theAirForceO.ce ofScientiﬁcResearch un.der grant number FA9550-04-1-0105. The U.S.Government is authorizedto reproduce anddistribute reportsforGovern.mental purposes notwithstanding any copyright annotation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as neces.sarily representing the o.cial policies or endorsements, ei.ther expressed or implied, of any of the above organizations or any person connected with them. 

7. REFERENCES 
[1] Marie-Flavie Aculair-Fortier, Djemel Ziou, Costas 
Armenakis, and Shengrui Wang. Survey of work on 
road extraction in aerial and satellite images. 
Technical report, Universite de Sherbrooke, 2000. 

[2] J. Patrick Bixler. Tracking text in mixed-mode 
documents. In ACM Conference on Document 
processing systems, 2000. 

[3] Ruini Cao and Chew Lim Tan. Text/graphics 
separation in map. In the Fourth International 
Workshop on Graphics Recognition Algorithms and 
Applications(GREC 2001), 2001. 

[4] Ching-Chien Chen. Automatically and Accurately 
Conﬂating Road Vector Data, Street map and 
Orthoimagery. PhD thesis, University of Southern 
California, 2005. 

[5] Ching-Chien Chen, Craig A. Knoblock, Cyrus Shahabi, Yao-Yi Chiang, and Snehal Thakkar. Automatically and accurately conﬂating orthoimagery and street map. In The 12th ACM International Symposium on Advances in Geographic Information Systems(ACM-GIS’04), 2004. 
[6] Sneha Desai, Craig A. Knoblock, Yao-Yi Chiang, Kandarp Desai, and Ching-Chien Chen. Automatically identifying and georeferencing street map on the web. In The 2nd International Workshop on Geographic Information Retrieval(GIR’05), 2005. 
[7] Lloyd Alan Fletcher and Rangachar Kasturi. A robust algorithm for text string separation from mixed text/graphics images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 10(6):910–918, 1988. 
[8] Ayman F. Habib and Robert E. Uebbing. Automatic extraction of primitives for conﬂation of raster map. Technical report, The Center for Mapping, The Ohio State University, 1999. 
[9] Gregory K. Myers, Prasanna G. Mulgaonkar, Chien-Huei Chen, Je. L. DeCurtins, and Edward Chen. Veriﬁcation-based approach for automated text and feature extraction from raster-scanned map. In Lecture Notes in Computer Science, volume 1072, pages 190–203. Springer, 1996. 
[10] William K. Pratt. Digital Image Processing: PIKS Inside. Wiley-Interscience, third edition edition, 2001. 
[11] Spinello Salvatore and Pascal Guitton. Contour line recognition from scanned topographic map. Technical report, University of Erlangen, 2001. 
[12] Hanan Samet and Aya So.er. A legend-driven geographic symbol recognition system. In 12th International Conference on Pattern Recognition, volume 2, pages 350–355, 1994. 
[13] Jianbo Shi and Carlo Tomasi. Good features to track. In IEEE Conference on Computer Vision and Pattern Recognition, 1994. 
[14] Carsten Steger, Helmut Mayer, and Bernd Radig. The role of grouping for road extraction. Automatic Extraction of Man-Made Objects from Aerial and Space Images(II), 245-256:1931–1952, 1997. 
[15] Yuan Yan Tang, Seong-Whan Lee, and Ching Y. Suen. Automatic document processing: A survey. Pattern Recognition, 29(12):1931–1952, 1996. 
[16] Aurelio Vel´azquez and Serguei Levachkine. Text/graphics separation and recognition in raster-scanned color cartographic map. In the Fifth International Workshop on Graphics Recognition Algorithms and Applications(GREC2001), 2003. 
[17] G.W. Zack, W.E. Rogers, and S.A. Latt. Automatic measurement of sister chromatid exchange frequency. Journal of Histochemistry and Cytochemistry, 25(7):741–753, 1977. 



Classiﬁcation of Line and Character pixel on Raster map  Discrete 
Cosine Transformation Coefﬁcients and Support Vector Machines 

Yao-Yi Chiang and Craig A. Knoblock 
University of Southern California 
Department of Computer Science and Information Sciences Institute 
4676 Admiralty Way, Marina del Rey, CA 90292 
yaoyichi@isi.edu, knoblock@isi.edu 


Abstract 

Raster map are widely available on the Internet. Valu.able information such as street lines and labels, however, are all hidden in the raster format. To utilize the informa.tion, it is important to recognize the line and character pix.els for further processing. This paper presents a novel al.gorithm  2-D Discrete Cosine Transformation (DCT) coefﬁcients and Support Vector Machines (SVM) to classify the pixel of lines and characters on raster map. The ex.periment results show that our algorithm achieves 98% pre.cision and 85% recall in classifying the line pixel and 83% precision and 96% recall in classifying the character pixel on a variety of raster map sources. 
1. Introduction 
Today, more and more sources provide raster map on the Internet (e.g., Google Map, Yahoo Map, and MSN Map). For applications to exploit the information on these raster map, the ﬁrst step is to automatically recognize the lines and the characters on them. For example, a geospatial conﬂation system [1] utilizes the street information ( the orientations and intersections of streets) to align raster map with satellite imagery. To manually extract lines and char.acters from a raster map, users have to manually select the pixel of lines or characters. This is a very tedious and time consuming process. In this paper, we present an algorithm, which uses the 2-D discrete cosine transformation (DCT) coefﬁcients and support vector machines (SVM) to auto.matically classify pixel on raster map into line or charac.ter classes. The classiﬁcation results ( line and character image) can be further used in vectorization components and OCR components to pull out the information such as the geometries and names of streets from the raster map. 
DCT has played an important role in many texture clas.siﬁcation applications for its outstanding ability to generate distinct features for different texture representations [2]. It 

 1. The areas of lines and characters on 
a raster map (TIGER/Line Map) 

transforms an image into the frequency domain where the strength of each frequency is represented by one of the DCT coefﬁcients. Within a local area ( a DCT window), the textures of the foreground and the background are differ.ent since the colors of the background are consistent while the colors of the foreground change frequently. Among the foreground objects, lines and characters also have different texture representations as shown in  1. 
In our algorithm, the classiﬁcation is pixel-based. Ini.tially every pixel is automatically classiﬁed into background or foreground classes  a threshold. The foreground pixel alone are sent to the SVM for line or character pixel classiﬁcation. The training of our SVM model will be de.scribed in Section 3. Support Vector Machines are widely used in many research ﬁelds that require classiﬁcation [3], especially in the area of pattern recognition. In the training process of our algorithm, the SVM constructs hyperplanes in a multidimensional space ( the feature space of the DCT coefﬁcients) that separates pixel of different classes ( line and character classes). SVM can quickly generate a model from a small set of training data and it is robust to noisy data. 
The remainder of this paper is organized as follows. Sec.tion 2 discusses the related work. Section 3 describes our approach to classify line and character pixel on the raster map. Section 4 reports on our experimental results and Sec.tion 5 is the discussion and future work. 

2. Related Work 
Much research work has been performed in the ﬁeld of text and graphics separation from documents [4, 5, 6, 7, 8]. Some of the previous work assumes that the line and char.acter pixel are not overlapping [5, 6, 7] and they extract characters by tracing and grouping connected objects. Oth.ers detect characters from more complex documents ( characters overlap lines)  the differences of the length of line segments in characters and lines [4]. Li et al.[8] separate the characters from lines and rebuild both of them within a local area on the raster map, but assume that all character areas are detected beforehand. These previous al.gorithms perform geometrical analyses of the foreground pixel and focus on ﬁnding characters. Our algorithm de.tects the line and character pixel by exploiting the differ.ences between line and character textures in the frequency domain, and we do not make any geometrical assumptions about the lines or characters. 
To achieve the best results, the previous algorithms re.quire users to setup the geometrical parameters. These pa.rameters are not straight forward and they are source depen.dent. Some examples of the geometrical parameters are: the size of a character, the length of a word, the gap between characters and between words. In our algorithm, there is no parameter adjustment required. We utilize a compara.tively simple training process to build the SVM classiﬁer. The SVM classiﬁer can be applied on different map even if they have not been seen during the training process. 
Furthermore, in order to trace the geometries of lines and characters, the foreground pixel need to be correctly separated from the background. The previous algorithms, however, usually assume that the foreground is already sep.arated from the background or the foreground can be eas.ily separated  histogram thresholding. This assump.tion does not hold on most of the computer generated raster map. Computer generated raster map usually contain shadows around the characters to enhance the visualization. The colors of the shadows and characters are similar and it is difﬁcult to remove the shadows by histogram thresh.olding. If the shadows are not completely removed before processing, the geometries of characters are not correct for geometrical analysis. Our algorithm ﬁnds the continuous colors in the raster map to separate the foreground from the background, and the SVM classiﬁer works well even if the shadows are not removed since it utilizes the differences of the texture representations between lines and characters. 

3. Approach 
The overall approach for this paper is shown in  2. The ﬁrst stage is to separate foreground pixel from the background. The second stage is to classify line and char.acter pixel among the foreground pixel. 


 2. Overall approach 

3.1. Separating Foreground pixel From the Background 
To separate foreground pixel from the background, we ﬁrst generate the DCT coefﬁcients for pixel on the input raster map  a 3-by-3 window. For efﬁciency, we sub.sample the input raster map by scanning the pixel on al.ternative rows/columns. The window size is intended to be small to ensure any object with continuous tones within a 3.by-3 block will be classiﬁed as background. For example, if a park is represented  a color green and it is larger than our DCT window, most of the green pixel will be classiﬁed as background except the pixel near the park boundaries. 
The distribution of energies among the DCT coefﬁcients of the background are signiﬁcantly different than the fore.ground. The property of consistent color in background re.sults in low (near 0) energies for high-frequency DCT coef.ﬁcients while the frequent color changes of the foreground result in higher energies for these coefﬁcients. We discard the DC term ( the frequency {0, 0}) since it represents the average pixel value of the DCT window and it is color dependent. We check the summation of the absolute value of the other 8 DCT coefﬁcients  a preset threshold of 0.0001 which allows for only minimal variation in the color. The pixel is classiﬁed as a background pixel if the summa.tion is smaller than the threshold. After we scan through the entire map and perform the thresholding, we have two images: the background image and the foreground image. There are still some mis-classiﬁed pixel, since some of the areas used to classify the pixel contain both background and foreground areas as described in the park example. We eliminate these false-positives by exploiting the color infor.mation on the original map. 
Different map sources use different colors to represent objects, however, a general property of the color usage is observed – a color is used only once to represent either the foreground or the background. For example, a TIGER/Line map uses a total of 22 colors on a particular map we tested. Among them, 2 are used for lines, 18 are used for charac.ters, and others are used for background. Thus, we assume that a color can only be either foreground color ( lines and characters) or background color and propagate the color information to update the ﬁnal results. The probability of a color to be a background color is the number of pixel of the color in the background image divided by the total number of pixel of the color in the original map. Since we clas.sify the pixel into 2 classes, according to the Maximum A Posteriori probability (MAP) rule, a color belongs to the background class if: 
P (Background|Color) ≥ 0.5 (1) 

3.2 Classifying Line and Character pixel 
After we have the foreground pixel, we want to further classify the foreground pixel into line pixel and character pixel. Characters are generally more complex than lines, so the energies of high-frequency DCT coefﬁcients of charac.ter textures are higher than the energies of these coefﬁcients of line textures. 
We generate the DCT coefﬁcients for each foreground pixel and send them to the SVM for classiﬁcation. We scan the foreground pixel  a larger DCT window ( 5.by-5) to capture the texture differences. Instead of  all DCT coefﬁcients as features for classiﬁcation, we use only 22 out of 25 high-frequency DCT coefﬁcients which have the highest differentiating power to classify line tex.tures and character textures. This number was determined experimentally. Although the SVM should be able to ﬁgure out how to weight each feature, our experiments show that discarding the low-frequency DCT coefﬁcients can help the SVM to reduce the computation time and enhance the accu.racy when generating the training model. 
To train the SVM model, we need training samples for line and character pixel. For the character training samples, we use a Mapquest Map that has characters in all different orientations with shadow pixel and manually remove line pixel from it. For the line training samples, we use two street map from Google Map and one from ViaMichelin Map and manually remove the characters from them. We use more than one map for line samples because we want the training samples to cover all different orientations of lines – straight lines (i.e. horizontal lines, vertical lines, and diagonal lines) and curved lines. 
Based on the 22 DCT coefﬁcients, the SVM classiﬁes foreground pixel into either line class or character class. Now we have one image of lines and one image of char.acters from the input raster map. Since the training sam.ples are character-only-images or line-only-images, the pix.els which have the DCT windows covering line areas and character areas are sometimes mis-classiﬁed. To clean up the results, we perform a 2-phase simple connected com.ponent analysis. A small connected object in either the re.sult image of lines or the result image of characters could be a “missing part” of a larger connected component in the other image. For example, a line segment might be mis.classiﬁed as character pixel because there are characters in its DCT window. If we move that line segment from the character image to the line image, it will connect to the orig.inal bigger line and the result is improved. If it connects to nothing in the line image, it is not a “missing part” of any.thing and we move it back to the character image. So we ﬁrst ﬁnd connected components which are smaller than the DCT window in the character image and move them to the line image. Then we perform the same analysis on the line image and again move small connected components to the character image. Thus, the result images are updated and only small connected components which are part of a larger connected component will be moved to the other image. 


4. Experimental Setup and Results 
We tested 9 online map sources as shown in Table 1. The test map are all disjoint from the training set and are ran.domly selected covering different countries. The SVM clas.siﬁer is built prior to the experiment  three map from three of the sources as described in Section 3. To evaluate the results, we manually extract an image of all lines and an image of all characters from every test map. If there are pixel overlaps between lines and characters, the pixel will appear in both images. We compared our algorithm against the work of Cao et al.[4]. We tried our best to optimize the parameters in Cao’s algorithm although the parameters may not give the absolute best results. Moreover, their al.gorithm uses histogram thresholding to separate the fore.ground pixel from the background, which is not valid on our test map. Thus, we sent the foreground pixel only to their algorithm instead of sending the original raster map. The outputs of our algorithm are two images of either all line or all characters for each input raster map. The output of the work of Cao et al. is an image of all characters since their work focuses on character extraction. We obtain the line image by computing the pixel difference between the character image and the foreground image. 
The precision is deﬁned as the number of correctly clas.siﬁed pixel in the result images of lines/characters divided by the total number of pixel in the manually extracted im.ages of all lines/characters. The recall is deﬁned as the number of correctly classiﬁed pixel in the result images of lines/characters divided by the total number of pixel in the result images of lines/characters. 
Our results show that we can detect almost every char.acter and large portions of lines. The ﬁrst thing to notice is that, because we counted the overlapped pixel twice in 
Table 1. Experimental results comparing our algorithm against the work of Cao et al. 
Map Source  Precision/Recall  of Classiﬁcation  
Line pixel  Character pixel  
Ours  Cao’s  Ours  Cao’s  
A9  99/91%  95/91%  79/98%  77/85%  
MSN  99/79%  91/87%  75/99%  81/86%  
Google  99/99%  95/99%  98/99%  95/72%  
Yahoo  95/91%  70/96%  91/92%  88/30%  
Mapquest  99/78%  88/73%  84/98%  76/85%  
Map24  95/74%  97/70%  73/96%  70/98%  
ViaMichelin  83/34%  44/57%  87/96%  90/68%  
Multimap  89/82%  98/64%  63/90%  46/97%  
TIGER/Line  99/94%  97/89%  83/99%  67/90%  
Average  98/85%  85/82%  83/96%  71/71%  

both line and character class, as long as there are overlapped pixel, the recall of the two classes will not be complimen.tary. The precision is lower in the character classiﬁcation than in the line classiﬁcation but the recall is higher. This is because if the line pixel is near/overlapped with the charac.ter pixel, it will be classiﬁed as a character pixel since it’s neighborhoods are complex textures rather than linear tex.tures. In the comparison, although Cao’s algorithm tried to rebuild the characters and lines, the results show that our algorithm still outperformed theirs on the precision and re.call. This is because when their algorithm extracts the char.acters touching lines, many line pixel that do not overlap with characters are still mis-classiﬁed as characters. The re.call of the line classiﬁcation on ViaMichelin Map of both algorithms are lower than average since there is signiﬁcant overlap between the characters and lines. 

5. Discussion and Future Work 
The main contribution of this paper is to provide a robust and efﬁcient algorithm to automatically and accurately clas.sify line and character pixel on raster map. Our algorithm can be applied on various map that have not been seen in the training process. 
We plan to further develop our work as follows: The classiﬁed character pixel will be used in the OCR compo.nent to recognize the characters. The line pixel will be used in the vectorization component to rebuild the lines. We also want to test our algorithm on more types of raster map. We focus on computer-generated raster map which have more consistent texture properties on lines and characters than hand-draw map, but we believe our algorithm can be applied to other types of map as well. 

6. Acknowledgement 
We would like to thank Dr. Chew Lim Tan for his gen.erous sharing of their code in [4]. We also want to thank Mr. Mark J. Carman for his contribution of the digital sig.nal processing knowledge in this paper. 
This research is based upon work supported in part by the National Science Foundation under Award No. IIS.0324955, in part by the Air Force Ofﬁce of Scientiﬁc Re.search under grant number FA9550-04-1-0105, and in part by a gift from Google. The U.S. Government is autho.rized to reproduce and distribute reports for Governmental purposes notwithstanding any copyright annotation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily rep.resenting the ofﬁcial policies or endorsements, either ex.pressed or implied, of any of the above organizations or any person connected with them. 


References 
[1] C.-C. Chen, C. A. Knoblock, C. Shahabi, Y.-Y. Chiang, and S. Thakkar. Automatically and accurately conﬂat.ing orthoimagery and street map. In Proceedings of the 12th ACM International Symposium on Advances in Geographic Information Systems, 2004. 
[2] T. Randen and J.H. Husoy. 	Filtering for texture classi.ﬁcation: A comparative study. IEEE Transactions on Pattern Analysis and Machine Intelligence, 21(4):291– 310, 1999. 
[3] C. J. C. Burges. 	A tutorial on support vector machines for pattern recognition. Data Mining and Knowledge Discovery, 2(2):955–974, 1998. 
[4] R. 	Cao and C. L. Tan. Text/graphics separation in map. In Proceedings of the 4th International Workshop on Graphics Recognition Algorithms and Applications, 2001. 
[5] J. P. Bixler. 	Tracking text in mixed-mode documents. In Proceedings of the ACM Conference on Document Processing Systems, 2000. 
[6] A. Vel´azquez and S. Levachkine. Text/graphics sep.aration and recognition in raster-scanned color carto.graphic map. In Proceedings of the 5th International Workshop on Graphics Recognition Algorithms and Ap.plications, 2003. 
[7] L. A. Fletcher and R. Kasturi. 	A robust algorithm for text string separation from mixed text/graphics images. 
IEEE Transactions on Pattern Analysis and Machine Intelligence, 10(6):910–918, 1988. 

[8] L. Li, G. Nagy, A. Samal, S. Seth, and Y. Xu. Coopera.tive text and line-art extraction from a topographic map. In Proceedings of the 5th International Conference on Document Analysis and Recognition, 1999. 
[9] T. Joachims. 	Making large-scale SVM learning prac.tical. Advances in Kernel Methods -Support Vector Learning, B. Sch¨olkopf and C. Burges and A. Smola (ed.), MIT-Press, 1999. 



Automatic Extraction of Road Intersection Position, 
Connectivity, and Orientations from Raster map 

Yao-Yi Chiang and Craig A. Knoblock
University of Southern California 
Department of Computer Science and Information Sciences Institute 
4676 Admiralty Way, Marina del Rey, CA 90292 
yaoyichi, knoblock@isi.edu 

ABSTRACT 
The road network is one of the most important types of in.formation on raster map. In particular, the set of road in.tersection templates, which consists of the road intersection positions, the road connectivities, and the road orientations, represents an abstraction of the road network and is more ac.curate and easier to extract than the extraction of the entire road network. To extract the road intersection templates from raster map, the thinning operator is commonly used to ﬁnd the basic structure of the road lines ( to extract the skeletons of the lines). However, the thinning operator produces distorted lines near line intersections, especially at the T-shaped intersections. Therefore, the extracted posi.tion of the road intersection and the road orientations are not accurate. In this paper, we utilize our previous work on automatically extracting road intersection positions to identify the road lines that intersect at the intersections and then trace the road orientations and reﬁne the positions of the road intersections. We compare the proposed approach with the usage of the thinning operator and show that our proposed approach extracts more accurate road intersection positions and road orientations than the previous approach. 
Categories and Subject Descriptors 
H.2.8 [Database Management]: Database Applications— 
Spatial Databases and GIS 
General Terms 
Algorithms, Design 
Keywords 
Raster map, road layer, road intersection template, road orientation, thinning 
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. ACM GIS ’08, November 5-7, 2008. Irvine, CA, USA Copyright 2008 ACM 978-1-60558-323-5/08/11 ...$5.00. 

1. INTRODUCTION 
Raster map are widely available and contain valuable in.formation, such as road lines, labels, and contour lines. For instance, a digital raster graphic (DRG), which is a georef.erenced scanned image of a United States Geological Sur.vey (USGS) topographic map, can be purchased from the USGS website or accessed freely from TerraServer-USA.1 Map repositories like the University of Texas Map Library2 contain information-rich scanned map for many areas and even historical scanned map. Moreover, web mapping ser.vice providers such as Google map,3 Microsoft Live Search map,4 and Yahoo map5 provide high quality digital map covering many countries with rich information layers such as business locations and tra.c information. 
To extract information from the raster map, one ap.proach is to process the vector data used to generate the raster map and then extract the desired information, such as the road vectors. However, it is generally di.cult to access the original vector data for many raster map. For example, the web mapping services provided by Google, Mi.crosoft, and Yahoo all provide their map only in the raster format and there is no public access to the original vector data. Furthermore, many raster map found on the Internet are simply images without any auxiliary information of the original vector data. Hence, a more general approach is to utilize image processing and graphics recognition technolo.gies to separate the information layers (e.g., the road layer and text layer) on the raster map and then extract and rebuild selected layers [7]. 
Among the information layers on the raster map, one of the most important layers is the road layer. Instead of ex.tracting the entire road layer, the set of road intersection templates, which consists of the road intersection position, connectivity, and orientations, provides the basic elements of the road layout and is more accurate and easier to extract than the extraction of the entire road network. Since the road layers commonly exist across many di.erent geospatial layers (e.g., satellite imagery, vector data, etc.), by match.ing the set of road intersection templates from a raster map with another set of road intersection templates from a geo.referenced data set (e.g., vector data), we can identify the geospatial extent of the raster map and align the raster map with other geospatial sources [3]. An example of an inte.
1http://terraserver-usa.com/ 2http://www.lib.utexas.edu/map/ 3http://map.google.com/ 4http://map.live.com/ 5http://map.yahoo.com 

grated and aligned view of a tourist map and satellite im.agery is shown in  1. Accurate road intersection tem.plates ( road intersection templates with accurate posi.tions, road orientations, and connectivities) help to reduce the search space during the matching process by selecting road intersections with the same connectivity and similar road orientations as initial matching candidates. The accu.rate positions of the templates also enhance the alignment results. Furthermore, the extracted road intersection tem.plates with accurate positions and road orientations can be used as seed points to extract roads from aerial images [10] as shown in  2, especially when the access to vector data is limited. 
To extract the road intersection templates from a raster map, the ﬁrst step is to detect the positions of the road inter.sections on the raster map and then convert the road lines 

 1: The integration of imagery from Google map and a tourist map (Tehran, Iran). 

 2: Use the extracted road intersection tem.plates to extract roads from imagery 
around the road intersections into vector format, which is di.cult since the raster map are usually complicated and contain many di.erent layers such as characters and road lines. In our previous work [7], we presented a technique that works on the pixel level of the raster map and uti.lizes the distinctive geometric properties of the road lines and road intersections to automatically extract road inter.sections from raster map. During the extraction process, our previous technology reconnects the broken lines by ﬁrst thickening the lines and then utilizing the thinning operator to generate one-pixel width lines to identify possible road intersections. 
The idea of the thinning operator [13] is to produce one-pixel width lines that represent the central lines of the thick lines ( to preserve the basic structures of the thick lines). The thinning operator ﬁrst identiﬁes every foreground pixel that connects to one or more background pixel as candidate to be converted to the background ( to reduce the region of foreground objects). Then a conﬁrmation step checks if the conversion of the candidate will cause any disappear.ance of original line branches to ensure the basic structures of the original objects will not be compromised. The thin.ning operator is robust and e.cient; however, the thinning operator distorts the lines near the intersections. As shown in  3, if we apply the thinning operator directly on the thick lines shown in  3(a), the lines that are near intersections are all distorted as shown in  3(b). Our approach in [7] to minimize the extent of the distortions is to ﬁrst erode the lines  an binary erosion operator [13] as shown in  3(c) and then apply the thinning operator. However, there are still distortions even after the erosion operator is applied as shown in  3(d). 
In the previous work, the proposed technologies focused on e.ciently extracting the positions of road intersections and 

(a) 
Thickened road lines 	(b) Result of applying the thinning operator without erosion 

(c)
Thickenedroadlinesaf-tererosion




 3: Distorted road lines near road intersec.tions with the thinning operator 

extracting the road orientations  a simpler technique that works on the one-pixel width road lines generated by the thinning operator. Therefore, the extracted road orien.tations su.er from the distortions produced by the thinning operator and the positions of the road intersections are not accurate. To overcome the distortion problem and to ex.tract accurate road intersection templates, in this paper, we present an approach that employs the results of our previous work to trace the road lines for accurate road orientations and then utilizes the road orientations to reﬁne the positions of the road intersections. 
The remainder of this paper is organized as follows. Sec.tion 2 explains our previous road intersection extraction work. Section 3 describes our improved approach to au.tomatically extract road intersection templates. Section 4 reports on our experimental results. Section 5 discusses the related work, and Section 6 presents the conclusion and fu.ture work. 

2. BACKGROUND WORK 
In our previous work [7], we proposed a technique to auto.matically extract the road intersections from various raster map. There are three major steps in our previous ap.proach. First, we remove the background pixel from the raster map. Next, we separate the road lines from other objects in the foreground pixel and rebuild the road lines. Finally, we detect road intersection candidates on the road lines and utilize the connectivity ( the number of lines that intersect at a given road intersection candidate) to ﬁnd actual road intersections. The road orientations are also computed in the last step as a by-product. We brieﬂy ex.plain these three steps of the previous approach in turn in the following subsections. 
2.1 	Automatic Segmentation 
Since the foreground pixel of the raster map contain the road layers, the ﬁrst step for extracting road intersections is to extract the foreground pixel. We utilize a technique called segmentation with automatically generated thresholds to separate the foreground pixel from the background pixel of the raster map. Because the background colors of raster map have a dominant number of pixel and the foreground colors have high contrast against the background colors, we generate the segmentation thresholds by analyzing the shape of the grayscale histogram of the raster map [15]. We ﬁrst identify the largest luminosity cluster in the histogram as the background cluster and then classify other clusters as either background clusters or foreground clusters by comparing the number of pixel in the clusters. After we remove the back.ground clusters from the original map shown in  4(a), we produce the binary map shown in  4(b) 
2.2 	Pre-Processing – Extract and Rebuild RoadLayer 
With the extracted foreground pixel, we ﬁrst utilize our parallel-pattern tracing algorithm [7] to detect the road width of the majority of roads on the raster map. For a given fore.ground line, the parallel-pattern tracing algorithm employs two convolution masks that work on the horizontal and ver.tical directions to search for corresponding parallel lines to determine the road width. The detected road width is shown with gray dashed lines in  4(c). The road width is im.portant in the latter steps to rebuild the road layer. 

(a) 
Original raster map (b) Binary map 

(c) 
Road width (d) Extracted road lines 

(e) 
Thickened road lines (f) Eroded road lines 

(g) 
Thinned road lines 	(h) Road intersection candi.dates 





 4: Automatic extraction of road intersections 
After we obtain the road width, we use the text/graphic separation technique [1] to remove the foreground pixel that do not hold the properties that constitute roads (e.g., pixel for labels, contour lines, etc). The extracted road layer is shown in  4(d). Finally, to extract the structure of the road layer, we utilize the binary dilation operator with the number of iterations determined by the detected road width to ﬁrst thicken the road line as shown in  4(e). Then we apply the binary erosion operator and the thinning operator to generate one-pixel width road lines as shown in  4(f) and  4(g). 


2.3 Determine Road Intersections and Extract 

Connectivity with Road Orientation 
With the one-pixel-width road lines, we utilize the corner detector [14] to detect road intersection candidates as shown with the cross marks on the road lines in  4(h). For each road intersection candidate, we draw a box around it and then use the number of foreground pixel that intersects with this rectangle as the connectivity of the intersection candidate as shown in  5. If the connectivity is less than three, we discard the point; otherwise it is identiﬁed as a road intersection point. Subsequently, without tracing the line pixel, we link the road intersection candidate to the intersected foreground pixel on the rectangle boundaries to compute the orientations of the road lines. 
The ﬁnal road intersection extraction results are shown in  6. Although we successfully extract most of the road intersections, the positions of some extracted road intersec.tions are not at the center points of the intersected lines and the road orientations are not accurate, especially the ones on the intersection of a T-shape roads. This is because the ex.tracted road lines near the intersections are distorted by the morphological operators ( the binary dilation operator, the binary erosion operator, and the thinning operator) as shown in  4(g); and the method we use to compute the road orientations does not take into account the distortions. In the next section, we present our improved approach to overcome the distortion problem and build an accurate road intersection template by utilizing the extracted road inter.section position, the extracted one-pixel-width lines, and the detected road width. 
3. 	AUTOMATIC EXTRACTION OF ROAD INTERSECTION TEMPLATES 
The overall approach to extract road intersection tem.plates is shown in  7, where the gray boxes indicate the results we use from our previous approach in [7]. Based on our previous work, we recognize that there is distortion near each road intersection, and the extent of the distortion is determined by the thickness of the line, which is deter.mined by the number of iterations of the morphological op.erators. And the number of iterations of the morphological operators is based on the road width that is detected  the parallel-pattern tracing algorithm. 
Therefore, with the results of the road intersection posi.tions and the road width, we ﬁrst generate a binary large object ( blob, a connected foreground object on an im-

 5: Construct lines to compute the road ori.entations 

 6: Extracted road intersections 

 7: Overall approach to extract the road in.tersection templates from raster map 
age) for each intersection to create a blob image. We then intersect the blob image with the thinned line image to label the location of the potential distorted lines. Finally, we trace the unlabeled lines to compute the road orientations and use the road orientations to update the positions of the road in.tersections to generate accurate road intersection templates. 


3.1 Generating Road Intersection Blobs 
Since the thinning operator produces distorted lines near the road intersections and the extent of the distortion is de.termined by the road width, we can utilize the extracted road intersections and the road width from our previous ap.proach [7] to label the locations of the potential distorted lines. 

For each extracted road intersection shown in  6, we generate a rectangular blob  the binary dilation oper.ator with twice the road width as the number of iterations. The result blob image is shown in  8(a). We then in.tersect the blob image with the thinned line image shown in  4(g) to label the locations of the potential distorted lines. As show in  8(b), the line segments within the gray boxes are labeled by the blob image as the potential distorted lines. 
Subsequently, we use the labeled image shown in Fig.ure 8(b) to detect road line candidates for each intersection. We identify the lines linked to each blob and their contact points by detecting the line pixel that have any neighbor.ing pixel labeled by the gray boxes. These contact points indicate the starting points of a road line candidates for the blob. In the example shown in  8(c), the road inter.section associated with the left blob has three contact points that are on the top, right, and bottom of the blob, and these points will be used as starting points to trace the road line candidates for the road intersections of the blob. 
Instead of drawing a rectangle around each intersection to ﬁnd the contact points as in our previous approach [7], we generate the blob image to take advantage of  image processing operations, which are more robust and easier to implement. For example, when we generate the blob image, if two intersections are very close to each other, their blobs automatically merge into one big blob by the binary dila.tion operator without additional implementations to com.pute the intersections of every rectangle box. This is impor.tant because when two intersections are very close to each other, the thinned line between them is totally distorted as shown in  9. With the merged blob shown in Fig.ure 9(d), we can associate the four lines linked to this blob with the two intersections as the road line candidates of the intersections. In the last step of updating road intersection templates, a ﬁltering step will decide which candidates need to be discarded so that the two intersections will still have three road lines instead of four. 

3.2 Tracing Road Lines 
For each road intersection blob, we start to trace the road lines from its contact points  the ﬂood-ﬁll algorithm shown in Table 1. The ﬂood-ﬁll algorithm ﬁrst labels the contact point as visited and then checks the eight neighbor.ing pixel of the contact point to ﬁnd unvisited points. If any of the eight neighboring pixel are not labeled as visited and are not within a blob, the neighboring pixel will be set as the next visit point for the ﬂood-ﬁll algorithm to consider. 
When the ﬂood-ﬁll algorithm considers a new pixel, we also record the position of the pixel to latter compute the road orientation. The number of pixel that the ﬂood-ﬁll algorithm can trace from each contact point is controlled by the variable MaxLinePixel shown in Table 1. The ﬂood-ﬁll algorithm counts the number of pixel that it has visited and makes sure the count is less than the MaxLinePixel variable every time before it visits a new pixel or it stops. As shown in  10, instead of tracing the whole curve starting from the two contact points ( the one on the right and the one on the bottom), we assume that roads near the con.tact points are straight within a small distance ( several meters within a street block on the raster map) and uti.lize the MaxLinePixel to ensure that the ﬂood-ﬁll algorithm traces only a small portion of the road lines near the contact 



(c) 
Contact points 

 8: Generate the blob image to label the road lines 
points. 
After the ﬂood-ﬁll algorithm walks through the lines from each contact point and records the position of the line seg.ment pixel, we utilize the Least-Squares Fitting algorithm to ﬁnd the linear functions of the lines. Assuming a linear function L for a set of line pixel traced by the ﬂood-ﬁll algo.rithm, by minimizing the sum of the squares of the vertical o.sets between the line pixel and the line L, the Least-Squares Fitting algorithm ﬁnds the straight line L that best represents the traced line pixel. The algorithm works as follows:6 Given a set of pixel locations {(xi,yi)}, i =1,..., n, in the map, ﬁnd the target line function L, where 
L : Y = m . X + b (1) 
6The proof of the Least-Squares Fitting algorithm can be found in most statistics textbooks or on the web at http://mathworld.wolfram.com/LeastSquaresFitting.html 
P PP 
n xy . xy 
m = PP (2) 
nx2 . ( x)2 
PP 
y . mx 
b = (3) 
n 

The computed line functions are then used in the next step of updating road intersection templates to identify actual road lines and reﬁne the positions of the road intersections. 

3.3 Updating Road Intersection Templates 
In the previous steps, we associated a set of road line candidates with a road intersection and traced the road line 




(c) 
Distorted thinned lines (d) Merged blobs 

 9: Merge nearby blobs to trace roads 
Table 1: Flood-ﬁll algorithm 

Flood-fill(Image, x, y) 
1  if Image.isALineP ixel(x, y) = T RU E  
2  AND Im age.isV isited(x, y) .= T RUE  
3  AND Image.isW ithInABlob(x, y) .= T RUE  
4  AND Pix elCount < MaxLinePixel  
5  then  
6  RecordP ixelP osition(x, y)  
7  P ixelCount + +  
8  Image.SetV isited(Image, x, y)  
9  Flood-fill(Image, x + 1, y)  
10  Flood-fill(Image, x . 1, y)  
11  Flood-fill(Image, x, y + 1)  
12  Flood-fill(Image, x, y . 1)  
13  Flood-fill(Image, x + 1, y + 1)  
14  Flood-fill(Image, x . 1, y . 1)  
15  Flood-fill(Image, x + 1, y . 1)  
16  Flood-fill(Image, x . 1, y + 1)  


 10: Trace only a small portion of the road lines near the contact points 
functions. In this step, we ﬁrst compute the intersections of these road line candidates and then ﬁlter out outliers of the road line candidates. 
There are three possible intersecting cases for the road line candidates of one intersection. The original map of the three possible intersection cases are shown on the left in  11. The middle images shown in  11 are the thinned lines with the locations of the potential distorted lines labeled by the blob images, and the right images are the traced line functions ( the line functions that are computed  the Least-Squares Fitting algorithm) drawn on a two-dimension plane. 
The ﬁrst case is where all of the road line candidates in.tersect at one point as shown in  11(a). The second case is where the road line candidates intersect at multiple points and the intersection points are all near the road inter.section position, as shown in  11(b). The third case is where the road line candidates intersect at multiple points and some of the points are not near the road intersection ( the outliers), as shown in  11(c). 
For the ﬁrst case shown in  11(a), the position of the updated road intersection template is the intersection point of the road line candidates, and the road orientations are the orientations of the intersecting roads, which are 0 degrees, 90 degrees, and 270 degrees, respectively. For the second case shown in  11(b), the position of the road intersection template is the centroid of the intersection points of all road line candidates and the road orientations are the orientations of the intersecting roads, which are 80 degrees, 172 degrees, 265 degrees, and 355 degrees, respectively. 
Since the extent of the distortion depends on the road width, the positional o.set between any intersection of the road line candidates and the road intersection should not be larger than the road width. We ﬁrst consider the upper road intersection in the third case shown in  11(c). The in.tersections where the dashed line intersects with the other two lines are more than a road width away from the upper road intersection, so we discard the dashed line and use the 


(a) Case one: the three lines intersect at one point 

(b) 
Case two: the three lines intersect at multiple points 

(c) 
Case three (the upper road intersection): the four lines intersect at multiple points with one outlier (the dashed line) 



 11: Filter out the outliers 
centroid of the intersection points of the other three road line candidates to update the position of the upper road in.tersection. The road orientations of this road templates are 60 degrees, 150 degrees, and 240 degrees, respectively. Sim.ilarly, for the lower road intersection shown in  11(c), the road line candidate that is almost parallel to the dashed line will be discarded, and the dashed line is kept as a road line for the road intersection. The road orientations of the lower road templates are 60 degrees, 240 degrees, and 330 degrees, respectively. 
The last example shows how the merged blob helps to extract correct road orientations even when one of the lines is totally distorted during the thinning operation. This case holds when the distorted line is very short and hence it is very likely to have the same orientation as the other lines. For example, in  11(c), the distorted line is part of a straight line that goes through the intersection, so it has the same orientation as the 240 degree line. 


4. EXPERIMENTS 
In this section, we evaluate our approach by conducting experiments on raster map from various sources. We ﬁrst explain the test data sets and our evaluation methodology and then analyze the experimental results and provide a comparison to our previous work [7]. 
4.1 Experimental Setup 
We evaluate 10 raster map from ﬁve di.erent sources, MapQuest map,7, OpenStreet map.8, Google map, Mi.
7http://www.mapquest.com/ 8http://www.openstreetmap.org/ 
crosoft Live Search map, and Yahoo map, The 10 map cover two areas in the United State, one in Los Angeles, Cal.ifornia and the other one in St. Louis, Missouri.  12 shows two example map for these areas. 
We ﬁrst apply our road intersection extraction application from our previous approach to detect the road width and extract a set of road intersection positions and the thinned road lines for each map. Then we utilize the technique de.scribed in this paper to extract the accurate road intersec.tion templates from the raster map. We report the accuracy of the extraction results  the positional o.set, orienta.tion o.set, and connectivity o.set. The positional o.set is the average number of pixel between the extracted road intersection templates and the actual road intersections on the raster map. The actual road intersections on the raster map are deﬁned as the centroids of the intersection areas of two or more intersecting road lines. The orientation o.set is the average number in degrees between the extract road ori.entations and the actual road orientations. The connectivity o.set is the total number of missed road lines. We manually examine each road intersection on the raster map to ob.tain the ground truth of the positions of the road intersec.tions, the connectivities and the road orientations.  13 shows the ground truth of two road intersection templates. For the road intersection template in  13(a), the road orientations are 0 degrees, 90 degrees, and, 270 degrees, re.spectively. 


4.2 Experimental Results 
From the 10 raster map, we extracted a total of 139 road intersection templates with 438 lines and the results are 

(a) 
Live Search map, Los Angeles, Cali.fornia 

(b) 
Yahoo map, St. Louis, Missouri 



 12: Examples of the test map 


(a) The ground truth on a (b) The ground truth on a map from Google map map from OpenStreet map 
 13: The ground truth 
shown in Table 2. The average positional o.set is 0.4 pix.els and the average orientation o.set is 0.24 degrees, which shows our extracted road intersection templates are very close to the ground truth on these test map. In order to achieve higher accuracy in the positional and orientation o..sets, we need to discard the lines that do not have accurate orientations in the ﬁltering step ( the outliers). So the connectivity is lower than our previous work; we missed 13 lines from a total of 451 lines in order to trace the lines to extract the road intersection templates. These 13 lines all belong to the road intersections that are near the boundaries of the map and hence we cannot ﬁnd road lines long enough to compute the correct orientations. 
We also compare the extracted template with the ex.tracted template  the methods of our previous approach in [7], which uses only the thinning operator. The compari.son results of the positional o.set and orientation o.set are shown in  14. Our proposed approach in this paper has a large improvement on the results of every map source for both positional o.set and orientation o.set.  16 shows example extraction results  the approach in this paper and our previous approach in [7]. This ﬁgure shows the distorted road lines from our previous work are corrected  the approach in this paper.9 

4.3 Computation Time 
The system is built  Microsoft Visual Studio 2005 on a Windows 2003 Server with Intel Xeon 1.8 GHZ Dual Processors and 1 GB of memory. The largest test map is 809 pixel by 580 pixel and it took 11 seconds to ﬁrst extract the road intersection positions  the approach in [7] and another 5 seconds to extract the road intersection templates  the techniques proposed in this paper. The dominant factors of the computation time are the map size, the number of foreground pixel of the raster map, and the number of road intersections on the raster map. The average time is 
10.5 seconds to extract the position of road intersections and 4.7 seconds to extract the road intersection templates, which is su.cient for many applications that need real-time intersection extraction results. 


5. RELATED WORK 
Much research work has been performed in the ﬁeld of extracting and recognizing graphic objects from raster map, 
9The comparison results for this map  only the thinning operator is shown in  6 


(b) 
Orientation o.set comparison (in degrees) 

 14: Experimental results  the approach from this paper and our previous work  the thinning operator 
such as extracting road intersections [6, 7, 8], separating lines from text [1, 5, 11, 12], and recognizing contour lines [4, 9] from raster map. 
In our previous work [6], we combine a variety of im.age processing and graphics recognition methods such as the morphological operators to automatically separate the road layers from the raster map and then extract the road intersection points. Since the thinning operator is used in the work, the extracted road intersections have high posi.tional and orientation o.sets when the road width is wide. To overcome this problem, in [7] we utilize the Localized Template Matching (LTM) [2] to improve the positional o..set. However, since the road orientations are not accurate, it is di.cult for LTM to ﬁnd the correct matches to improve the positions and further correct the road orientations. In this paper, we utilize the detected road width, the positions of the road intersections, and the thinned lines to trace the road lines and then update the road intersection positions to achieve the best results on the extraction of road intersection templates compared to our previous work. 
For the other related work speciﬁcally working on extract.ing road intersections, Habib et al. [8] utilize several image processing algorithms to automatically extract road inter.sections from raster map. In [8], they detect the corner points on the extracted road edges and then classify the 

Table 2: The positional o.set, orientation o.set, and the connectivity o.set 
Map Source  Number of Extracted Road Intersections  Number of Extracted Road Lines  Positional O.set (in pixel)  Orientation O.set (in degrees)  Connectivity O.set (number of lines)  
Google map  28  87  0.12  0  3  
Live Search map  28  91  0.52  0.37  2  
Yahoo map  27  82  0.35  0.24  5  
MapQuest map  25  83  0.69  0.55  0  
OpenStreet map  31  95  0.37  0.29  3  



(b) 
MapQuest map (this paper) 

 15: Experimental results  the approach from this paper and our previous work  the thinning operator 
corner points into groups. The centroids of the classiﬁed groups are the road intersections. Without tracing the road lines, false-positive corner points or intersections of T-shape roads signiﬁcantly shift the centroids away from the correct locations. In comparison, our approach explicitly traces the road lines for accurate positions of the road intersection tem.plates and accurate orientations of the intersecting roads. 
To extract text and lines from documents or map, the research on text/graphics separation [1, 5, 11, 12] performs geometrical analyses of the foreground pixel or exploits the di.erences between line and character textures to achieve their goals. Cao et al. [1] detect characters from map where characters overlap lines  the di.erences in the length of line segments of characters and lines. Nagy et al. [11, 12] ﬁrst separate the characters from the lines  connected component analysis and then focus on local areas to rebuild the lines and characters  various methods. Our other work in [5] detects the line and character pixel by exploiting the di.erences between line and character textures in the frequency domain. 
Furthermore, Khotanzad et al. [9] utilize a color segmenta.tion method and exploit the geometric properties of contour lines such as that contour lines never intersect each other to extract the contour lines from scanned USGS topographic map. Chen et al. [4] latter extend the color segmenta.tion method from Khotanzad et al. [9] to handle common topographic map ( not limited to USGS topographic map)  local segmentation techniques. However, these text/graphics separation and line extraction approaches do not perform further analysis of the lines to determine the lo.cations of the line intersections and orientations; hence they provide only partial solutions to the problem of extracting the road intersection templates. 


6. CONCLUSION AND FUTURE WORK 
In this paper, we proposed a technique to automatically extract road intersection templates from raster map. Our experiment shows e.cient and accurate results with the po.sitional o.set as 0.4 pixel and orientation o.set as 0.24 de.grees on average. The result is a set of road intersection templates, which consists of the positions and connectivities of the road intersections and the orientations of the roads that intersect at the intersection for each raster map. With the accurate road intersection templates, conﬂation applica.tions can signiﬁcantly reduce the possible matching candi.dates during the point matching process by  the road orientations as one feature to select possible matches. More.over, applications that work on imagery to extract roads can also beneﬁt from the accurate road intersection templates to enhance the quality of their extraction results. 
We plan to extend our work to fully vectorize the road lines from the raster map. For example, we plan to discover the relationship between the road intersections as well as to extract feature points from the road lines other than road intersections, such as corner points on curve roads. The fully vectorized road network then can be used to match with other vector data to fuse the raster map with other geospatial data even more e.ciently. 


7. ACKNOWLEDGMENTS 
This research is based upon work supported in part by the University of Southern California under the Viterbi School Doctoral Fellowship, in part by the National Science Foun.dation under award number IIS-0324955, in part by the United States Air Force under contract number FA9550-08.C-0010, and in part by a gift from Microsoft. 
The U.S. Government is authorized to reproduce and dis.(b) Thinning operator (c) This paper 



 16: Detail view of various road intersection templates 
tribute reports for Governmental purposes notwithstanding any copyright annotation thereon. The views and conclu.sions contained herein are those of the authors and should not be interpreted as necessarily representing the o.cial policies or endorsements, either expressed or implied, of any of the above organizations or any person connected with them. 

8. REFERENCES 
[1] R. Cao and C. L. Tan. Text/graphics separation in 
map. In Proceedings of the Fourth International 

Workshop on Graphics Recognition Algorithms and Applications, pages 167–177, Springer-Verlag, London, UK, 2002. 
[2] C.-C. Chen, C. A. Knoblock, and C. Shahabi. 
Automatically conﬂating road vector data with 
orthoimagery. GeoInformatica, 10(4):495–530, 2006. 

[3] C.-C. Chen, C. A. Knoblock, and C. Shahabi. Automatically and accurately conﬂating raster map with orthoimagery. GeoInformatica, 12(3):377–410, 2008. 
[4] Y. Chen, R. Wang, and J. Qian. Extracting contour 
lines from common-conditioned topographic map. 

IEEE Transactions on Geoscience and Remote Sensing, 44(4):1048–1057, 2006. 
[5] Y.-Y. Chiang and C. A. Knoblock. Classiﬁcation of line and character pixel on raster map  discrete cosine transformation coe.cients and support vector machine. In Proceedings of the 18th International Conference on Pattern Recognition, pages 1034–1037, 2006. 
[6] Y.-Y. Chiang, C. A. Knoblock, and C.-C. Chen. Automatic extraction of road intersections from raster map. In Proceedings of the 13th Annual ACM International Symposium on Geographic Information Systems, pages 267–276, 2005. 
[7] Y.-Y. Chiang, C. A. Knoblock, C. Shahabi, and C.-C. Chen. Automatic and accurate extraction of road intersections from raster map. Geoinformatica, 2008. 
[8] A. F. Habib and R. E. Uebbing. Automatic extraction of primitives for conﬂation of raster map. Technical report, The Center for Mapping, 1999. 
[9] A. Khotanzad and E. Zink. Contour line and 
geographic feature extraction from USGS color 
topographical paper map. IEEE Transactions on 
Pattern Analysis and Machine Intelligence, 
25(1):18–31, 2003. 

[10] G. Koutaki and K. Uchimura. Automatic road extraction based on cross detection in suburb. In Proceedings of the SPIE, Image Processing: Algorithms and Systems III, volume 5299, pages 337–344, 2004. 
[11] L. Li, G. Nagy, A. Samal, S. Seth, and Y. Xu. Cooperative text and line-art extraction from a topographic map. In Proceedings of the Fifth International Conference on Document Analysis and Recognition, pages 467–470, 1999. 
[12] L. Li, G. Nagy, A. Samal, S. C. Seth, and Y. Xu. Integrated text and line-art extraction from a topographic map. International Journal of Document Analysis and Recognition, 2(4):177–185, 2000. 
[13] W. K. Pratt. Digital Image Processing: PIKS Scientiﬁc Inside. Wiley-Interscience, 3rd edition, 2001. 
[14] J. Shi and C. Tomasi. Good features to track. In Proceedings of the Computer Vision and Pattern Recognition, pages 593–600, 1994. 
[15] G. W. Zack, W. E. Rogers, and S. A. Latt. Automatic measurement of sister chromatid exchange frequency. Journal of Histochemistry and Cytochemistry, 25(7):741–753, 1977. 

Geoinformatica DOI 10.1007/s10707-008-0046-3 
Automatic and Accurate Extraction of Road Intersections from Raster map 
Yao-Yi Chiang · Craig A. Knoblock · Cyrus Shahabi · Ching-Chien Chen 
Received: 23 June 2006 / Revised: 19 December 2007 / Accepted: 4 February 2008 © Springer Science + Business Media, LLC 2008 
Abstract Since map are widely available for many areas around the globe, they provide a valuable resource to help understand other geospatial sources such as to identify roads or to annotate buildings in imagery. To utilize the map for under.standing other geospatial sources, one of the most valuable types of information we need from the map is the road network, because the roads are common features used across different geospatial data sets. Speciﬁcally, the set of road intersections of the map provides key information about the road network, which includes the location of the road junctions, the number of roads that meet at the intersections ( connectivity), and the orientations of these roads. The set of road intersections helps to identify roads on imagery by serving as initial seed templates to locate road pixel. Moreover, a conﬂation system can use the road intersections as reference features ( control point set) to align the map with other geospatial sources, such as 
This work is based on an earlier paper: Automatic Extraction of Road Intersections from Raster map, in Proceedings of the 13th annual ACM International Symposium on Geographic Information Systems, Pages: 267–276, ©ACM, 2005. http://doi.acm.org/10.1145/1097064.1097102. 
Y.-Y. Chiang (B) · C. A. Knoblock Department of Computer Science and Information Sciences Institute, University of Southern California, Marina del Rey, CA 90292 USA e-mail: yaoyichi@isi.edu 
C. A. Knoblock e-mail: knoblock@isi.edu 
C. Shahabi Computer Science Department, University of Southern California, SAL 300, Los Angeles, CA 90089-0781, USA e-mail: shahabi@usc.edu 
C.-C. Chen Geosemble Technologies, 2041 Rosecrans Ave. Suite 245 El Segundo, CA 90245 USA e-mail: jchen@geosemble.com 

Geoinformatica 

aerial imagery or vector data. In this paper, we present a framework for automatically and accurately extracting road intersections from raster map. Identifying the road intersections is difﬁcult because raster map typically contain much information such as roads, symbols, characters, or even contour lines. We combine a variety of image processing and graphics recognition methods to automatically separate roads from the raster map and then extract the road intersections. The extracted information includes a set of road intersection positions, the road connectivity, and road orientations. For the problem of road intersection extraction, our approach achieves over 95% precision (correctness) with over 75% recall (completeness) on average on a set of 70 raster map from a variety of sources. 
Keywords Raster map · Road layer · Road intersection · Imagery · Conﬂation · Fusion · Vector data · Geospatial data integration 
1 Introduction 
Humans have a very long history of  map. Fortunately, due to the widespread use of Geographic Information System and the availability of high quality scanners, we can now obtain many map in raster format from various sources, such as digitally scanned map from USGS Topographic map on Microsoft Terraserver1 or computer generated map from U.S Census Bureau2 (TIGER/Line map), Google map,3 Yahoo map,4 MapQuest map,5 etc. Not only are the raster map readily accessible compared to other geospatial data (e.g., vector data), but they provide very rich information, such as road names, landmarks, or even contour lines. Consider the fact that road map are generally the most frequently used map type and the road layers commonly exist across many different geospatial data sources, the road network can be used to fuse a map with other geospatial data. In particular, instead of extracting the whole road network, road intersection templates ( the positions of the road intersections, the road connectivity and orientations) provide the basic elements of the road layout and are comparatively easy to extract. For example, the road intersection templates can be used to extract roads from other sources, such as aerial images [13]. As shown in Fig. 1, without  the whole road network, the extracted road intersection templates can be used as seed templates to extract the roads from the imagery of Tehran, Iran where we have limited access to the vector data. Moreover, since the layout of a set of road intersections within a certain area is usually unique, we can utilize the road intersections as the “ﬁngerprint” of the raster map to determine the extent of a map that is not georeferenced [9]; and further, the road intersections can be used as feature points to align the raster map with other geospatial sources [5]. For example, Fig. 2a shows a hybrid view of Tehran, Iran from Google Map where only major roads are shown with labels. By matching the layout 
1http://terraserver-usa.com/ 2http://tiger.census.gov/cgi-bin/mapurfer 3http://map.google.com 4http://map.yahoo.com 5http://www.mapquest.com 

Geoinformatica 
Fig. 1 Use the extracted road intersections as seed templates to extract roads from imagery 

of road intersections from a tourist map found on the Internet shown in Fig. 2btothe corresponding layout on the satellite imagery, we can create a enhanced integrated representation of the map and imagery as shown in Fig. 3. 
Fig. 2 The integration of imagery from Google Map and a tourist map (Tehran, Iran). a The Google Map hybrid view. b The tourist map in Farsi 

Geoinformatica 


Automatic extraction of road intersections is difﬁcult due(e.g., map geocoordi.nates, scales, legend, original vector data, etc) and also the great complexity of map. For example, for a Google Map snapshot in an article or an evacuation map posted on a wildﬁre news website, there is no uniform metadata or auxiliary information that can be automatically found and parsed to help utilize the map without human intervention. In addition, map typically contain multiple layers of roads, buildings, symbols, characters, etc. People interpret the map by examining the map context such as road labels or looking for the map legends, which is a difﬁcult task for machines. To overcome these problems, we present a framework, which works on the pixel level and utilizes the distinctive geometric properties of road lines and intersections to automatically extract road intersections from raster map. The inputs of our approach are raster map from a variety of sources without any auxiliary information such as the color of the roads, vector data, or legend information, etc. 

Geoinformatica 

a	b 

Fig. 4 Example input raster map. a USGS Topographic Map. b TIGER/Line Map 
Two examples of the input raster map are shown in Fig. 4. The outputs are the positions of the road intersection points, the number of roads that meet at each intersection as well as the orientation of each intersected road. 
For the problem of automatic extraction of road intersections from raster map, much of the previous work provides partial solutions [2], [6], [10], [11], [15]–[18], [24]–[26]. A majority of the work [2], [6], [15], [26] focuses on extracting text from map and the road lines are typically ignored in the process. Others such as Salvatore and Guitton [24] work on extracting and rebuilding the lines; however they rely heavily on prior learned colors to separate the lines for speciﬁc data sets, which is not generally useful in our automatic process. Habib et al. [11] assume the map contain only roads, and work on road lines directly to extract intersections. These previous approaches require manual interactions as they provide only partial solutions; and we build on some of the previous work in our framework to solve speciﬁc sub-problems. For example, to extract road lines from the foreground pixel, we incorporate Cao et al.’s algorithm [6] as the ﬁrst step to remove small connected objects such as characters from the map. Our approach is a complete solution that does not rely on prior knowledge of the input raster map. 
This paper is based on our earlier paper [7] of this work, and the new contributions of this paper are as follows: 
1. 	
We describe our approach in more detail (Section 2). 

2. 	
We describe how we utilize the Localized Template Matching (LTM) [4]to improve our algorithm (Section 2.6.3). We also report the similarity value generated by LTM to evaluate the quality of the extracted road intersection templates ( the road intersection, connectivity and orientations) (Section 3). 

3. 	
We present a more extensive set of experiments testing our approach on more map sources, and we also report the computation time with respect to the number of foreground pixel on a map (Section 3). 



Geoinformatica 

4. 	
We report our precision, recall, and F-measure on different positional accuracy levels (Section 3). 

5. 	
We report the results of a comparison on the same set of map sources  the approach in this paper against the approaches in our previous work [3]and an earlier paper of this work [7](Section 3). 


The remainder of this paper is organized as follows. Section 2 describes our approach to automatically extract road intersections. Section 3 reports on our experimental results. Section 4 discusses the related work and Section 5 presents the conclusion and future work. 
2 Automatic road intersection extraction 
The overall approach described in this paper is shown in Fig. 5. There are three major steps in our approach: 
1. 	
Automatic Segmentation We ﬁrst extract the foreground pixel from the raster mapasshowninFig. 6a and Fig. 6b. 

2. 	
Pre-Processing – Extract and Rebuild Road Layer After we obtain the fore.ground pixel, we further extract the road pixel by removing pixel which do not hold the properties to constitute roads (e.g., pixel for labels, contour lines, etc) as shown in Fig. 6c. Then we utilize a number of image processing operators to rebuild the extracted roads as shown in Fig. 6d. 

3. 	
Determine Road Intersections and Extract Connectivity with Road Orientation With the extracted roads, we detect possible road intersection candidates and utilize the number of roads that meet at an intersection candidate ( connec.tivity) and the road orientations to determine an actual road intersection. The extracted road templates are shown as black thick lines in Fig. 6e. 


The following sub-sections describe each component in detail. 
Fig. 5 Approach to extract road intersections 


Road Intersection Points with Connectivity and Orientation 
Geoinformatica 

Fig. 6 Remove the background then extract and rebuild the road layer from the raster map. a Original map. b Foreground objects. c Broken lines. d Restored lines. e Extract Intersections 

Geoinformatica 

2.1 Automatic segmentation 
In this step, the input is a raster map, and our goal is to extract the foreground pixel from the map. We utilize a technique called segmentation with automatically generated thresholds to separate the foreground from the background pixel. Many segmentation methods are discussed and proposed for various applications [23], and we implement a method that analyze the shape of the grayscale histogram and classify the histogram clusters based on their sizes. Our implementation is based on our observations of the color usage on raster map, which are: 
1. 	
The background colors of raster map have a dominant number of pixel; and the number of foreground pixel are signiﬁcantly smaller than the number of foreground background pixel. 

2. 	
The foreground colors have high contrast against the background colors. 


The ﬁrst step is to convert the original input raster map to an 8-bit grayscale ( 256 luminosity levels) image as shown in Fig. 7a. The conversion is done by computing the average value of the red, green and blue strength of each pixel for the grayscale level. The grayscale histogram of Fig. 7aisshowninFig. 8. In the histogram, the X-axis represents the luminosity level; from 0 (black) to 255 (white), and the Y-axis has the histogram values that represent the number of pixel for each luminosity level. We then partition the histogram into luminosity clusters and label the clusters as either background or foreground clusters. 
Since the background colors have a dominant number of pixel, we start from searching for the the global maximum on the histogram value to identify the ﬁrst background cluster. As shown in Fig. 8, we ﬁrst ﬁnd PEAK 1, then we check whether 

a	b 
Fig. 7 Before and after automatic segmentation (USGS Topographic Map). a Before (grayscale map). b After (binary map) 

Geoinformatica 

Fig. 8 Find Cluster 1’s left bound threshold  the triangle method 
PEAK 1 is closer to 255 (white) or 0 (black) in the histogram to determine which part in the histogram ( the portion of histogram to the left or right of PEAK 1) contains the foreground colors. If the peak is closer to 255, such as PEAK 1, 255 (white) is used as the right boundary of the ﬁrst background cluster. This is because the foreground colors generally have high contrast against the background colors. For example, if a light gray color of luminosity 200 is used as the background, it is common to ﬁnd the foreground colors spread in the histogram between 0 to 200 instead of between 200 to 255. As a result, for Cluster 1 in Fig. 8, we identify 255 as its right boundary, and its left boundary is located  the triangle method proposed by Zack [27], which will be discussed latter. On the other hand, if PEAK 3 is the global maximum, we will use 0 as its left boundary, and its right boundary is then located  the triangle method. 
After we ﬁnd the ﬁrst cluster, we search for next peak and use the triangle method to locate the cluster’s left and right boundaries until every luminosity level in the histogram belongs to some cluster. Any clustering algorithm could be used to ﬁnd the cluster boundary, and we use the triangle method proposed by Zack [27]for its simplicity. As shown in Fig. 8, to ﬁnd the left or right boundary, we construct a line called triangle line between the peak and the 0 or 255 end point and compute the distance between each Y-value and the triangle line. The luminosity level that has its Y-value under the triangle line and has the maximum distance is the cluster’s left/right boundary as the dotted line indicates in Fig. 8. If every luminosity level has its Y-value above the triangle line, then the 0 or 255 end point is used as the left or right boundary of the cluster. For example, if we try to ﬁnd the left boundary for Cluster 3  the triangle method, we will ﬁnd that every Y-value on the left of PEAK 3 is above the triangle line, so 0 is used for the left boundary of Cluster 3. 

Geoinformatica 

Table 1 Number of pixel in each cluster  Cluster 1  Cluster 2  Cluster 3  
Cluster pixel  316846  237876  85278  
Cluster pixel / total pixel  50%  37%  13%  

In our example, Cluster 1 is ﬁrst classiﬁed as the ﬁrst background cluster. The foreground colors usually have high contrast against the background colors, so the cluster which is the farthest from the ﬁrst background cluster in the histogram is the ﬁrst foreground cluster ( Cluster 3 in our example). For the remaining clusters, we compare the number of pixel in each cluster to the background cluster and the foreground cluster. If the number of pixel of a cluster is closer to the number of pixel of the background clusters than foreground clusters, it is classiﬁed as a background cluster; otherwise it is a foreground cluster. For example, as shown in Table 1, the number of pixel in Cluster 2 is closer to Cluster 1(background) than Cluster 3 (foreground), so Cluster 2 is classiﬁed as a background cluster. The idea is, if a cluster uses as many pixel as any of the background cluster, we do not want to include it in our result for two reasons. First, the cluster could be another color used in the background. Second, the cluster could be the color used to ﬁll up large objects on the map such as parks, lakes, etc. Since our goal is to look for road lines in the foreground pixel, the cluster should be discarded in either case. After each cluster is classiﬁed as background or foreground, we remove the background clusters ( convert the color of pixel in background clusters to white). An example of the resulting binary image is shown in Fig. 7b. 
There is no universal solution for the automatic segmentation problem. An edge detector is sometimes used instead of thresholding [11]; however, the edge detector is sensitive to noise, which makes it difﬁcult to use for our approach. Moreover, with the presence of characters, the edge detector makes the resulting characters bigger than the original ones since the edge detector produces two edges for a character line segment. Big characters have more overlap with road lines and hence severely sabotage the results of the next step that extract and rebuild the road layer. On the other hand, a simple thresholding algorithm can be applied to a wide-range of map sources. 
2.2 Pre-processing—extracting and rebuilding road layers 
After we separate the foreground from the background pixel, we have a binary raster map that contains multiple layers such as roads, labels, etc. In this module, we extract and rebuild the road layer from the binary map. In order to extract the road layer, we need to remove the objects that do not possess the geometric properties of roads. Without losing generality, our assumptions about roads on raster map are: 
1. 	
Road lines are straight within a small distance ( several meters). 

2. 	
The linear structures ( lines) in a map are mainly roads. The majority of the roads share the same road format—single-line roads or double-line roads, and the double-line roads have the same road width within a map. Examples of single-line and double-line map are shown in Fig. 9 



Geoinformatica 

Fig. 9 Single-line and double-line map. a Single-line map from TIGER/Line Map. b Double-line map from Google Map 
3. 	Unlike label layers or building layers, which could have many small connected objects, road lines are connected to each other as road networks and road layers usually have a small number of connected objects or even only one large connected object – the whole road layer is connected. 
Based on these assumptions, we classify the map into two format, the single-line map and double-line map, automatically by detecting the format of the majority of the roads on the map. If a map is classiﬁed as double-line road format (e.g., Google Map in Fig. 9b), we trace the double-line roads and remove single-line objects (e.g., contour lines, rivers, railroads, etc) from the map. For other map that have the majority roads in single-line format (e.g., TIGER/Line map in Fig. 9a), we classify them as single-line map and all linear structures on the map are preserved for further processing. Since we classify the map into either double-line map or single-line map, we might lose some roads in map with mainly double-line roads and some single-line roads. However, by identifying the major road format we dramatically removed many unwanted lines such as the contour lines. After we detect the road format and process the map, we remove text labels from the remaining foreground pixel and reconnect the broken lines. The following subsections describe our algorithm to automatically check the road layer format, to trace the road lines, and to rebuild the road layer. 
2.2.1 Double-line format detection and parallel-pattern tracing 
To detect the road format, we need to differentiate the geometric properties of single-line and double-line roads. We ﬁrst deﬁne one-pixel width straight lines in the grid domain ( the raster format). As shown in Fig. 10, there are four possible one-pixel width straight lines constituted by pixel C’s eight adjacent pixel, which represent the lines of 0.,45.,90., and 135. respectively. These four lines are the basic 

Geoinformatica 

Fig. 10 Basic single-line road elements 

elements to constitute other straight lines with various slopes starting from pixel C. For example, in Fig. 11, to represent a 30. line, one of the basic elements, a 45. line segment, is drawn in black, then additional gray pixel is added to tilt the line. Hence, by identifying these basic elements, we can identify straight lines in single-line format.Based on the four basic elements, if the pixel C is on a double-line roads shown in Fig. 12, there are eight different parallel-patterns of double-line roads ( the dashed cells are possible parallel lines). If we can detect any of the eight patterns for a given foreground pixel, we classify the pixel as a double-line road pixel. As shown in Fig. 12, we draw a cross at the pixel C  the size of the road width; and the cross locates at least two road line pixel on each pattern, one at the horizontal direction and the other one at the vertical direction. Therefore, if we know the road 


Geoinformatica 
Fig. 12 Basic double-line road elements 

width, we can determine whether a foreground pixel is on a double-line road layer by searching for the corresponding foreground pixel within a distance of road width (RW)6 in vertical and horizontal directions to ﬁnd the parallel pattern. Two examples are shown in Fig. 13. If a foreground pixel is on a horizontal or vertical road line as shown in Fig. 13a, we can ﬁnd two foreground pixel along the orientation of the road line within a distance of RW and at least another foreground pixel on the corresponding parallel road line in a distance of RW. If the orientation of the road 
6RW is used in this paper as a variable representing the road width in pixel. 

Geoinformatica 


Fig. 13 Double-line format checking and parallel-pattern tracing. a RW = 3. b RW = 4 
line is neither horizontal nor vertical as shown in Fig. 13b, we can ﬁnd one foreground pixel on both horizontal and vertical directions on the corresponding parallel road line in a distance of RW as shown in Fig. 13b. By tracing the parallel pattern, we are able to detect double-line road pixel; however, we still need to know: 
1. 	
The format ( single-line or double-line format) of the input road layer. If it is a single-line road layer, we skip this procedure. 

2. 	
The road width (RW). 


To detect the road layer format, we apply the parallel-pattern tracing algorithm on the binary map varying the road width from one to M pixel and remove foreground pixel which are not classiﬁed as road line pixel for a given road width. M is the maximum width in pixel of a double-line road we expect from the map. For example, on a 2 m/pixel map given M as 10 pixel, 20 m wide double-line roads are the widest double-line road we want to detect from the map. It is an input parameter for the user to ﬁne-tune the algorithm. A bigger M ensures that the roads on high resolution map can be found, but it requires more processing time. Since we do not know the map resolution, we use M equal to 10 in the following examples in this section. 
After we remove foreground pixel that are not detected as a road line pixel for the road width 1 to M, we then compute the ratio of the remaining foreground pixel divided by the original foreground pixel for each road width. For a given RW,ifthe majority of foreground pixel possess the double-line property, we classify the raster map as a double-line map and the road width is RW. 14 shows the results of this process of various double-line and single-line map. At the beginning of this process when the road width is one pixel, the remaining foreground pixel ratios are close to one (100%). After RW increases, the ratio starts to decline. This is because foreground pixel tend to be near each other, and it is easier to ﬁnd corresponding pixel even if the road width is incorrect or the map is not a double-line map. The peaks in the chart shown in Fig. 14a imply that the input raster map have double-line road layer and the road width is correct since most of the road pixel are not removed at the correct road width. For example, the high resolution ESRI Map has a peak at two pixel, the high resolution MapQuest Map, high resolution Yahoo Map, and USGS Topographic Map have a peak at four pixel in the chart as shown in Fig. 14a. ESRI map, MapQuest map which are not high resolution, and the TIGER/Line map are all single line map, which do not have any peak as shown in Fig. 14b. 

Geoinformatica 
Fig. 14 Double-line format checking. a Double-line format road layers. b Single-line format road layers 

 this method, we detect road layers in double-line format automatically and also obtain the road width by searching for the peak. For example, from Fig. 14a, we know the USGS Topographic map is a double-line map with road width equal to 4 pixel. Hence, we apply the parallel-pattern tracing algorithm setting RW to 4 pixel. The resulting image is shown in Fig. 15 with the contour lines removed. 
There are some exceptions to use parallel-pattern tracing to trace double-line pixel. As shown in Fig. 16, foreground pixel 1 to 8 are the example pixel which will be classiﬁed as a road line pixel  our parallel-pattern tracing algorithm, while pixel A to D are the example pixel which belong to the double-line road layer but will not be classiﬁed as a road line pixel. In Fig. 16, gray pixel are the corresponding pixel of pixel 1 to 8 in horizontal/vertical direction or on the corresponding parallel lines. Although after the parallel-pattern tracing, pixel A to D will be removed resulting in gaps between line segments, the majority of road pixel are detected and the gaps will be ﬁxed later when we rebuild the whole road layer. 
2.2.2 Text/graphics separation 
After we detect the road format and process the map according to its format, the remaining noise comes from the small connected objects, e.g. buildings, symbols, 

Geoinformatica 


Fig. 15 USGS Topographic Map before and after parallel-pattern tracing. a Before. b After 
characters, etc. The small connected objects tend to be near each other on the raster map, such as characters that are close to each other to form strings, and buildings that are close to each other on a street block. The text/graphics separation algo.rithms [2], [6], [10], [15]–[18], [25], [26] are very suitable for grouping and removing these types of objects. The text/graphics separation algorithms start by identifying small connected objects and then use various algorithms to search neighborhood objects in order to build object groups [25]. 
We apply Cao et al.’s algorithm described in [6] for text/graphics separation. Their algorithm ﬁrst removes small connected objects that do not overlap with other objects in the raster map as shown in Fig. 17b and then checks the length of each remaining line segment to determine if the line segment belongs to a graphic object. If a line segment is longer than a preset threshold, it is considered a graphic object ( a line); otherwise, it is a text object ( not a line). The identiﬁed text objects are shown in Fig. 17c, and the ﬁnal result after text/graphics separation is show in Fig. 17d. The broken road lines are inevitable after the removal of those objects touching lines, and we can reconnect them when rebuilding the road layer. 
With Cao et al.’s algorithm, we need to specify several parameters for the geometric properties of the characters, such as the size of one character and the 

Geoinformatica 
A B 3 2 1  3 3 2 3 2 1 2 1  4  3  4 4  4  5  C D  5 5  6  6 6  6  7  8 8  7 7  8  7  

Fig. 16 The exceptions in double-line format checking and parallel-pattern tracing (white cells are background pixel) 
maximum length of a word. Since we do not have the information to setup the algorithm for different map, we ﬁrst conduct several initial tests on a disjoint set of map and select a set of parameters to be used in our experiments. Our experiment shows that Cao et al.’s algorithm is very robust since the input parameters do not have to be exact. Also, the characters sizes of many computer generated raster map vary in a small range for users to read the map comfortably. For example, the map from Google Map at different zooming level and map from Yahoo Map have similar sizes of street labels. For the scanned map, the characters could be enormously enlarged/shrunk depending on the scan resolution; and the text/graphics separation algorithm will fail given that no additional geometric properties of the characters are provided. However, scanned map usually are scaled down for user-friendly viewing or to be displayed on the Internet, so it is not common that the input map is a scanned map with a extremely high/low scan resolution and enormously large/small characters. 
2.2.3 Rebuilding road layers: binary morphological operators 
In the previous steps, we extracted the road layer and created broken lines during the extraction. In this step, we utilize the binary morphological operators to reconnect the lines and ﬁx the gaps. Binary morphological operators are easily implemented  hit-or-miss transformations with various size masks [19], and are often used in various document analysis algorithms as fundamental operations [1]. The hit-or-miss transformation is performed in our approach as follows. We use 3-by-3 binary masks to scan over the input binary images. If the masks match the underlying pixel, it is a “hit”; otherwise, it is a “miss”. Each operator uses different masks to perform hit-or.miss transformations and performs different actions as a result of a “hit” or “miss”. We describe each operator in turn. 
2.3 Binary dilation 
The basic effect of a binary dilation operator is to expand the region of foreground pixel [19] and we use it to thicken the lines and reconnect adjacent pixel. As 

Geoinformatica 


Fig. 17 TIGER/Line map before and after text/graphics separation. a Binary TIGER/Line map. b Remove small connected components. c Identify non-line objects. d After text/graphics separation 
shown in Fig. 18, if a background pixel has any foreground pixel in its eight adjacent pixel ( a “hit”), it is ﬁlled up as a foreground pixel ( the action resulting from the “hit”). For example, Fig. 19 shows that after two iterations, the general dilation operator ﬁxes the gap between two broken lines. Moreover, if the roads are in double-line format, the two parallel lines are combined to a single line after the 


Geoinformatica 

general dilation operator as shown in Fig. 19. The resulting image after performing three iterations of the binary dilation operator on Fig. 20a is shown in Fig. 20b. The number of iterations determines the maximum gap size that we can ﬁx; the gaps smaller than six pixel are reconnected after performing three iterations of binary dilation operator and all road lines have become thicker. For double-line raster map, the number of iterations is selected based on the width of the roads in order to merge the two parallel lines. For single-line map, we utilize three iterations to ﬁx gaps smaller than six pixel, which is chosen based on our initial experiments. Smaller number of iterations prevent two different roads from merging together but result in more broken lines; while larger number of iterations ﬁx wide gaps but have the risk of merging two different roads. 
2.4 Binary erosion 
The idea of a binary erosion operator is to reduce the region of foreground pix.els [19]. We use it to reduce the thick lines and maintain an orientation similar to the original orientation prior to applying the binary morphological operators. If a foreground pixel has any background pixel in its eight adjacent pixel ( a “hit”), 


Geoinformatica 

Fig. 21 Erosion operator (black cells are foreground pixel) 


Fig. 22 The effect of the binary erosion operator (black cells are foreground pixel) 

Fig. 23 TIGER/Line map before and after erosion. a After dilation. b After erosion 


Geoinformatica 
it is converted to a background pixel ( the action resulting from the “hit”) as shown in Fig. 21. For example, Fig. 22 shows that after two iterations, the general erosion operator reduces the width of the thick lines. The resulting image after performing two iterations of the binary erosion operator on Fig. 23a is shown in Fig. 23b. 
2.5 Thinning 
After applying the binary dilation and erosion operators, we have road layers composed from road lines with various widths. But we need the road lines to have exactly one pixel width to detect interest points and the connectivity in the next step, and the thinning operator can produce the one pixel width results. The effect of the thinning operator is shown in Fig. 24. We do not use the thinning operator right after the binary dilation operator because the binary erosion operator has the opposite affect to the binary dilation operator, which prevent the orientation of road lines near the intersections from being distorted, as shown in Fig. 25.We utilize a generic thinning operator that is a conditional erosion operator with an extra conﬁrmation step [19]. The ﬁrst step of the thinning operator is to mark every foreground pixel that connects to one or more background pixel ( the same idea as the binary erosion operator) as candidate to be converted to the background. Then the conﬁrmation step checks if the conversion of the candidate will cause any disappearance of original line branches to ensure the basic structure of the original objects will not be compromised. The resulting image after performing the thinning operator on Fig. 26aisshowninFig. 26b. 
2.6 Determine road intersections, connectivity, and orientation 
In this step, we automatically extract road intersections, connectivity ( the number of roads that meet at an intersection), and the orientation of the roads inter.secting at each intersection from the road layer. In addition, we utilize the extracted information ( the position of the extracted intersection point, connectivity, and orientation) and the original map to verify and improve the results in the last step of this module. 


Geoinformatica 


Fig. 26 TIGER/Line map before and after thinning. a After erosion. b After thinning 
2.6.1 Detection of road intersection candidates 
With the roads from the preprocessing steps, we need to locate possible road intersection points. A road intersection point is a point at which more than two lines meet with different tangents. To detect possible intersection points, we start by  an interest operator. As the name interest operator implies, it detects “interest points” from an image as starting points for other operators to further work on. We use the interest operator proposed by Shi and Tomasi [22] and implemented in OpenCV7 to ﬁnd the interest points as the road intersection candidates. 
The interest operator checks the color variation around every foreground pixel to identify interest points, and it assigns a quality value to each interest point. If one interest points lies within the predeﬁned radius R of some interest points with higher quality value, it will be discarded. For example, consider Fig. 27, where pixel 1 to 5 are all interest points. With the radius R deﬁned as 5 pixel, salient point 2 is too close to salient point 1 which has a higher quality value. As a result, we discard salient point 2 while salient point 1 is kept as a road intersection candidate. Salient point 4 is also discarded, because it lies within the 5 pixel radius of salient point 3. Salient point 5 is considered as a road intersection point candidate, since it is not close to any other interest points with higher quality value. The radius R of 5 pixel is selected through experimentation. If we select a smaller radius, we will have more road intersection candidates; otherwise we will have fewer candidates. Consider the fact that road intersections are not generally near each other, the radius of 5 pixel is reasonable for processing map. These road intersection candidates are then passed to the next module for the determination of the actual road intersections. 
2.6.2 Filtering intersections, extracting road connectivity and orientation 
An interest point could be detected on a road line where the slope of the road suddenly changes. One example is the point 5 shown in Fig. 27. So we have to ﬁlter 
7http://sourceforge.net/projects/opencvlibrary, GoodFeaturesToTrack function. 

Geoinformatica 

Fig. 27 The interest points (black cells are foreground pixel) 
out the interest points that do not have the geometric property of an intersection point. Every road intersection point should have more than two line segments which meet at that point. The deﬁnition of intersection connectivity is the number of line segments intersecting at an intersection point, and it is the main criteria to ﬁlter road intersection points from interest points. 
We assume roads on raster map are straight within a small distance ( several meters). For each of the interest points detected by the interest operator, we draw a rectangle around it as shown in Fig. 28. The size of the rectangle is based on the maximum length in our assumption that the road lines are straight. In our example shown in Fig. 28, we use an 11-by-11 rectangle on the raster map with resolution 2 m/pixel, which means we assume the road lines are straight within 5 pixel or 10 m (e.g., on the horizontal direction, a line of length 11 pixel is divided into 5 pixel to the left, one center pixel and 5 pixel to the right). Although the rectangle size can vary for different raster map with various resolutions, we use a small rectangle size to assure even with lower resolution raster map, the assumption that road lines within the rectangle are straight is still valid. 
The connectivity of an interest point is the number of foreground pixel that intersects with this rectangle since the road lines are all single pixel width. If the connectivity is less than three, we discard the point; otherwise it is identiﬁed as a road intersection point. Subsequently, we link the interest points to the intersected foreground pixel on the rectangle boundaries to compute the slope ( orientation) of the road lines as shown in Fig. 29. 
In this step, we do not trace the pixel between the center pixel and the intersected pixel at the rectangle boundaries. In general, this step could introduce errors if the 


Geoinformatica 

Fig. 29 Construct lines to compute orientation 

intersected pixel are from other road lines which do not intersect at the center pixel or if the road lines within the rectangle are not straight. This usually happens in low-resolution map; however, in the general case, the rectangle is much smaller than the size of a street block, and it is unlikely to have other road lines intersect or have non-straight road lines. Moreover, we save signiﬁcant computation time by avoiding the tracing of every possible road line between the center and the rectangle box. 
2.6.3 Localized template matching 
LTM [5] is the last step in our approach, which enhances the accuracy of the extracted position, connectivity, and orientation. During the preprocessing steps to rebuild the road layer ( the binary morphological operators), we might shift the road lines from their original position or even create false branches. In order to ensure accurate results, we utilize the LTM to compare the extracted results with the original raster map and return a similarity value. 
For every extracted road intersection, we construct a template based on the road layer format, connectivity, and road orientation. For example, Fig. 30a shows a road intersection point with connectivity equal to 4, and the orientations of the intersected roads are 0, 90, 180, and 270 degree, respectively. If the raster map is a double-line map, we use the road width RW from the parallel-pattern tracing step with one pixel wide lines ( the black lines in the ﬁgures) to construct the template as shown in Fig. 30b; otherwise, we use one pixel width lines to construct a single-line template as showninFig. 30c ( the line width is the road width for single-line roads). After we have the template, we utilize the LTM function implemented in [5] to search locally from the position of the extracted intersection point as shown in Fig. 31. LTM will locate regions in the binary raster map that are most similar in terms of the geometry to the generated template. The outputs of LTM are the position of the matched template and a similarity value. If the similarity is larger than a pre-set threshold, we adjust the position of the intersection point; otherwise we discard the intersection point. As shown in Fig. 32, LTM adjusts the circled intersection points 

Geoinformatica 

a bc 
Fig. 30 Templates for different road format. a Extracted intersection. b Double-line format. c Single-line format 
to the precise location on the original raster map, and the arrows show the directions of the adjustments. The differences are only a few pixel, so the ﬁgures need to be studied carefully to see the differences. For example, in the upper-left circle of Fig. 32a, LTM adjusts the intersection several pixel lower and makes it match the exact intersection location on the original map; for the three circles on the bottom, LTM moves the intersections to their right for exact matches. 
3 Experiments 
In this section, we evaluated our approach by conducting experiments on raster map from various sources. Section 3.1 explains the test datasets and the initial parameter settings. Section 3.2 presents our evaluation methodology. Section 3.3 analyzes the experimental results and provide a comparison to our previous work [3]and an earlier paper of this work in [7]. Section 3.4 describes the execution platform and discusses the computation time. 


Geoinformatica 


Fig. 32 Results before/after the LTM (TIGER/Line Map). a Before LTM. b After LTM 

Geoinformatica 
3.1 Experimental setup 
We experimented with computer generated map and scanned map from 12 sources8 covering cities of the United States and some European countries. We ﬁrst arbi.trarily selected 70 detailed street map with a resolution range from 1.85 m/pixel to 7 m/pixel.9 In addition, we deliberately selected 7 low-resolution abstract map (resolutions range from 7 m/pixel to 14.5 m/pixel) to test our approach on more complex raster map that have signiﬁcant overlap between lines and characters. 
We did not use any prior information about the input map. Instead we used a set of default parameters for all the input map based on practical experience on a small set of data (disjoint with our test dataset in this experiment), which may not produce the best results for all raster map sources but demonstrate our capability to handle a variety of map. The size of small connected objects to be removed in the text/graphics separation step is set to 20-by-20 pixel. The number of iterations for the binary dilation and erosion operators on a single-line map are three and two respectively ( a gap smaller than six pixel can be ﬁxed). In the intersection ﬁltering and connectivity and orientation extracting step, we used a 21-by-21 pixel rectangular box (10 pixel to the left, 10 pixel to the right plus one center pixel). The similarity threshold for the LTM is 50%. We could optimize these parameters for one particular source to produce the best results if we incorporate prior knowledge of the sources in advance. 
3.2 Evaluation methodology 
The output of our approach is a set of road intersection positions along with the road connectivity and the orientations. We ﬁrst report the precision (correctness) and recall (completeness)10 for the accuracy of the extracted intersection positions. For the displacement quality of the results, we randomly selected two map from each source to examine the positional displacement. We also report the geometry similarity between the intersection templates we extracted and the original map to analyze the quality of the road connectivity and orientation. 
The precision (correctness) is deﬁned as the number of correctly extracted road intersection points divided by the number of extracted road intersection points. The recall (completeness) is deﬁned as the number of correctly extracted road intersection points divided by the number of road intersections on the raster map. The positional displacement is the distance in pixel between the correctly extracted road intersection points and the corresponding actual road intersections. Correctly extracted road intersection points are deﬁned as follows: if we can ﬁnd a road intersection on the original raster map within a N pixel radius of the extracted road intersection point, it is considered a correctly extracted road intersection point. Based on our practical experience, if an extracted intersection is within ﬁve pixel radius to any road intersections on the original map, it usually corresponds to 
8ESRI Map, MapQuest Map, TIGER/Line Map, Yahoo Map, A9 Map, MSN Map, Google Map, Map24 Map, ViaMichelin Map, Multimap Map, USGS Topographic Map, Thomas Brothers Map. 
9Some of the sources do not provide resolution information. 
10The terms precision and recall are common evaluation terminologies in information retrieval and correctness and completeness are often used alternatively in geomatics and remote sensing [12]. 

Geoinformatica 

an actual intersections on the original map but was shifted during the extraction; otherwise it is more likely a false-positive generated during our process to rebuild the roads. Thus, N equal to ﬁve is used as the upper bound to report our results. We report the precision and recall as we vary N from 0 to 5 for a subset of testing data in Section “3.3;” and we use N equal to 5 for any other places in the paper when precision and recall are mentioned. N represents the maximum positional displacements an application can tolerate. Different usages of the extracted inter.sections have different tolerance levels on the value of N. For example, consider an application that utilizes the extracted intersections as seed templates to search for neighborhood road pixel on aerial imagery [13], it needs as many intersections as possible; however, it is likely to have a lower requirement on the positional accuracy of the intersections ( N can be larger). On the other hand, a conﬂation system such as [5] requires higher positional accuracy ( a smaller N) to match the set of road intersections to another set of road intersections from another source. Road intersections on the original map are deﬁned as the intersection points of two or more road lines for single-line map or any pixel inside the intersection areas where two or more roads intersect for double-line map. 
The geometric similarity represents the similarity between the extracted intersec.tion template and the binary raster map, which is computed with LTM as described in Section 2.6.3 andusedin[4]. For an intersection template T with w x h pixel and the binary raster map B, the geometric similarity is deﬁned in as: 
.h .
w 

y=1 x=1 T(x, y)B(X + x, Y + y)
GS(T) = .(1) 
.h .w .w 

x=1 T(x, y)2 .hx=1 B(X + x, Y + y)2 
y=1 y=1 

where T(x,y) equals one, if (x,y) belongs to the intersection template; otherwise T(x,y) equals zero. B(x,y) equals one, if (x,y) is a foreground pixel on the binary raster map; otherwise B(x,y) equals zero. In other words, the geometric similarity is a normalized cross correlation between the template and the binary raster map which ranges from zero to one [4]. 
3.3 Experimental results 
We report the results with respect to the map sources as shown in Table 2.The average precision is 95% and the recall is 75% under the set of parameters discussed in Section 3.1. In particular, for map sources like the A9 Map, the Map24 Map, and the ViaMichelin Map, the precision is 100% because of the ﬁne quality of their map ( less noise, same width roads etc.). In our experiments the USGS Topographic map have the lowest precision and recall besides the low-resolution raster map. This is because USGS Topographic map contain more information layers than other map sources and the quality of USGS Topographic map is not as good as computer generated raster map due to the poor scan quality. The average geometric similarity of the extracted intersection template generated from LTM is 72%. We can improve the similarity if we track the line pixel when ﬁltering the intersections to generate the templates, but it will require more computation time. Our approach generates a set of representative features for applications that require high quality of corresponding features for their matching process. In [5], a map to imagery conﬂation system proposed by Chen et al. build on the results of our approach ( the set of 

Geoinformatica 
Table 2 Average precision, recall, and F-measure with respect to various sources 
Map source (number of test map) Precision (%) Recall (%) F-Measure (%) 
ESRI map (10) 93 71 81 MapQuest map (9) 98 66 79 TIGER/Line map (9) 97 84 90 Yahoo map (10) 95 76 84 A9 map (5) 100 93 97 MSN map (5) 97 88 92 Google map (5) 98 86 91 Map24 map (5) 100 82 90 ViaMichelin map (5) 100 98 99 Multimap map (5) 98 85 91 USGS Topographic map (10) 82 60 69 Thomas Brothers map (2) 98 65 79 
road intersection templates) and achieved their goal to identify the geospatial extent of the raster map and to align the map with satellite imagery. 
The value of positional displacements for two randomly selected map from each source is 0.25 pixel on average; and the Root Mean Squared Error (RMSE) is 0.82, which means the majority of extracted intersections are within one pixel radius of the actual intersections on the map. Some extracted intersections are still not on the precise original position since we construct the LTM template  1-pixel width roads instead of  the original road line width, which is unknown. As shown in Fig. 33, we also report the recall and precision with the positional displacement, N, varies from 0 pixel ( we extract the exact position) to 5 pixel. Intuitively, the precision and recall are higher when the N increases. For applications that do not 
100% 90% 80% 70% 60% 50% 40% 30% 20% 10% 0% 


Geoinformatica 

Table 3 Experimental results with respect to the resolution  
Precision (%)  Recall (%)  
Resolution higher then 7 m/pixel (70 map)  95  75  
Resolution lower then 7 m/pixel (7 map)  83  27  

require ﬁnding the exact location of the intersections, we can achieve higher precision and recalls. 
For comparison, we selected 7 low-resolution abstract map (resolutions range from 7 m/pixel to 14.5 m/pixel) to test our approach on more complex raster map that have signiﬁcant overlap between lines and characters. The experimental results of the 7 low-resolution abstract map compared to the set of 70 high-resolution map are shown in Table 3. The low-resolution map ( resolutions lower than 7 m/pixel) have signiﬁcantly lower average recall. This is because the characters and symbols touch the lines more frequently as shown in Fig. 34. During the preprocessing steps, we use a text/graphics separation program to remove the characters and labels, and it removes many of the road lines in a low-resolution map. Moreover, the size of street blocks on the low-resolution map are usually smaller than the window size we use in the intersection ﬁlter, which leads to inaccurate identiﬁcation of the road orientations. 
Finally, we tested our approach in this paper against an earlier paper of this work in [7] and the previous work of Chen et al. [3]  map from the same map sources ( ESRI, TIGER/Line, MapQuest, and Yahoo). The results are shown in Fig. 35. To our best knowledge, besides [7]and [3], the closest work we found is from Habib et al. [11], but they have very different assumptions of the input map than us (they assume the map contain only roads) and also there was no numeric results reported in the paper. We also conduct signiﬁcance tests on the comparison with the precision and recall of our previous work in [7] and this paper  T-distribution at p < 0.05 with the 95 test map. The precision is signiﬁcantly improved from [3] and slightly improved from our earlier paper on this work [7] (the difference is statistically signiﬁcant) resulting from the usage of LTM. The F-measure is also slightly improved from the previous work. For the quality of the intersection positions, we picked two 


Geoinformatica 
Fig. 35 Comparison with previous work  100%  
90%  
80%  
70%  
60%  
50%  
40%  
30%  
20%  
10%  
0%  


Chen et al. 04 ACM-GIS'05 paper on This paper this work 
map for each source to calculate the positional displacement. The RMSE of the positional displacement  the approach in this paper is 1.01, which is improved from 1.37 in [7] due to the use of LTM. The recall in [7] is slightly higher than the recall of this paper (the difference is not statistically signiﬁcant) since the LTM ﬁlters some correct intersections with incorrect orientation and connectivity templates. Two example results from our experiments are shown in Fig. 36. In these ﬁgures, an “X” marks the one road intersection point extracted by our system. 
3.4 Computation time 
Our test platform is an Intel Xeon 1.8 GHz Dual Processors server with 1 GB memory and the development tool is Microsoft Visual Studio 2003. The efﬁciency 


Geoinformatica 


0 pixel 20,000 40,000 60,000 80,000 100,000 120,000 140,000 pixel pixel pixel pixel pixel pixel pixel 
Fig. 37 Computation time is related to the number of foreground pixel 
of our approach depends on the computation time for each component, and the dominant factors are the map size and the number of foreground pixel in the raster map. As shown in Fig. 37, the computation time increases when the number of foreground pixel increases. Raster map that contain more information need more time than others. USGS Topographic map are the most informative raster map in our experiments, and it took less than one minute to extract the road intersections from an 800 . 600 topographic map with around 120 k foreground pixel on our testing platform. Other sources require fewer than 20 s on images of size less than 500 . 400. The LTM algorithm takes about 0.25 s to match an intersection point. 
4 Related work 
There is a variety of research on object recognition from raster map, such as road intersection extraction [11], contour line extraction [24], general object recogni.tion [17], [21], and text/graphics separation [2], [6], [10], [15]–[18], [25], [26]. 
Habib et al. [11] utilize several image processing algorithms to automatically extract road intersections from raster map. To extract the road layer, Habib et al. simply assume there are only road lines on the input raster map, and use an edge detector to separate the roads from the background. With the extracted road lines, Habib et al. utilize an interest operator to detect the corner points ( interest points in our paper) on the road edges and then search for corner point groups. The centroids of the resulting groups are the road intersection points. In this case, false-positive corner points or intersections of T-shape roads signiﬁcantly shift the centroids away from the correct locations. After the intersections are extracted, they extract the connectivity and orientations  manually identiﬁed knowledge of the 

Geoinformatica 
road format and the road width. In comparison, our approach detects the road layer format and road width automatically to rebuild the road layer. Moreover, the usage of LTM with the road width ensures the extracted intersections are on the original positions without manually verifying the results. 
Salvatore and Guitton [24] use a color classiﬁcation technique to extract contour lines from the topographic map. Their technique requires prior knowledge to generate a proper set of color thresholds for a speciﬁc set of map. However, in reality, the thresholds for different topographic map covering different areas may vary depending on the quality of the raster map. With our approach, we separate the contour lines from the roads by distinguishing their different geometric representations. In addition, the previous work has the goal to ensure the resulting contour lines have continuity similar to the original while our focus is on the road lines that are close to the intersections. 
Samet et al. [21] use the legends in a learning process to identify objects on the raster map. Meyers et al. [17] use a veriﬁcation based approach with map legends and speciﬁcations to extract data from raster map. These approaches both need prior knowledge ( legend layer and map speciﬁcation) of the input raster map, and the training process needs to be repeated when the map source changes. 
Finally, much research work has been performed in the ﬁeld of text/graphics separation from documents and map [2], [6], [10], [15]–[18], [25], [26], which is related to one of our step to extract and rebuild the road layer. Among the text/ graphics separation research, [2], [10], [26] assume that the line and character pixel are not overlapping and they extract characters by tracing and grouping connected objects. Cao et al. [6] detects characters from more complex documents ( characters overlap lines)  the differences of the length of line segments in characters and lines. Li et al. [15, 16] and Nagy et al. [18] ﬁrst separate the characters from the lines  connected component analysis and then focus on local area to rebuild the lines and characters  various methods. Generally, the text/graphics separation research emphasize on extracting characters, hence it provides a partial solution to our goal to extract the intersection templates. In Section 2.2.2,wedescribe how we incorporate the algorithm from [6] to remove the characters before we utilize the binary morphological operators to rebuild the roads. 
The main difference between our approach and the previous work on map problems is that the previous work requires additional information hence they provide partial solutions to the problem of automatic road intersection extraction. We assume a more general scenario to handle various map sources [9]; and our approach requires no prior knowledge while it still can be tuned with additional information, if available. 
5 Conclusion and future work 
The main contribution of this paper is to provide a complete framework to automati.cally and accurately extract the intersections from raster map. We also identify other valuable information such as the road format ( single-line format or double-line format) and road width to help the extraction process. 
Our approach achieves 95% precision and 75% recall on average when auto.matically extracting road intersections from raster map with resolution higher than 

Geoinformatica 

7m/pixel without any prior information. The result is a set of accurate features that can be used to exploit other geospatial data or to intergrate a raster map with other geospatial sources, thus creating an integrated view. For example, in [5], Chen et al. used the extracted intersections to align the map to imagery. Moreover, for a road extraction application, the georeferenced road intersections can be used as seed templates to extract the roads from imagery [13]. In the work in [9], Desai et al. applied our automatic intersection extraction technique on map returned from image search engines and successfully identify the road intersection points for geospatial fusion systems to identify the geocoordinates of the input map. 
There are three primary assumptions of our current approach. First, the back.ground pixel must be separable  the difference in luminosity level from the foreground pixel. This means that the background pixel must have the dominant color in the raster map. On certain raster map that contain numerous objects and the number of foreground pixel is larger than that of the background pixel, those foreground objects seriously overlap with each other making the automatic processing nearly impossible. Even if we can remove the background pixel on these raster map, removing noisy objects touching road lines results in too many broken road segments that are difﬁcult to reconnect. Second, although our approach works with no prior knowledge of the map scales, low-resolution raster map ( in our experiments, above 7 m/pixel) may result in low precision and recall. Third, as mentioned in Section 2.2.2, if the characters on the input map are signiﬁcantly different than our preset value, we cannot remove the character from lines and the results will have many incorrectly identiﬁed intersections. 
In future work, we plan to address these issues. First, we plan to exploit texture classiﬁcation methods [20] to handle those raster map in which the background color is not the dominate color such as tourist map. Second, although the text/graphics separation program performs well with one set of default parameters in our exper.iments, we still need to specify these parameters for each map to achieve the best results. Instead of tuning for every map, we want to utilize classiﬁcation methods in the frequency domain [8], [14] to separate line and character textures, which do not require any geometric parameters. Third, we want to further enhance the extracted road layer. In our approach, we do not focus on repairing the road network, but rather on rebuilding the roads close to the intersections. Hence we generate a comparatively coarse road layer from the original raster map. With the help of vectorization algorithms, we can further repair the road layer and generate the road vector data from the raster map. 
Acknowledgements This research is based upon work supported in part by the United States Air Force under contract number FA9550-08-C-0010, in part by the National Science Foundation under Award No. IIS-0324955, in part by the Air Force Ofﬁce of Scientiﬁc Research under grant number FA9550-07-1-0416, in part by a gift from Microsoft, and in part by the Department of Homeland Security under ONR grant number N00014-07-1-0149. The U.S. Government is authorized to repro.duce and distribute reports for Governmental purposes notwithstanding any copyright annotation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the ofﬁcial policies or endorsements, either expressed or implied, of any of the above organizations or any person connected with them. We would like to thank Dr. Chew Lim Tan for his generous sharing of their code in [6]. 

Geoinformatica 
References 
1. 
Agam G, Dinstein I (1996) Generalized morphological operators applied to map-analysis. In: The proceedings of the 6th international workshop on advances in structural and syntactical pattern recognition. Springer-Verlag, pp 60–69 

2. 
Bixler JP (2000) Tracking text in mixed-mode documents. In: The ACM conference on document processing systems, ACM, Santa Fe, New Mexico, pp 177–185 

3. 
Chen C-C, Knoblock CA, Shahabi C, Chiang Y-Y, Thakkar S (2004) Automatically and accu.rately conﬂating orthoimagery and street map. In: The 12th ACM international symposium on advances in geographic information systems, ACM, Washington, D.C., pp 47–56 

4. 
Chen C-C, Knoblock CA, Shahabi C (2006) Automatically conﬂating road vector data with orthoimagery. Geoinformatica 10(4):495–530 

5. 
Chen C-C, Knoblock CA, Shahabi C (2008) Automatically and accurately conﬂating raster map with orthoimagery. GeoInformatica (in press) 

6. 
Cao R, 	Tan CL (2001) Text/graphics separation in map. In: The 4th international work.shop on graphics recognition algorithms and applications, Kingston, Ontario. Springer Verlag, pp 167–177 

7. 
Chiang Y-Y, Knoblock CA, Chen C-C (2005) Automatic extraction of road intersections from raster map. In: The 13th ACM international symposium on advances in geographic information systems, Bremen, ACM 

8. 
Chiang Y-Y, Knoblock CA (2006) Classiﬁcation of line and character pixel on raster map  discrete cosine transformation coefﬁcients and support vector machines. In: The international conference on pattern recognition, Hong Kong, IEEE Computer Society 

9. 
Desai S, Knoblock CA, Chiang Y-Y, Desai K, Chen C-C (2005) Automatically identifying and georeferencing street map on the web. In: The 2nd international workshop on geographic information retrieval, Bremen, ACM 

10. 
Fletcher LA, Kasturi R (1988) A 	robust algorithm for text string separation from mixed text/graphics images. IEEE Trans Pattern Anal Mach Intell 10(6):910–918 

11. 
Habib AF, Uebbing RE (1999) Automatic extraction of primitives for conﬂation of raster map. Technical report. The Center for Mapping, The Ohio State University 

12. 
Heipk C, May H, Wiedemann C, Jamet O. (1997) Evaluation of automatic road extraction. In: The ISPRS Conference, vol 32, pp 3–2W3 

13. 
Koutaki G, Uchimura K (2004) Automatic road extraction based on cross detection in suburb. In: Image processing: algorithms and systems III. Proceedings of the SPIE, vol 5299, pp 337–344 

14. 
Keslassy I, Kalman M, Wang D, Girod B (2001) Classiﬁcation of compound images based on transform coefﬁcient likelihood. In: The international conference on image processing, Thessa.loniki, IEEE 

15. 
Li L, Nagy G, Samal A, Seth S, Xu Y (1999) Cooperative text and line-art extraction from a topographic map. In: Proceedings of the 5th international conference on document analysis and recognition, IEEE 

16. 
Li L, Nagy G, Samal A, Seth S, Xu Y (2000) Integrated text and line-art extraction from a topographic map. IJDAR 2(4):177–185 

17. 
Myers GK, Mulgaonkar PG, Chen C-H, DeCurtins JL, Chen E (1996) Veriﬁcation-based ap.proach for automated text and feature extraction from raster-scanned map. In: Lecture notes in computer science, vol 1072. Springer, pp 190–203 

18. 
Nagy G, Sama A, Set S, Fisher T, Guthmann E, Kalafala K, Li L, Sivasubraniam S, Xu Y (1997) Reading street names from map -Technical challenges. In: Procs. GIS/LIS conference, pp 89–97 

19. 
Pratt WK (2001) Digital image processing: PIKS inside, 3rd edn. Wiley-Interscience, New York 

20. 
Randen T, Husoy JH (1999) Filtering for texture classiﬁcation: a comparative study. IEEE Trans Pattern Anal Mach Intell 21(4):291–310 

21. 
Samet H, Soffer A (1994) A legend-driven geographic symbol recognition system. In: The 12th international conference on pattern recognition, IEEE, Jerusalem, vol 2, pp 350–355, October 

22. 
Shi J, Tomasi C (1994) Good features to track. In: The IEEE conference on computer vision and pattern recognition, IEEE, Seattle 

23. 
Sezgin M, Sankur B (2004) Survey over image thresholding techniques and quantitative perfor.mance evaluation. J Electron Imaging 13(1):146–165 

24. 
Salvatore S, Guitton P (2001) Contour line recognition from scanned topographic map. Techni.cal report. University of Erlangen 

Geoinformatica 


25. 
Tang YY, Lee S-W, Suen CY (1996) Automatic document processing: a survey. Pattern Recogn 29(12):1931–1952 

26. 
Velázquez A, Levachkine S (2003) Text/graphics separation and recognition in raster-scanned color cartographic map. In: The 5th international workshop on graphics recognition algorithms and applications, Barcelona, Catalonia 

27. 
Zack GW, Rogers WE, Latt SA (1977) Automatic measurement of sister chromatid exchange frequency. J Histochem Cytochem 25(7):741–753 





Yao-Yi Chiang is currently a Ph.D. student at the University of Southern California (USC). He received his B.S. in Information Management from National Taiwan University in 2000 and then his 
M.S. degree in Computer Science from the USC in December 2004. His research interests are on the automatic fusion of geographical data. He has worked extensively on the problem of automatically utilize raster map for understanding other geospatial sources and has wrote and co-authored several papers on automatically f map and imagery as well as automatic map processing. Prior to his doctoral study at USC, Yao-Yi worked as a Research Scientist for Information Sciences Institute and Geosemble Technologies. 

Craig A. Knoblock is a Senior Project Leader at the Information Sciences Institute and a Research Professor in Computer Science at the USC. He is also the Chief Scientist for Geosemble Tech.nologies, which is a USC spinoff company that is commercializing work on geospatial integration. He received his Ph.D. in Computer Science from Carnegie Mellon. His current research interests include information integration, automated planning, machine learning, and constraint reasoning and the application of these techniques to geospatial data integration. He is a Fellow of the American Association of Artiﬁcial Intelligence. 

Geoinformatica 

Cyrus Shahabi is currently an Associate Professor and the Director of the Information Laboratory (InfoLAB) at the Computer Science Department and also a Research Area Director at the NSF’s Integrated Media Systems Center at the USC. He received his B.S. in Computer Engineering from Sharif University of Technology in 1989 and then his M.S. and Ph.D. degrees in Computer Science from the USC in May 1993 and August 1996, respectively. He has two books and more than hundred articles, book chapters, and conference papers in the areas of databases, geographic information system (GIS) and multimedia. Dr. Shahabi’s current research interests include Geospatial and Multidimensional Data Analysis, Peer-to-Peer Systems and Streaming Architectures. He is currently an associate editor of the IEEE Transactions on Parallel and Distributed Systems and on the editorial board of ACM Computers in Entertainment magazine. He is also a member of the steering committees of IEEE NetDB and the general co-chair of ACM GIS 2007. He serves on many conference program committees such as VLDB 2008, ACM SIGKDD 2006 to 2008, IEEE ICDE 2006 and 2008, SSTD 2005 and ACM SIGMOD 2004. Dr. Shahabi is the recipient of the 2002 National Science Foundation CAREER Award and 2003 Presidential Early Career Awards for Scientists and Engineers. In 2001, he also received an award from the Okawa Foundations. 

Ching-Chien Chen is the Director of Research and Development at Geosemble Technologies. He received his Ph.D. degree in Computer Science from the USC for a dissertation that presented novel approaches to automatically align road vector data, street map and orthoimagery. His research interests are on the fusion of geographical data, such as imagery, vector data and raster map with open source data. His current research activities include the automatic conﬂation of geospatial data, automatic processing of raster map and design of GML-enabled and GIS-related web services. Dr. Chen has a number of publications on the topic of automatic conﬂation of geospatial data sources. 


An Approach to Automatic Road Vectorization of Raster map 

Yao-Yi Chiang and Craig A. Knoblock 
University of Southern California 
Department of Computer Science and Information Sciences Institute 
4676 Admiralty Way, Marina del Rey, CA 90292, USA 
E-mail: [yaoyichi, knoblock]@isi.edu 

1 Introduction 
Rater map are widely available for many areas around the globe. The road network that commonly exists in raster map is an important source of road vector data for the areas for which road vectors are not readily available. To extract the road vector data from raster map, the ﬁrst step is to extract the pixel that represent roads from the raster map ( the road raster layers in the raster map) and then extract the road vector data from the road pixel. Since the roads usually overlap with other map features and raster map very often contain noise introduced during the processes of image compression and scanning, the extraction of road pixel from raster map is difﬁcult. Moreover, for extracting the road vectors from road pixel, previous work commonly uses the thinning operator [6] or line grouping and parallel-line matching techniques [1] to obtain the skeletons of the connected objects composed of road pixel. The thinning operator is robust and requires no parameter tunings; however, the thinned lines ( the skeletons) are usually distorted around the line intersections and hence the extracted road vectors are not accurate, especially when the thinning operator is applied on thick lines. On the other hand, the line grouping and parallel-line matching techniques require manual settings on various parameters to identify the skeletons, such as the maximum difference between the slopes of two line segments to be merged. 
In this paper, we present an approach for extracting accurate road vector data from raster map with no parameter tunings. We ﬁrst utilize our previous techniques to extract the road pixel from raster map [4] and generate a road raster layer that contains one-pixel width road lines representing the skeletons of the connected objects composed of the extracted road pixel [5]. Next, we employ the road-intersection template extraction technique [3] to identify road intersections and extract accurate road-intersection templates from the road raster layer. Finally, we trace the pixel of the road raster layer for extracting the road vectors and use the accurate road-intersection templates to avoid the distortions caused by the thinning operator around each intersection. 
2 Road Pixel and Road Raster Layer Extraction 
To extract the road pixel from raster map, we utilize our technique in [4], which requires very little user input and is capable of handling various types of raster map, especially scanned map. Since raster map often contain thousands of colors, our technique [4] ﬁrst employs the Mean-shift and K-means segmentation methods to reduce the number of colors in the input raster map and then identiﬁes the road colors from road areas labeled by the users. With the identiﬁed road colors, we can generate a color ﬁlter to extract the road pixel.  1(a) shows a sample zoning map where the roads are not explicitly drawn but are shown as white areas between the parcels, and  1(b) shows the extracted road pixel (black). 
After we extract the road pixel, we identify the road format ( single-line or parallel double-line roads) and the road width  the parallel-pattern tracing technique and then use the morphological operations (e.g., the erosion and dilation operators) to remove solid areas and reconnect lines [2, 5]. The number of iterations for applying the morphological operations is selected  the automatically identiﬁed format and road width.  1(c) shows the results after we apply the morphological operations. To generate the road raster layer, we ﬁrst erode the thick lines shown in  1(c)  a binary erosion operator, which minimizes the distortions caused by the thinning operator that will be applied next. Then, the thinning operator is used to generate the road raster layer that contains one-pixel width road lines.  1(d) shows the extracted road raster layer. 

(a) An example map (b) Extracted road pixel (c) After morphological operations 

(d) 
Extracted road raster layer (e) Salient points and road intersections (f) Label the distortions 

(g) 
Extracted road-intersection templates 	(h) Extracted road vectors  1: Extract road vector data from an example map 



3 Road Vectorization 
To vectorize the road raster layer, we detect salient points on the one-pixel width road lines  the corner detector [7]. Since there are distortions around the road intersections, the extracted road vectors will not be accurate if we simply use the salient points as the connecting nodes of the road vectors. Therefore, we utilize our technique in [5] to detect road intersections among the salient points. For each salient point, we draw a rectangle around it and then use the number of line pixel that intersects with this rectangle as the connectivity to identify true road intersections. A road intersection must have at least three line segments meet at the intersection ( the connectivity is larger or equal to three).  1(e) shows the detected salient points as circles and the road intersections as X marks. To avoid the distortions and extract accurate road vectors, we utilize our technique in [3] to generate a blob image for labeling the locations of the distorted lines for each identiﬁed road intersection as the gray boxes ( the blobs) shown in  1(f). The size of each blob is decided by the detected road format and road width  the parallel-pattern tracing technique [5]. Then, we trace the lines that are not overlapping with the gray boxes to compute accurate orientations for each road that connects to a road intersection. The road orientations are used to update the positions of the road intersections.  1(g) shows the extracted road-intersection templates ( the road intersection positions, road connectivities, and road orientations), and the positions of the road-intersection templates now accurately locate on the medial lines of the thick lines. Finally, we trace the line pixel to connect the salient points and generate the topology of the road network. For the salient points that are identiﬁed as road intersections, we use the road-intersection templates to update the position of the salient point so that the road vectors around the road intersections are accurate.  1(h) shows the extracted road vectors. The road vectors around the road intersections are accurate despite the distortions of the one-pixel width road lines shown in  1(f). 
4 Related Work 
Itonaga et al. [6] present a method to extract road vectors from digitally generated map ( not scanned map). They employ a stochastic relaxation approach to ﬁrst extract the road areas and then apply the thinning operator to extract one-pixel width road network. The distortions around the road intersections are corrected based on the straightness of the roads, which is determined  user speciﬁed constraints, such as the road width and the differences between the slopes of road lines. In comparison, our approach can be used on a variety of raster map including scanned map, and we avoid the distortion  the automatically identiﬁed road-intersection positions, which requires no parameter settings. 
Bin et al. [1] work on scanned map to extract the road vectors. Instead of  the thinning operator, in [1], the medial lines of parallel road lines are ﬁrst produced and then linked to generate the road vectors. In general, the vectorization results of utilizing the medial lines can be very accurate for the vectors around the intersections, but the extraction processes require more manual speciﬁed parameters than the thinning operator, such as the thresholds to group medial-line segments and to produce the road intersections. 
5 Discussion 
In this paper, we employ the thinning operator to generate the skeletons of road pixel for extracting road vector data from raster map. We utilize the automatically extracted road-intersection templates to avoid the distortions around the intersections caused by the thinning operator. We show that our technique extracts accurate road vectors from an example raster map with no parameter tunings. 
6 Acknowledgement 
This research is based upon work supported in part by the University of Southern California under the Viterbi School Doctoral Fellowship, and in part by the United States Air Force under contract number FA9550-08-C.0010. 
The U.S. Government is authorized to reproduce and distribute reports for Governmental purposes notwith.standing any copyright annotation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the ofﬁcial policies or endorsements, either expressed or implied, of any of the above organizations or any person connected with them. 
References 
[1] D. Bin and W. K. Cheong. A system for automatic extraction of road network from map. In Proceedings of the 1998 IEEE International Joint Symposia on Intelligence and Systems, pages 359–366, 1998. 
[2] R. Cao and C. L. Tan. 	Text/graphics separation in map. In Proceedings of the Fourth International Workshop on Graphics Recognition Algorithms and Applications, pages 167–177. Springer-Verlag, 2002. 
[3] Y.-Y. Chiang and C. A. Knoblock. 	Automatic extraction of road intersection position, connectivity, and orientations from raster map. In Proceedings of the 16th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, pages 1–10, 2008. 
[4] Y.-Y. Chiang and C. A. Knoblock. A method for automatically extracting road layers from raster map. In Proceedings of the Tenth International Conference on Document Analysis and Recognition, 2009. 
[5] Y.-Y. Chiang, C. A. Knoblock, C. Shahabi, and C.-C. Chen. 	Automatic and accurate extraction of road intersections from raster map. GeoInformatica, 13(2):121–157, 2008. 
[6] W. Itonaga, I. Matsuda, N. Yoneyama, and S. Ito. Automatic extraction of road networks from map images. Electronics and Communications in Japan (Part II: Electronics), 86(4):62–72, 2003. 
[7] J. Shi and C. Tomasi. Good features to track. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 1994. 
Extracting Road Vector Data from Raster map 
Yao-Yi Chiang and Craig A. Knoblock 
University of Southern California, 
Department of Computer Science and Information Sciences Institute 
4676 Admiralty Way, Marina del Rey, CA 90292, USA 

{yaoyichi,knoblock}@isi.edu 
Abstract. Raster map are an important source of road information. Because of the overlapping map features (e.g., roads and text labels) and the varying image quality, extracting road vector data from raster map usually requires signiﬁcant user input to achieve accurate results. In this paper, we present an accurate road vectorization technique that minimizes user input by combining our previous work on extracting road pixel and road-intersection templates to extract accurate road vector data from raster map. Our approach enables GIS applications to exploit the road information in raster map for the areas where the road vector data are otherwise not easily accessible, such as the countries of the Middle East. We show that our approach requires minimal user input and achieves an average of 93.2% completeness and 95.6% correctness in an experiment  raster map from various sources. 
Key words: GIS, raster map, road vectorization, map processing 
1 Introduction 

Humans have a long history of  map. In particular, paper map have been widely used since the early years for documenting geospatial information. Because of the availability of low cost and high-resolution scanners and the Internet, we can now obtain a huge number of scanned map in raster format from various sources. Since map commonly contain road networks, raster map are an important source of road vector data for areas where road vector data are not readily available. Moreover, we can use the road vector data as features to register map to other geospatial data, such as imagery, and create an integrated view of heterogeneous geospatial data sets [3]. 
Extracting road vector data from raster map is a challenging task. First, the extraction of road pixel is di.cult since raster map very often contain noise from image compression and scanning processes and roads often overlap with other map features. Further, for converting the road pixel to vector format, the previous work commonly uses the thinning operator [11] or line grouping and parallel-line matching techniques [1] to identify the road centerlines. The thinning operator can produce distorted lines around intersections and hence the extracted road vector data are not accurate without manual adjustment [1]. The line grouping and parallel-line matching techniques require manual settings on various parameters to identify the accurate centerlines, such as the maximum di.erence between the slopes of two line segments to be merged [11] . 
In this paper, we present a general technique that requires minimal user input for extracting accurate road vector data from raster map with varying map complexity (e.g., overlapping features) and image quality. We exploit our previous work on extracting road pixel from raster map [5] and utilize the thin.ning operator to determine the road centerlines. We then automatically correct the distortions near road intersections caused by the thinning operator  our previous techniques on extracting accurate road-intersection templates from raster map [4; 6] to extract accurate road vector data. We tested our road vec.torization technique on a variety of map including scanned and digital map from di.erent sources and compared our results to a commercial map digitizing application. 
The remainder of this paper is organized as follows. Section 2 discusses related work on road extraction from map. Section 3 presents our approach to extract the road pixel from raster map. Section 4 describes our approach to generate the road vector data from the extracted road pixel. Section 5 reports on our experimental results, and Section 6 presents the conclusion and future work. 
2 Related Work 

Much research work has been performed in the ﬁeld of extracting road informa.tion from raster map, such as separating lines from text [2; 14], detecting road intersections [8], and extracting road vector data [1; 11] from raster map. In the previous work on text/graphics separation from raster map, Cao and Tan [2] and Li et al. [14] utilize a preset grayscale threshold to remove the background pixel from raster map and then detect text labels from the remaining fore.ground layers of the map, and the road pixel are the by-product ( only the road pixel are extracted) after the text pixel are identiﬁed. Since in their work, the main goal is to recognize the text labels, they do not process the raster map further to extract the road vector data. 
Some of the previous work assumes a simpler type of raster map for their algorithms. Habib et al. [8] extract road intersections from raster map that con.tain road lines only. Itonaga et al. [11] employ a stochastic relaxation approach to ﬁrst extract the road areas from digitally generated map ( not scanned map) and then apply the thinning operator to extract the road vector data. The distorted lines around the road intersections are corrected based on the straightness of the roads, which is determined  user speciﬁed constraints, such as the road width. In comparison, our approach can process a variety of raster map including scanned map, and we avoid the distortion with no pa.rameter settings. Bin et al. [1] work on scanned map to extract the road vector data. Instead of  the thinning operator, in [1], the medial lines of parallel road lines are ﬁrst produced and then linked to generate the road vector data. In general, the vectorization results of utilizing the medial lines of parallel road lines can be very accurate for the lines around the intersections, but the extrac.tion processes require more manually speciﬁed parameters than the thinning operator, such as the thresholds to group medial-line segments and to produce the road intersections. 
In addition to the research work, a commercial application called R2V from the Able Software is an automated raster to vector conversion software special.ized in digitizing raster map. To vectorize roads in raster map  R2V, the user needs to ﬁrst manually specify samples of road pixel or select a set of color threshold to identify the road pixel. The manual work on specifying samples of road pixel can be laborious, especially for scanned map with numerous colors, and the color thresholding function does not work if one set of threshold cannot separate all of the road pixel from other pixel. In comparison, our approach automatically identiﬁes road colors from a few user labels for extracting the road pixel. After the road pixel are extracted, R2V can automatically trace the centerlines of the extracted road pixel and generate the road vector data. Our approach detects the road format and road width automatically and uses the detected road information to extract accurate road vector data. In our ex.periments, we tested R2V  our test map and show that our automatic technique generates better results. 
3 Extracting Road pixel 

Distinct colors commonly represent di.erent layers ( a set of pixel represent.ing a particular geographic feature) in a raster map, such as roads, contour lines, and text labels. By identifying the colors that represent roads in a raster map, we can extract the road pixel from the map. However, raster map usually contain numerous colors due to scanning and/or compression processes and the poor con.dition of the original documents (e.g., color variation from aging, shadows from folding lines, etc.). For example,  1(a) shows a 200x200-pixel tile cropped from a scanned map. The tile has 20,822 distinct colors, which makes it di.cult to select the road colors manually. To overcome this di.culty, many techniques have been developed to ﬁrst group the colors of individual feature layers into clusters based on the assumption that the color variation within a feature layer is smaller than the variation between feature layers [5; 10; 12; 13]. Therefore, the feature layers can be extracted by selecting speciﬁc clusters. In this paper, we utilize our supervised map decomposition technique in [5] to extract the road pixel, which requires minimal user input and is capable of handling various types of raster map, especially scanned map. 
The supervised map decomposition technique ﬁrst employs two color quanti.zation techniques to reduce the number of colors in the raster map. To preserve object edges while clustering the colors in a raster map, we ﬁrst employ the Mean-shift algorithm [7], which merges two colors into one by considering their distance in the color space (we use a color distance of 25 in the red, blue, and green color space) as well as in the image space (we use a spatial distance of 3 pixel). The Mean-shift algorithm reduces the number of colors in  1(a) by 72% as shown in  1(b). To further merge similar colors in the raster map for reducing the user input to select the road colors, we apply the K-means algorithm with a user speciﬁed K to generate a quantized map image with at most K colors. The K-means algorithm can signiﬁcantly reduce the number of colors in a raster map by maximizing the inter-cluster color variance; however, since the K-means algorithm considers only the color space, it is very likely that the resulting map has merged features with a small K. For example,  1(c) shows the quantized map with K as 8 and the text labels have the same color as 

(a) An example tile (b) The Mean-shift result 

(c) The K-means result, K=8 (d) The K-means result, K=16 
Fig. 1. An example map tile and the color quantization results with color cubes 


(a) User labels centered at road lines (b) Extracted road pixel 
Fig. 2. Extracting road pixel  road color identiﬁed by analyzing user labels 
the road edges. Therefore, the user would need to select a larger K to separate di.erent features, such as in the quantized map in  1(d) with K as 16. 
With the quantized map, the user provides labels of road areas such as the two user labels shown in  2(a), and the map decomposition technique then exploits the fact that a user label is required to be centered at a road line or a road intersection to identify the road colors.  this approach, the user only has to provide enough user labels to cover each road color in the raster map, such as one for the white roads and one for the yellow roads in  2(a).  2(b) shows the extracted road pixel by  the road colors identiﬁed  these two user labels. 
4 Vectorizing Road pixel 

Once we have the road pixel, we generate the road vector data by ﬁrst deter.mine the road centerlines and then vectorize the centerlines.  3(a) and  3(b) show an example map tile from a scanned map and the road pixel extracted from the map  the approach described in the previous section. The extracted road pixel contain objects other than roads since they are drawn  the same color as roads. In addition, some of the road lines in the extracted road layer are broken since the missing pixel also belong to the text labels and grid lines ( overlapping features) and these pixel are not drawn  the road colors. To separate the non-road features from the road pixel, we exploit the distinctive geometric properties of road lines such as road lines are linear ob.jects and are connected, to remove solid areas and small connected-components. Next, we apply the closing operator to reconnect one-pixel wide gaps and ﬁll small holes. The closing operator ﬁrst expands the foreground areas by one pixel ( one iteration of the dilation operation) and then expands the background areas by one pixel ( one iteration of the erosion operation).  3(c) shows the results after we apply the closing operator, where the red circles show that some of the missing road pixel are ﬁlled if the missing parts are small, especially the places where the text labels overlap with roads. 
In order to reconnect broken lines with larger gaps automatically, we expand the areas of road pixel by utilizing the binary dilation operator as shown in  3(e). We determine the number of iterations of the dilation operator ( how far the foreground region should expend)  the road width and road format ( double-line and single-line roads) identiﬁed automatically by the Parallel-Pattern Tracing algorithm [6]. In a road layer where road lines are drawn as single lines ( single-line format) as the example shown in  3, the detected road width is the thickness of the majority of the road lines in the road layer as the dashed lines shown in  3(d). If a road line is drawn  two parallel lines ( double-line format), the road width is the pixel distance between corresponding road pixel on the parallel lines. During the thickening process, we also merge parallel lines into thick single lines if the road layer is double-line format. 
To generate the centerline representation of the thickened road lines, we apply the binary erosion operator and the thinning operator as shown in  3(f) and  3(g). We use the erosion operator to shrink the road areas before we apply the thinning operator because the thinning operator distorts lines near the intersections and the extent of the distortion depends on the thickness of the lines before the thinning operator is applied. Although the binary erosion operator helps to minimize the extent of the distortion caused by the thinning operator, the road geometry near the intersections is still not accurate, especially near T-shape intersections.  3(g) shows the distorted examples of the road geometry around a T-shape intersection and  3(h) shows an inaccurate results if the distorted lines are traced to generate the road vector data. 
For correcting the distortion around the intersection points and generating accurate road vector data from the thinned-line image ( 3(g)), we ﬁrst de.tect intersections of the thinned lines to mark potential distorted lines. We utilize the corner detector [15] to detect intersection candidates and then use the con.nectivity of the candidates to determine actual road intersections [6].  3(i) shows the intersection candidates in blue circles and the actual intersections with cross marks. Since the extent of the distortion around each intersection is determined by the thickness of the thickened lines (which is decided by the road width and the dilation operator), we can mark potential distorted thinned-lines 

(a) 
An example map (b) Extracted road pixel (c) Results of applying the closing operation 

(d) Road width (e) Thickened roads (f) Eroded roads 

(g) 
Thinned roads with distortion around intersections (h) Distorted results 

(i)
Salientpointsandroadintersections











(l) Straight-line patterns 	(m) Extracted road vector data 
Fig. 3. Extracting road vector data from an example map 

Fig. 4. Pseudo codes for tracing line pixel 
near an intersection point  a gray box with size as the thickness of the thickened lines as shown in  3(j). We then trace the lines outside the gray boxes to generate accurate road orientations and update the positions of the road intersections based on the intersecting roads and their orientations. Fig.ure 3(k) shows a portion of example extraction results. The road lines around the intersections are accurate despite the distortion of the thinned lines shown in  3(g). 
With the accurate positions of the road intersections and the knowledge of potential distorted areas, we start to trace the road pixel on the thinned-line image to generate the road vector data. The thinned-line image contains three types of pixel: the non-distorted road pixel, distorted road pixel, and background pixel, (as shown in  3(j), they are the black pixel not covered by the gray boxes, black pixel in the gray boxes, and white pixel, respectively). We create a list of connecting nodes (CNs) of the road vector data. A CN is a point where two lines meet at di.erent angles. We ﬁrst add the detected road intersections into the CN list. Then, we identify the CNs among the non-distorted road pixel  a 3x3-pixel window to check if the pixel has any of the straight-line patterns shown in  3(l). We add the pixel to the CN list if we do not detect a straight-line pattern since the road pixel is not on a straight line. 
To determine the connectivity between the CNs, we trace the road pixel  an eight-connectivity ﬂood-ﬁll algorithm shown in  4. The ﬂood-ﬁll algorithm starts from a CN, travels through the road pixel (both non-distorted and distorted ones), and stops at another CN. Finally, for the CNs that are road intersections, we use the previously updated road intersection positions as the CNs’ positions. The CN list and the determined connectivity are the results of our extracted road vector data.  3(m) shows the extracted road vector data. The road vector data around the road intersections are accurate since the distorted lines are not traced by the ﬂood-ﬁll algorithm and the intersection positions are updated  accurate road orientations. 
5 Experiments 

We evaluated our road vectorization approach  three raster map produced from di.erent sources. Two map are scanned map (350dpi) covering the city of 
Table 1. The number of colors in the image for user labeling of each tested map and the number of user labels for extracting the road pixel 
Tile Number  ITM Map  Gecko Map  UN Map  
1  2  3  4  5  6  7  8  9  10  1  2  3  4  5  6  7  8  9  10  
Colors  8  16  8  16  16  16  16  16  16  8  8  8  16  16  16  8  16  16  16  8  90  
User Labels  4  3  3  4  3  2  4  3  3  2  2  2  2  2  2  2  3  3  3  2  1  

Bagdad, Iraq published by Gizi map and International Travel map (ITM). We cropped and tested 10 map tiles (800x600 pixel each) from each of the scanned map. The paper map have been folded, and the folding lines cause inevitable shadows and color di.erences between areas in the scanned map, which enriches our test data since the cropped tiles from the same map have various color usages and image quality. In addition to the scanned map, we tested a digitally generated map covering Afghanistan published by the United Nations (UN).1 The digital map (3300x2550 pixel) shows the main and secondary roads, cities, political boundaries, airports, and railroads of the nation. We tested the digital map as a single tile in our experiments. For comparison, we also tested the automatic road vectorization function in R2V from the Able Software. 
We ﬁrst applied our supervised map decomposition technique described in Section 2 to extract the road pixel from the test map. We pre-processed the scanned map tiles  the Mean-shift and K-means algorithms with K as 8, 16, 24, and 32 to generate four quantized images for each map tile. The user started the user-labeling task from the quantized image containing eight colors. If the user cannot distinguish the road pixel from other map features (e.g., back.ground) in the quantized image, the user will then select an image containing more colors (a higher K) for user labeling. We did not apply the color segmen.tation algorithms on the digital map before user labeling. This is because the digital map contains a smaller number of colors ( 90 unique colors) and there is only one color representing both the major and secondary roads in the map. Table 1 shows the numbers of colors in the images used for user labeling and the numbers of user labels used for extracting the road pixel. The user-labeling task is the only process that requires user input in our experiments, and for all of the scanned map tiles, only two to four labels were needed. 
We tested R2V on extracting the road pixel from the test map. Since the scanned map contain numerous colors, we need more than one set of color thresholds to extract the road pixel (which R2V only allows one) or signiﬁcant user e.ort to manually specify sample pixel for each of the road colors. There.fore, we did not successfully extract the road pixel from the scanned map  R2V. For the digital map, we used one set of color threshold to extract the road pixel  R2V. 
For the extracted road vector data, we report the accuracy of the extrac.tion results  the road extraction metrics proposed in [9], which include the completeness, correctness, quality, redundancy, and the root-mean-square (RMS) di.erence. We manually drew the centerlines of every road lines in the 
1 http://unama.unmissions.org/ 

Table 2. Numeric results of the extracted road vector data from the scanned Gecko and ITM map (four-pixel-wide bu.er)  our approach 
ITM  Gecko  ITM  Gecko  ITM  Gecko  ITM  Gecko  ITM  Gecko  
Tile  Completeness  Correctness  Quality  Redundancy  RMS Di..  
1  98.7%  97.8%  96.7%  85.8%  95.5%  84.1%  0.07%  0%  2.34  1.69  
2  99.3%  97.4%  93.6%  97.5%  92.9%  95%  0%  0%  1.23  3.51  
3  98.1%  93.3%  75.8%  97.8%  74.7%  91.4%  0%  6.7%  1.52  2.46  
4  91.7%  97.2%  96.0%  98.7%  88.3%  96%  0%  1.73%  2.57  1.61  
5  92.0%  98.9%  94.7%  97.9%  87.5%  96.8%  0%  0%  2.79  1.32  
6  92.7%  88.6%  99.0%  90.2%  91.9%  80.1%  0%  0.41%  2.50  3.2  
7  97.5%  97.3%  99.2%  98%  96.7%  95.4%  3.34%  6.52%  1.81  1.65  
8  95.1%  93.4%  97.1%  94%  92.5%  88.2%  0%  0%  2.02  2.56  
9  93.7%  99.0%  94.6%  83.3%  88.9%  82.6%  0%  0%  2.21  1.54  
10  97.1%  98.7%  85.9%  94%  83.7%  92.9%  0.7%  1.7%  2.20  1.47  
Avg.  95.6%  96.2%  93.3%  93.7%  89.3%  90.3%  0.6%  1.7%  2.12  2.1  

map as the ground truth. The completeness and correctness represent how complete/correct the extracted road vector data are (the optimum is 100%). The quality is a combination metric of completeness and correctness (the op.timum is 100%). The redundancy shows the di.erence in percentage between the correctly extracted line and the matched ground truth (the optimum is 0). The RMS di.erence is the average distance between the extracted lines and the ground truth, which represents the geometrical accuracy of the extracted road vector data. To generate these metrics, the authors in [9] suggest  a bu.er width as half of the road width in the test data. In our test map, the roads are ﬁve and eight pixel wide in the digital map and are seven to ten pixel wide in the scanned map. We used a bu.er width of four pixel. 
Table 2 and Table 3 show the numeric results. The average completeness, correctness, and quality are around 90% to 96% and the average redundancy numbers are around 1% for the scanned and digital map.  5 shows some example results, where the geometry of the extracted road vector data are very close to the road centerlines in the map. Some broken lines are not connected (ca lower completeness numbers, such as the digital map) since the gaps are larger than the iterations of the dilation operations (we automatically detected the road format as single-line roads and used three iterations of the dilation operator to ﬁx the gaps smaller than six pixel). The broken lines could be reconnected with post-processing on the road vector data since the gaps are now smaller than they were in the extracted road layers resulting from the dilation operations. The tiles 3 and 10 of the ITM map and tiles 1 and 9 of the Gecko map have lower correctness since parts of non-road features are also extracted  the identiﬁed road colors and those parts contribute to false-positive road 
Table 3. Numeric results of the extracted road vector data from the UN digital map (four-pixel-wide bu.er)  our approach and R2V 
Tested Technique  Completeness  Correctness  Quality  Redundancy  RMS Di..  
This Paper  87.9%  99.9%  87.8%  0%  3.75  
Able R2V  76.1%  96.7%  74.2%  18.92%  3.91  

vector data.  5(a) to  5(c) show the ITM tile 3 where the runway is represented  the same color as the white roads and hence are extracted as road pixel. This type of false-positives could be further removed by including user validation steps after the road pixel were extracted. Some tiles have higher redundancy numbers such as the Gecko tiles 3 and 7, which is because some of the straight road lines in these tiles were extracted as shorter line segments with a small orientation variation and their bu.ers overlap with each other. The average RMS di.erences are under three pixel for scanned map and under four for the digital map, which shows that the thinning operator and our approach to correct the distortion result in good quality road geometry. Table 3 shows our approach achieved better results than R2V.2 The lower completeness of R2V is because R2V did not automatically connect broken road pixel. The lower correctness and high redundancy of R2V is because R2V generated small line segments instead of long and smooth lines and did not generate accurate road lines near the intersections. 
For the computation time, we built our test system  Microsoft Visual Studio 2008 running on a Windows XP Professional Virtual Machine installed on 
2.4 GHz Intel Core 2 machine with one GB of memory. The average processing time for vectorizing the road pixel for a map tile (800x600 pixel) is 13 seconds. The dominant factors of the computation time are the image size, the number of road pixel in the raster map, and the number of road intersections in the road layer. 
6 Conclusion and Future Work 
We present a general technique that extracts accurate road vector data from het.erogeneous raster map with minimal user input. We utilize our previous work [5] to handle raster map with varying image quality and exploit the accurate road-intersection templates [4; 6] to prevent distorted extraction results. We show that our technique extracts accurate road vector data from three raster map with varying color usage and image quality. In the future, we plan to test our approach on more map from various sources and test to include post-processing on the road vector data to improve the results. 
7 Acknowledgments 

This research is based upon work supported in part by the University of South.ern California under the Viterbi School Doctoral Fellowship, and in part by the United States Air Force under contract number FA9550-08-C-0010. The U.S. Government is authorized to reproduce and distribute reports for Governmental purposes notwithstanding any copyright annotation thereon. The views and con.clusions contained herein are those of the authors and should not be interpreted as necessarily representing the o.cial policies or endorsements, either expressed or implied, of any of the above organizations or any person connected with them. 
2 We used the “Auto Vectorize” function in R2V without manual post-processing (a) ITM tile 3 (b) Road pixel of (a) (c) Road vector data of (a) 


(d) 
ITM tile 6 (e) Road pixel of (d) (f) Road vector data of (d) 

(g) 
ITM tile 9 (h) Road pixel of (g) (i) Road vector data of (g) 

(j) 
Gecko tile 7 (k) Road pixel of (j) (l) Road vector data of (j) 





(m) UN map (portion) (n) Road pixel of (m) (o) Road vector data of (m) Fig. 5. Examples of the road vectorization results 
Bibliography 

[1] D. Bin and W. K. Cheong. A system for automatic extraction of road net.work from map. In Proceedings of the IEEE International Joint Symposia on Intel ligence and Systems, pages 359–366, 1998. 
[2] R. Cao and C. L. Tan. 	Text/graphics separation in map. In Proceedings of the Fourth GREC Workshop, pages 167–177, 2002. 
[3] C.-C. Chen, C. A. Knoblock, and C. Shahabi. Automatically and accurately conﬂating raster map with orthoimagery. GeoInformatica, 12(3):377–410, 2008. 
[4] Y.-Y. Chiang and C. A. Knoblock. Automatic extraction of road intersec.tion position, connectivity, and orientations from raster map. In Proceed.ings of the 16th ACM GIS, pages 1–10, 2008. 
[5] Y.-Y. Chiang and C. A. Knoblock. A method for automatically extracting road layers from raster map. In Proceedings of the Tenth ICDAR, 2009. 
[6] Y.-Y. Chiang, C. A. Knoblock, C. Shahabi, and C.-C. Chen. Automatic and accurate extraction of road intersections from raster map. GeoInformatica, 13(2):121–157, 2008. 
[7] D. Comaniciu and P. Meer. Mean shift: a robust approach toward feature space analysis. IEEE Transactions on PAMI, 24(5):603–619, 2002. 
[8] A. Habib, R. Uebbing, and A. Asmamaw. 	Automatic extraction of road intersections from raster map. Project Report submitted to the Center for Mapping, The Ohio State University, 1999. 
[9] C. Heipke, H. Mayer, C. Wiedemann, and O. Jamet. Evaluation of auto.matic road extraction. In International Archives of Photogrammetry and Remote Sensing, pages 47–56, 1997. 
[10] T. C. Henderson, T. Linton, S. Potupchik, and A. Ostanin. 	Automatic segmentation of semantic classes in raster map images. In the Eighth GREC Workshop, 2009. 
[11] W. Itonaga, I. Matsuda, N. Yoneyama, and S. Ito. Automatic extraction of road networks from map images. Electronics and Communications in Japan (Part II: Electronics), 86(4):62–72, 2003. 
[12] V. Lacroix. 	Automatic palette identiﬁcation of colored graphics. In the Eighth GREC Workshop, 2009. 
[13] S. Leyk and R. Boesch. Colors of the past: color image segmentation in his.torical topographic map based on homogeneity. GeoInformatica, 14(1):1– 21, 2010. 
[14] L. Li, G. Nagy, A. Samal, S. C. Seth, and Y. Xu. Integrated text and line-art extraction from a topographic map. IJDAR, 2(4):177–185, 2000. 
[15] J. Shi and C. Tomasi. Good features to track. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 1994. 
A Method for Automatically Extracting Road Layers from Raster map 
Yao-Yi Chiang and Craig A. Knoblock 
University of Southern California 
Department of Computer Science and Information Sciences Institute 
4676 Admiralty Way, Marina del Rey, CA 90292 
yaoyichi, knoblock@isi.edu 

Abstract 

To exploit the road network in raster map, the ﬁrst step is to extract the pixel that constitute the roads and then vectorize the road pixel. Identifying colors that represent roads in raster map for extracting road pixel is difﬁcult since raster map often contain numerous colors due to the noise introduced during the processes of image compression and scanning. In this paper, we present an approach that minimizes the required user input for identifying the road colors representing the road network in a raster map. We can then use the identiﬁed road colors to extract road pixel from the map. Our approach can be used on scanned and compressed map that are otherwise difﬁcult to process au.tomatically and tedious to process manually. We tested our approach with 100 map from a variety of sources, which in.clude 90 scanned map with various compression levels and 10 computer generated map. We successfully identiﬁed the road colors and extracted the road pixel from all test map with fewer than four user labels per map on average. 
1. Introduction 
Numerous map are available in raster format and are easily accessible from the Internet. For example, the scanned USGS topographic map can be downloaded from the Microsoft Terraserver and many other information rich raster map can be found in map repositories such as the University of Texas Map Library. 1 To utilize the informa.tion in the raster map, in our previous work, we developed a technology to ﬁrst identify the road intersection templates in the raster map [4] and then match the set of road inter.section templates with another set of road intersection tem.plates from a georeferenced data set (e.g., vector data) [2] to identify the geocoordinates of the map and align the map with the georeferenced data. For the automatic road intersection extraction process, in [4], we employed a his.togram analysis approach to extract the foreground pixel from the raster map and utilized a text/graphics separation 
1http://www.lib.utexas.edu/map/ 
algorithm [1] to extract the road lines. However, due to the complexity of map and noise introduced in producing the map in raster format (e.g., scanning), it is difﬁcult to sep.arate the foreground pixel from raster map automatically; even manual work requires tedious work to select the col.ors that represent the foreground pixel in a scanned map. Without properly extracting the foreground pixel, the road lines cannot be extracted since the text/graphics separation algorithm will not have a correct input to work with. 
In this paper, we present a supervised technique that re.quires minimum user input for extracting the road pixel from raster map. Our technique automatically identiﬁes the colors that represent roads in the raster map by analyz.ing user labels ( rectangular image areas labeled by the user). The only requirement of each user label is that the label needs to be approximately centered at a road intersec.tion or a road line segment. The user label does not have to contain only road pixel, which makes the user-labeling task easier and practical. The identiﬁed road colors are then used to generate a color ﬁlter to extract the road pixel from the raster map. The extracted road pixel can be reﬁned  simple morphological operations or sent to a text/graphics separation component to remove the characters if the road colors are also used to draw other map features, such as text. The remainder of this paper is organized as follows. Section 2 discusses the related work. Section 3 describes our approach to extract the road pixel from a raster map. Section 4 reports on our experimental results, and Section 5 provides the discussion and future work. 
2. Related Work 
Much research work has been performed in the ﬁeld of extracting graphic features from raster map, such as ex.tracting road lines and contour lines. To extract the graphic features, a binarization step is commonly used to ﬁrst ex.tract the foreground pixel, and there are a variety of algo.rithms that are then used to extract features from the fore.ground pixel. Our approach in this paper is a binarization step that can be used for road extraction if the road colors are only used for roads, which is generally more efﬁcient than analyzing the geometry of the extracted foreground pixel to extract roads [1, 9]. If the road colors are used on other map features, our method can be used to replace the binarization steps in existing text/graphics techniques to reduce the number of foreground pixel that need to be pro.cessed, which makes it possible to handle more complex map. In this section, we review different feature extraction techniques and focus on their binarization steps. 
Cao et al. [1] utilize a preset grayscale threshold to re.move the background pixel from raster map and then de.tect text labels from the remaining foreground layers of the map. Li et al. [9] utilize a ﬁxed color ﬁlter to extract the “black layer” from the USGS topographic map and then work on the black layers to extract and rebuild the text labels and lines. Pouderoux et al. [10] also use a preset grayscale threshold to generate a binary map for their toponym recog.nition algorithm, and the authors also note that additional methods such as the K-means color segmentation should be adopted to process more complex map. Habib et al. [7] work on the raster map which contain only road lines to extract road intersections, and hence their binarization step is simply an edge detector. These approaches each handle a speciﬁc type of map that can be processed by their de.fault binarization methods. Our extracted road pixel can be used as the input to these feature extraction techniques that employ simpler binarization steps to further extract speciﬁc graphic features (e.g., road vectors or road intersections) or to reﬁne the road lines if needed. 
In our previous work [4], we utilize a histogram anal.ysis method to automatically separate the foreground pix.els from the raster map and then identify the road pixel and reconnect the road lines. Although the histogram analy.sis method does enable us to generate different binarization thresholds for different map, the histogram analysis does not handle scanned map well since the noise introduced in the scanning process is sometimes difﬁcult to remove auto.matically. Our method in this paper includes user training and is capable of handling more diverse types of map, es.pecially scanned map. 
Other techniques include user training in their binariza.tion steps. Salvatore and Guitton [11] use color classiﬁca.tion methods as their ﬁrst step to extract contour lines from topographic map. Khotanzad et al. [8] utilize a color seg.mentation method with user annotations to extract the con.tour lines from the USGS topographic map. Chen et al. [3] extended the color segmentation method in [8] to handle common topographic map ( not limited to the USGS topographic map)  local segmentation techniques. 
These techniques with user training are generally able to handle more complex map. However, their user train.ing processes are complicated and laborious. In [11], their technique requires manual examinations on the input map to generate a proper set of color thresholds for a speciﬁc set of map. In practice, a scanned topographic map can have thousands of colors and the thresholds for different to.pographic map may vary depending on the quality of the map, which makes the manual examination repetitive work. In [3, 8], the users are required to labeled all combinations of line and background colors. For comparison, our ap.proach in this paper requires that the user label only a few road areas, which is simpler and more straightforward. 
3. Automatic Extraction of Road pixel from Raster map 
The input of our technique for extracting the road pixel is either a scanned map or a digital map (map generated directly from vector data), and the output is the road pixel of the raster map. We ﬁrst apply color segmentation algo.rithms to quantize the input image. The color segmentation algorithms reduce the number of colors that we need to ver.ify for identifying the road colors in the next step. Then, the user labels several road areas in the quantized map. From the user labels, we identify a set of road colors and gener.ate a color ﬁlter to extract the road pixel from the raster map. The details of each step and the labeling criteria are described in the following subsections in turn. 
3.1. Color Segmentation 
Raster map usually contain numerous colors. For exam.ple, the sample raster map shown in  1(a) has 285,735 colors, which is difﬁcult for identifying road colors in the latter steps. The two color segmentation techniques we use to reduce the number of colors in the input image are the Mean-shift [5] and the K-means [6] algorithms. 
The Mean-shift algorithm merges two colors into one by considering their distance in the color space (we use a color distance of 25 in the red, blue, and green color space in our method) as well as in the image space (we use a spatial dis.tance of 3 pixel in our method), which helps to preserve object edges while performing the color segmentation. Fig.ure 1(c) shows a portion of the sample map where the num.ber of colors is reduced by 50% ( 155,299 colors) by applying the Mean-shift algorithm. Next, we apply the K-means algorithm to ensure that the ﬁnal quantized map has a number of colors that is not larger than K (we use K=10 in our method).  1(d) shows a portion of the sample map where there are only 10 different colors remaining after applying the K-means algorithm. 
3.2. User Labeling 
The number of colors in the quantized map is less than or equal to 10 after the color segmentation ( K=10), so the number of road colors is lower than 10. In this user-labeling step, the user needs to provide a user label for each road color in the quantized map. A user label is a rectangle 


(a) An example scanned map 


(b) The original map 	(c) Use the Mean-shift 
algorithm only 

 1. An example map and the quantized 
map with their grayscale histograms 
that should be large enough to cover a road intersection or a road segment. To label the road colors, the user ﬁrst selects the size of the label. Then, the user clicks on the approxi.mate center of a road line or a road intersection to indicate the center of the label. The user label should be (approxi.mately) centered at a road intersection or at the center of a road line, which is the constraint we exploit to identify the road colors in the next step. For example, the right side of  2 shows a typical labeling result to extract the road pixel from the quantized map shown in the left. The two user labels cover one road intersection and one road seg.ment, which contain the two road colors ( yellow and white) in the quantized map. 

3.3. Road Colors Identiﬁcation 
To identify the road colors, we ﬁrst decompose each user label into color images so that every color image each con.tains only one color from the user label. For example, the user label shown in the top-right of  2 is decomposed into six different color images shown in the top of Fig.ure 3. For each color image, we extract the skeletons of every connected object in the image  the thinning op.erator, and apply the Hough transformation [6] to identify a set of Hough lines from the connected objects’ skeletons. Since the image center of a user label is the center of a road line or a road intersection, we compute the average distance 


 2. The quantized map and user labels 

between the detected Hough lines to the image center for each color image as a measure to determine if the color pix.els ( non-black pixel) in an image represent roads in the raster map. 
 3 shows the detected Hough lines of each image, where the Hough lines that are within a distance threshold to the image centers are drawn in red and others are drawn in blue (this distance threshold is only used in the paper to help explain the idea). We can see that the images that con.tain road pixel (image 3, 4, and 5) have more red lines than blue lines and hence the average distances between their Hough lines to their image centers are smaller than the other images. Therefore, the image that has the smallest average distance is ﬁrst classiﬁed as the road-pixel image ( the color pixel in the image are road pixel in the raster map). The other color images that have their average distances within one pixel to the smallest average distance are also classiﬁed as road-pixel images. This criterion allow the user label to be a few pixel off (depending on the size of the user label) from the actual center of the road line or road intersection in the map, which makes the user labeling easier. In our example, image 5 has the smallest average distance, so image 5 is ﬁrst classiﬁed as a road-pixel image. Then, since image 4 is the only image that has its average distance within a one-pixel distance to the smallest average distance, image 4 is also classiﬁed as a road-pixel image. 
The Hough transformation method relies on the num.bers of detected Hough lines so if a color that is used on roads has a smaller number of pixel compared to the ma.jor road colors, such as the image 3 shown in  3, the image will not be classiﬁed as a road-pixel image us.ing the Hough transformation method. Therefore, we em.ploy a template matching algorithm to check if any of the color images that are not classiﬁed as road-pixel images us.ing the Hough transformation method ( image 0 to 3) is a road-pixel image. We ﬁrst generate a road template image  the already classiﬁed road-pixel images. In our exam.ple, the road template image is the combination of image 4 and 5 as shown in  4. In the road template image, the color pixel are road pixel and the black pixel are the 




 3. The color images and identiﬁed Hough lines (background is shown in black) 



 4. The road template image 
background. Next, the road template image is used to eval.uate image 0 to 3 iteratively. For a color pixel, C(x, y), in a given color image to be evaluated, we search a 3-by-3 pixel neighborhood centered (x, y) in the road template image to detect if there exists any road pixel ( color pixel). If one or more road pixel are detected, the pixel C(x, y) is marked as a road pixel since it is spatially near one or more road pixel. After every color pixel in a given color image is examined, if more than 50% of the color pixel in that im.age are marked as road pixel, the given image is classiﬁed as a road-pixel image. 
Every user label is processed  the method described in this section and a set of road-pixel image is identiﬁed for each label. The colors in the road-pixel images ( the identiﬁed road colors) are then used in the next step to generate a color ﬁlter. 
3.4. Road Pixel Extraction 
We generate a color ﬁlter  all identiﬁed road colors and then scan the quantized map to extract the road pixel. The colors in the quantized map that are not in the color ﬁlter are ﬁlled with white pixel and the others are ﬁlled up with black pixel.  5(a) shows the extracted road pixel of  1(a). The extracted road pixel then can be reﬁned  morphological operations (e.g., the erosion and dilation operators) to remove solid areas and reconnect lines as shown in  5(b). 
4. Experimental Setup and Results 
We tested our approach on seven sets of map from var.ious sources. The ﬁrst three sets of map were 30 topo.graphic map (600x600 pixel each) from USGS covering three different U.S. cities. The USGS topographic map are commonly used in the experiments of map feature extrac.tion research [9, 8], which contain noise introduced during the map production processes, and different areas of the to.pographic map have very different degrees of noise and 
 5. Road extraction results 

different color usages. In addition to the USGS map, we extended our experiments to test our technique on process.ing commonly accessible scanned map  three sets of 60 map (2000x2000 pixel each) cropped from three dif.ferent scanned map (350dpi) covering Bagdad, Iraq. The three scanned map were published from different publish.ers2 and different legends were used in the map. Moreover, since the original paper map have been folded, there were folding lines on the paper map that caused inevitable shad.ows and color differences on different areas of the scanned raster map. Therefore, even the map cropped from the same scanned map have various color usages. The last set of map were 10 digital map (1500x1500 pixel each) from Rand McNally covering St. Louis, U.S., which were tested in our experiment because the digital map have white back.ground and a majority of roads on the map have light col.ors. As a result, the roads of light colors are usually misclas.siﬁed as background pixel  our automatic approach, which analyzes the grayscale histogram [4]. 
In the experiment, the user utilized our implemented sys.tem to label road areas in each test map for extracting the road pixel from the map. Our implemented system gen.erated a road extraction result after every user labeling, and the user can preview the result in real-time to decide if more labeling needs to be done for a better extraction result. A successful extraction is where the user can use our imple.mented system to extract the majority of the road colors in a raster map without examining the image histogram or se.lecting individual colors. The majority of the road colors means that if not every road color in the raster map is iden.tiﬁed, the road topology can still be rebuilt  morpho.logical operations (e.g., the erosion and dilation operators).  5 shows a typical result with the majority of road colors were identiﬁed  our system and the result after morphological operations to remove solid areas and small connected objects. 
We successfully extracted the road pixel from all test map, and the average number of user labels per map for 
2The paper map are produced by Gizi map, Gecko map, and Inter.national Travel map 
Map Set  Map Count  Avg. User Labels  Avg. Computation Time (s.)  
1. USGS topo. map  19  1.5  1.5  
2. USGS topo. map  5  1.6  1.2  
3. USGS topo. map  6  1.8  1.8  
4. Scanned map  12  2.7  2.7  
5. Scanned map  18  2.1  3.3  
6. Scanned map  30  2.6  4.2  
7. Rand McNally map  10  3.9  2.4  

Table 1. Experimental results of average user 

labels and computation time per map 
every test set was under four as shown in Table 1. After the automatic extraction process, the user can then examine the results and decide if any post-processing needs to be ap.plied. The extraction results of the scanned Iraq map need only morphological operations for post-processing; the op.erations can be found in the implementation of [1]. The ex.traction results of the Rand McNally and USGS topographic map require text/graphics separation techniques to further remove the characters, since their road colors are also used for characters. For the USGS topographic map, many of the extraction results include contour lines. This is because the brown pixel in the USGS topographic map are used on the contour lines and roads. The contour lines can be removed by the parallel-pattern tracing method in [4]. 
The system was built  Microsoft Visual Studio 2005 on a Windows 2003 Server with Intel Xeon 1.8 GHz Dual Processors and 1 GB of memory. Tabel 1 shows the average computation time per map in each set. The computation time was the computer time needed to process each user label and produce the extraction result for an input map, which mainly depended on the size of the map and the num.ber of labels. The average computation time for a map was below ﬁve seconds, which allows us to generate a real-time preview of the result after each user labeling. 
5. Discussion 
In this paper, we present an approach to automatically extract road pixel from raster map. Our approach exploits the constraint that the user labels are centered at road inter.sections or road lines to identify road colors for extracting the road pixel from the raster map automatically. The ex.periments show successful road-pixel extractions from all of the 100 test map with less than four user labels per map in average. The extraction results enable feature extraction techniques such as the road intersection extraction or road vectorization to handle more diverse types of map. In the future, we plan to utilize a content-based image retrieval ap.proach to automatically reuse the identiﬁed color ﬁlters for road extraction task on similar map. 
6. Acknowledgement 
This research is based upon work supported in part by the University of Southern California under the Viterbi School Doctoral Fellowship, in part by the United States Air Force under contract number FA9550-08-C-0010. 
The U.S. Government is authorized to reproduce and dis.tribute reports for Governmental purposes notwithstanding any copyright annotation thereon. The views and conclu.sions contained herein are those of the authors and should not be interpreted as necessarily representing the ofﬁcial policies or endorsements, either expressed or implied, of any of the above organizations or any person connected with them. 
References 
[1] R. Cao and C. L. Tan. 	Text/graphics separation in map. In Proceedings of the Fourth International Workshop on Graphics Recognition Algorithms and Applications, pages 167–177. Springer-Verlag, 2002. 
[2] C.-C. Chen, C. A. Knoblock, and C. Shahabi. 	Automat.ically and accurately conﬂating raster map with orthoim.agery. GeoInformatica, 12(3):377–410, 2008. 
[3] Y. Chen, R. Wang, and J. Qian. 	Extracting contour lines from common-conditioned topographic map. IEEE Trans.actions on Geoscience and Remote Sensing, 44(4):1048– 1057, 2006. 
[4] Y.-Y. Chiang, C. A. Knoblock, C. Shahabi, and C.-C. Chen. Automatic and accurate extraction of road intersections from raster map. GeoInformatica, 13(2):121–157, 2008. 
[5] D. Comaniciu and P. Meer. 	Mean shift: a robust approach toward feature space analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 24(5):603–619, 2002. 
[6] D. A. Forsyth and J. Ponce. 	Computer Vision: A Modern Approach. Prentice Hall Professional Technical Reference, 2002. 
[7] A. Habib, R. Uebbing, and A. Asmamaw. Automatic extrac.tion of road intersections from raster map. Technical report, Center for Mapping, 1999. 
[8] A. Khotanzad and E. Zink. Contour line and geographic fea.ture extraction from usgs color topographical paper map. 
IEEE Transactions on Pattern Analysis and Machine Intelli.gence, 25(1):18–31, 2003. 

[9] L. Li, G. Nagy, A. Samal, S. C. Seth, and Y. Xu. Integrated text and line-art extraction from a topographic map. In.ternational Journal of Document Analysis and Recognition, 2(4):177–185, 2000. 
[10] J. Pouderoux, J. C. Gonzato, A. Pereira, and P. Guitton. To.ponym recognition in scanned color topographic map. In 
Proceedings of the Ninth International Conference on Doc.ument Analysis and Recognition, volume 1, pages 531–535, Sept. 2007. 

[11] S. Salvatore and P. Guitton. Contour line recognition from scanned topographic map. In Proceedings of the Winter School of Computer Graphics, 2004. 
Classiﬁcation of Raster map 
for Automatic Feature Extraction 

Yao-Yi Chiang and Craig A. Knoblock
University of Southern California 
Department of Computer Science and Information Sciences Institute 
4676 Admiralty Way, Marina del Rey, CA 90292 
yaoyichi, knoblock@isi.edu 

ABSTRACT 
Raster map are widely available and contain useful geo.graphic features such as labels and road lines. To extract the geographic features, most research work relies on a man.ual step to ﬁrst extract the foreground pixel from the map  the distinctive colors or grayscale intensities of the pixel. This strategy requires user interaction for each map to select a set of thresholds. In this paper, we present a map classiﬁcation technique that uses an image compari.son feature called the luminance-boundary histogram and a nearest-neighbor classiﬁer to identify raster map with sim.ilar grayscale intensity usage. We can then apply previ.ously learned thresholds to separate the foreground pixel from the raster map that are classiﬁed in the same group instead of manually examining each map. We show that the luminance-boundary histogram achieves 95% accuracy in our map classiﬁcation experiment compared to 13.33%, 86.67%, and 88.33%  three traditional image compar.ison features. The accurate map classiﬁcation results make it possible to extract geographic features from previously unseen raster map. 
Categories and Subject Descriptors 
H.2.8 [Database Management]: Database Applications— 
Spatial Databases and GIS 
General Terms 
Algorithms, Design 
Keywords 
Raster Map Classiﬁcation, Content-Based Image Retrieval, Image Similarity, Luminance-Boundary Histogram, Color-Coherence Vectors, Color Histogram, Color Moments 
1. INTRODUCTION 
Due to the popularity of Geographic Information System (GIS) and high quality scanners, we can now obtain more 
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. ACM GIS ’09 Seattle, WA USA Copyright 2009 ACM 978-1-60558-649-6/09/11 ...$5.00. 
and more raster map from various sources on the Internet. For example, the United States Geological Survey (USGS) distributes various types of scanned map, such as topo.graphic map and thematic map displaying water resources through their own website1 and the TerraServer-USA web.site.2 Map repositories such as the University of Texas Map Library3 contain information rich scanned map for many areas around the globe, including historical scanned map. Moreover, in our previous work, we developed an automatic approach to mining collections of map from the Web [13]. In that work, we harvest images from image search engines (e.g., Yahoo Image Search) and then identify raster map among the images. 
Raster map are an important source of geospatial infor.mation. First, raster map provide geographic features that are di.cult to ﬁnd elsewhere, such as the landmarks in his.torical map. Moreover, for certain types of geographic fea.tures, raster map contain the most complete set of data, such as the USGS topographic map that have the contour lines of the entire United States in various scales. However, because of the variety of image quality (e.g., poor image quality from scanning and/or image compression processes), the complexity of map, and the typical lack of metadata (e.g., map geocoordinates, map source, original vector data, etc.), it is di.cult for computers to automatically extract ge.ographic features from the map and utilize the information locked in the raster format. 
To automatically process the raster map, much of the re.search work relies on user input to extract the foreground pixel (e.g., pixel representing road lines and text labels) from the map as a preprocessing step of their feature ex.traction algorithms. Salvatore and Guitton [15] use a color classiﬁcation technique as their ﬁrst step to extract contour lines from topographic map. Cao et al. [1] utilize a preset grayscale threshold to remove the background pixel from raster map and then detect text labels from the map. Li et al. [10] utilize an image ﬁlter to ﬁrst extract the “black layer” from the USGS topographic map and then work on the black layers to extract and rebuild the text labels and lines. Khotanzad et al. [9] utilize a color segmentation method with user annotations to extract the contour lines from the USGS topographic map. Chen et al. [2] later extend the color segmentation method from Khotanzad et al. [9] to han.dle common topographic map ( not limited to the USGS topographic map)  local segmentation techniques. 
1http://nmviewogc.cr.usgs.gov/viewer.htm 2http://terraserver-usa.com/ 3http://www.lib.utexas.edu/map/ 


(a) Adjusting thresholds (b) Foreground pixel 
 1: Adjusting thresholds in the grayscale histogram to extract foreground pixel from a TIGER/Line map 
These feature extraction techniques all require prior knowl.edge of the raster map and experiments to generate a proper set of color or grayscale thresholds to ﬁrst extract foreground pixel from the map. In particular, because the raster map need to be readable when printed with non-color printers, the luminance ( the grayscale intensity) is the most rep.resentative component among the color components by de.sign, such as the red, green, and blue (RGB) or hue, satura.tion, and luminance (HSL) components. Therefore, to sep.arate the foreground pixel from a raster map, a common approach is to manually examine the grayscale histogram of the raster map and select the luminance intervals that separate the foreground pixel from the map background. For example,  1 shows that we can adjust the thresh.olds in the grayscale histogram  an image processing software4 to identify a luminance interval for separating the foreground pixel from a TIGER/Line map.5 
Because of the varieties of image quality and complexity of raster map, it is tedious to manually examine every input map for extracting the foreground pixel from the map. To minimize the manual work and to enable automatic feature extraction processes, we need to be able to automatically reuse the trained map proﬁles ( the luminance intervals) on applicable map.  2 shows a feature extraction system. The example system includes a map classiﬁcation component to eliminate the repetitive manual examination step and hence the system can automatically process previ.ously unseen map if the map is classiﬁed as similar to one of the trained map. For example, if we have a trained map proﬁle to extract the foreground pixel from the map shown in  3(a), the map classiﬁcation component can auto.matically select the trained map proﬁle for a new input map shown in  3(c) to extract the foreground pixel from the map as shown in  3(d). The solution might seem to be straightforward with this example of a TIGER/Line map, where we can simply apply the same set of thresholds on map from the same source to extract their foreground pixel. However, it is di.cult to determine the source of a raster map automatically. To make this problem worse, even map from a single map source may need di.erent sets of thresholds due to the noise from scanning or compression processes. For example, as shown in  4 and  5, 
4We use ImageJ (http://rsbweb.nih.gov/ij/) to demonstrate 
the manual approach. 
5http://tiger.census.gov/ 


 2: A feature extraction system with a map classiﬁcation component 

(a) 
Trained map (b) Foreground pixel of the trained map 

(c) 
New input map 	(d) Foreground pixel of the new input map 



 3: Example map from TIGER/Line and their foreground pixel 
both topographic map are from USGS and the colors of roads are very di.erent from one area to another (the noise in  5 comes with the original topographic map down.loaded from TerraServer-USA). In this example, the topo.graphic map covering El Segundo, CA requires the lumi.nance interval of 0 to 184 and the one covering St. Louis requires the luminance interval of 0 to 36 to extract their foreground pixel. 
In this paper, we developed an image comparison feature, called the luminance-boundary histogram. We built a map classiﬁcation technique based on a nearest-neighbor classi.ﬁer  the luminance-boundary histogram to compare an input map with previously trained map for identifying an applicable map proﬁle to extract the foreground pixel from the input map. The luminance-boundary histogram is based on the spatial relationships between the luminance levels used in the raster map ( the luminance usage of an image). To demonstrate the spatial relationships between luminance levels,  6 shows two map from Google map, from 


(a) A topographic map (b) The detail view 
 4: An example USGS topographic map cov.ering El Segundo, CA 

(a) A topographic map (b) The detail view 
 5: An example USGS topographic map cov.ering St. Louis, MO 
which we can extract their foreground pixel by applying the same luminance intervals on the map’ grayscale his.togram. The pixel of various luminance levels are used in the two map to constitute lines, characters, and back.ground. Although the number of pixel of each luminance level is di.erent between  6(a) and  6(b), the lu.minance usage in both images are similar. For example, the black pixel that make up the skeletons of the characters are surrounded by the same set of gray pixel used as shadows to make the characters stand out against the white pixel in the two map. Likewise, the set of gray pixel used to draw road boundaries are always in between the white road pixel and light gray background pixel. Therefore, we can compare the luminance-boundary histograms of raster map to identify the map that have similar luminance usage and then apply the same map proﬁle to extract their foreground pixel. 
The remainder of this paper is organized as follows. Sec.tion 2 discusses related work on map classiﬁcation features. Section 3 describes our approach to generate and compare the luminance-boundary histogram. Section 4 reports on our experimental results, and Section 5 presents the conclusion and future work. 
2. 	RELATED WORK ON MAP CLASSIFI.CATION FEATURES 
One approach for classifying raster map is based on the metadata of the map (e.g., map names, production time, location, geocoordinates, themes, etc.). The metadata can be manually speciﬁed or generated automatically from the surrounding text of the raster map in the document where the map was obtained (e.g., a web page or an article)[8]. Map classiﬁcation  metadata helps to answer queries such as ﬁnding the historical raster map of a speciﬁc region for a speciﬁc year, but does not enable our goal to iden.tify map sharing the same luminance usage. On the other 

(a) Area one (b) Area two 
 6: The spatial relationships between lumi.nance levels are similar in the two images from Google map 
hand, our map classiﬁcation technique compares the image content of raster map  the luminance-boundary his.togram, which identiﬁes the raster map that share similar spatial relationships between their luminance levels. In this section, we focus on reviewing image features used to ﬁnd similar images and compare these image features with our luminance-boundary histogram. 
The research on comparing image content to ﬁnd simi.lar images is called content-based image retrieval (CBIR), which has been a very active research topic [16]. Generally, the image features used to compare two images for identi.fying the similarity between them fall into one of the three categories: shape features, texture features, and color fea.tures. Shape features are used for recognizing objects of similar shapes in images, such as  the histogram of ori.ented gradient (HOG) descriptors for pedestrian detection from photos [7]. Since objects of the same shape in two map can be drawn with di.erent colors, the fact that two raster map both have objects of the same shape does not indicate the similarity between their luminance usage. For the tex.ture features, a commonly used set is the Tamura texture features [19], which are based on psychological experiments on human interpretation of images. The Tamura features describe the overall image texture, such as the coarseness and the contrast of the images. Another type of texture fea.ture ﬁrst transforms the image into another domain (e.g., the frequency domain) and then generates descriptions of the image textures  the transformed image, such as the Gabor wavelet transform features [11]. Other texture features are based on the edge detectors such as the edge histogram descriptor [12], which splits the image into small regions and generates the edge histogram for each region. For our problem of classifying the raster map based on their luminance usage,  the texture features that are gener.ated from the whole image to represent the overall texture of an image does not help much since two images with sim.ilar textures (e.g., contrast) do not necessarily share similar luminance usage. Instead, we utilize the luminance di.er.ences locally around each luminance level in the image to build the luminance-boundary histogram. 
For the color features, the three major ones are the color histogram, color moments, and color-coherence vectors. These color features can work on any single or combined color com.ponents, such as  only the H or V component from the HSV color domain or a weighted combination of the R, G, and B component from the RGB color domain. If only the V component or a speciﬁc RGB combination is used, the color features then represent the luminous intensities in the im.age; otherwise, the H component only can be quantized and then used to represent the colors in the image. The color histogram records the number of pixel used for every color in an image [18]. For the color histograms of two images to be similar, the corresponding colors in the two histograms should have similar numbers of pixel in the images, which is often not the case with two raster map sharing similar spa.tial relationships between their luminance levels. The color moments [18] are based on statistical analysis and use the average, standard deviation, and skewness of the color his.togram as features to describe an image. Since the color mo.ments are built on the color histogram, the color moments rely on the assumption that two similar images use simi.lar numbers of pixel for the colors in the color histogram. Moreover, the color moments require manual adjustments and experiments to tune the weights of each of the statistic components. The color-coherence vectors [14] incorporate the sizes of color regions into the color histogram and usu.ally produce better and more robust results among the three color features. However, the region size needs to be tuned to achieve the best result and the size parameter cannot be intuitively applied to compare raster map. 

These color features capture some concepts of the color or luminance usage (depends on the choice of color compo.nents) in the raster map, such as the type of colors or lumi.nous intensities used in the images or in a region smaller or larger than a predeﬁned size. However, these color features do not take into account the spatial relationships between colors or luminance levels used in the image, and hence do not help much on ﬁnding raster map with similar lumi.nance usage. The luminance-boundary histogram requires no threshold tuning and represents the luminance usage of a raster map by capturing the spatial relationships between the map’s luminance levels. We compare the luminance-boundary histogram with the color histogram, color mo.ments, and color-coherence vectors in our experiments. The experiments show that the luminance-boundary histogram is e.cient and produces the best and most robust results compared to the other color features for classifying raster map. 
3. 	RASTER MAP CLASSIFICATION BASED ON IMAGE CONTENT 
In this section, we present our map classiﬁcation tech.nique that uses the luminance-boundary histogram and a nearest-neighbor classiﬁer for identifying raster map shar.ing the same luminance interval for extracting their fore.ground pixel. We describe the technical details to generate the luminance-boundary histogram and the metric that the nearest-neighbor classiﬁer uses to compare the luminance-boundary histograms of two images. 
3.1 	Generating the Luminance-Boundary His.togram 
We generate two luminance-boundary histograms (LBH) for an image. The ﬁrst luminance-boundary histogram is called the high luminance-boundary histogram (HLBH), in which the X-axis represents the luminance spectrum and the Y-axis represents the normalized high luminance-boundary value of each luminance level in the luminance spectrum. We compute the high luminance-boundary value of a luminance level, L,  the luminous intensities that have higher lu.minous values than L and are adjacent to L. Similarly, we 

 7: The approach to generate the high and low luminance-boundary histograms 
generate a second luminance-boundary histogram called the low luminance-boundary histogram (LLBH), which contains the normalized low luminance-boundary values. We com.pute the low luminance-boundary value of a luminance level, L,  the luminous intensities that have lower luminous values than L and are adjacent to L. 
The HLBH and LLBH are designed to capture the lumi.nance usage of a raster map ( the spatial relationships between luminance levels) by exploiting the luminous di.er.ences between adjacent luminance levels in the map. The HLBH value of a luminance level indicates a higher bound.ary in the grayscale histogram that separates the luminance level from its adjacent luminance levels in the raster map. Similarly, the LLBH value indicates a lower boundary. For example, a luminance level of 128 with its HLBH value as 10 implies that 138 is the average luminance intensity of the pixel that have their intensity level higher than 128 and are around pixel of luminance level 128 in the raster map. For two map that have similar luminance usage and share the same luminance intervals to extract their foreground pix.els, there are similar boundaries in the grayscale histogram to separate the two map’ foreground and background lu.minous intensities. Therefore, we can compare the LBH of the new input map and trained map to determine if any of the trained map proﬁle can be applied on the new map for extracting its foreground pixel. 
The overall approach to generate the HLBH and LLBH is shown in  7. We ﬁrst convert the input raster map to a grayscale image with 256 luminance buckets ( the lu.minance spectrum is quantized to 256 levels). Then, we scan every pixel on the grayscale image to collect the high and low luminance-boundary values for each luminance bucket to generate the HLBH and LLBH. The algorithm to com.pute and normalize the HLBH and LLBH values from a grayscale image is shown in Table 1. The following subsec.tions describe the algorithm in detail. 
3.1.1 Extracting the Luminance Component 
We extract the luminance component from the raster map by converting the map to a grayscale image. The luminance is chosen instead of  one or all of the R, G, and B components or H, S, and L components for two reasons. 

Table 1: Pseudo code for generating luminance-boundary histograms 
/* Global variables */ 
Histogram HLBH, LLBH 
Histogram HLBP ixelCount, LLBP ixelCount 
GrayscaleImage GImage 
Function GenerateHistogram() 
For each row Y in the GImage 
For each colum X in the GImage Luminance L = GImage[X, Y ] HLBH[L]=HLBH[L]+ GetHLBValue(X,Y) LLBH[L]=LLBH[L]+ GetLLBValue(X,Y) 
End For 
End For 

/* Average the histograms*/ 
Value totalHLB, totalLLB 
Foreach Luminance L in the GImage HLBH[L]= HLBH[L]/ HLBP ixelCount[L] totalHLB = totalHLB + HLBH[L] LLBH[L]= LLBH[L]/ HLBP ixelCount[L] totalLLB = totalLLB + LLBH[L] 
End Foreach 
/* Normalize the histograms*/ 
Foreach Luminance L in the GImage 
HLBH[L]= HLBH[L]/ totalHLB 
LLBH[L]= LLBH[L]/ totalLLB 

End Foreach 
End Function 
/* Return the lowest luminance higher than the luminance of the center pixel in a 3-by-3 area */ Function GetHLBValue(X, Y) 
Luminance LHLB = 256 
Luminance Lc = GImage[X, Y ] 
For Integer I = .1; I< 2; I++ 

For Integer J = .1; J< 2; J++ Luminance Ln = GImage[X + I,Y + J] IF(Ln <LHLB AND Ln >Lc) 
LHLB = Ln 
End IF 

End For 
End For 
IF LHLB .256 

= 
HLBP ixelCount[Lc]++ 
Return LHLB -Lc 

Else 
Return 0 
End IF 

End Function 
/* Return the highest luminance lower than the luminance of the center pixel in a 3-by-3 area */ Function GetLLBValue(X, Y) 
Luminance LLLB = -1 
... 

IF(Ln >LLLB AND Ln <Lc) ... IF LLLB .-1 
= HLBP ixelCount[Lc]++ Return Lc -LLLB 
Else 
Return 0 
End IF 

End Function 

(a) The color map (b) The grayscale map 
 8: An example map from Google map and the map in grayscale 

 9: Luminance levels of sample line pixel (background pixel are shown in white) 
First,  the one-dimensional features ( the L compo.nent) is more computational e.cient than  the three dimensional features ( the R, G, and B or H, S, and L components). Second, the luminance component is the most representative component by design since most of the map need to be readable when printed with non-color printers. For a color in the RGB domain, the luminous intensity is calculated as follows  the RMY ﬁlter:6 
LuminousIntensity = R . 0.5+ G . 0.419 + B . 0.081 (1) 
 8 shows a color map from Google map and the converted grayscale image of the map. In spite of the grayscale conversion, the objects on the grayscale map are still rec.ognizable, as they are in the original color map. 
3.1.2 Computing Luminance-Boundary Values 
With the grayscale image, we generate the high and low luminance-boundary map ( images contain the luminance-boundary values) by scanning each pixel in the image. For each pixel, we search on a 3-by-3 pixel neighborhood to compute the high and low luminance-boundary values for the luminance level of the center pixel.  9 shows an example of a line segment in pixel-view ( every cell is an image pixel), and the number on each pixel represents the luminance level of the pixel, such as 0 for black (fore.ground) and 255 for white (background). To compute the luminance-boundary values for the pixel identiﬁed by the dashed circles, we search their 8 neighboring pixel ( in a 3-by-3 pixel neighborhood) that intersect with the circles. 
6The RMY ﬁlter is one of the existing techniques to compute the luminance from the RGB color space. More discussions about the color space conversions can be found in [17] 




(a) The high luminance-boundary map 	(a) The high luminance-boundary histogram 



(b) The low luminance-boundary map 
 10: High and low luminance-boundary map 
For the high luminance-boundary value, we search in the 3-by-3 pixel area to ﬁnd the neighboring luminance that is higher than the luminance of the center pixel, L(X, Y ), and the di.erence between the neighboring luminance and L(X, Y ) is minimum ( the least upper bound). For the low luminance-boundary value, we search in the same 3-by.3 pixel area to ﬁnd the neighboring luminance that is lower than the luminance of the center pixel, L(X, Y ), and the di.erence between the neighboring luminance and L(X, Y ) is minimum ( the greatest lower bound). Formally, the luminance of a pixel in image M at (X, Y ) is L(X, Y ), and Du,v.{.1,0,1} represent the eight luminance di.erences be.tween the pixel and its neighboring pixel in the 3-by-3 area. The high luminance-boundary value of the pixel, M(X, Y ), is the smallest positive number in Du,v.{.1,0,1}, where 
Du,v.{.1,0,1} = {L(X + u, Y + v) . L(X, Y )} (2) 
If every number in Du,v.{.1,0,1} is negative or equal to 0, the high luminance-boundary value is 0 ( no neighboring pixel has a higher luminance level than the pixel M(X, Y )). Similarly, the low luminance-boundary value of the pixel M(X, Y ) is the largest negative number in Du,v.{.1,0,1}. If every number in Du,v.{.1,0,1} is positive or equals to 0, the low luminance-boundary value is 0 ( no neighboring pixel has a lower luminance level than the pixel M(X, Y )).  10 shows the high and low luminance-boundary map of  9. The pixel that are crossed out by the dashed rectangle are the boarder pixel, which are generally not considered in any convolution type image processing because of their lack of eight neighboring pixel. 
3.1.3 Normalizing Luminance-Boundary Histograms 
We sum-up the luminance-boundary values for each lu.minance level in each of the luminance-boundary map to 
(b) The low luminance-boundary histogram 
 11: The luminance-boundary histograms 
generate the normalized high and low luminance-boundary histograms. The normalized luminance-boundary values of a luminance level represent the comparative importance of the luminance level in a raster map since the luminance level of a highlighted feature have comparatively high contrast against the luminance levels of the feature’s neighboring pix.els than other features or the background. The normalized 
luminance-boundary values are computed as follows:  
Normalized high luminance-boundary value:  
255X  
nHLBValuei  = HLBValuei/  HLBValuei  (3)  
i=0  
Normalized low luminance-boundary value:  
255X  
nLLBValuei  = LLBValuei/  LLBValuei  (4)  
i=0  

We normalize the luminance-boundary value of each lumi.nance level by dividing the each of the luminance-boundary value with the summation of the luminance-boundary value of every luminance level in the raster map. The normaliza.tion of the high and low luminance-boundary values are done separately.  11 shows the two luminance-boundary histograms after we normalize the high and low luminance-boundary values of each luminance level in  9. 
3.2 	Comparing Luminance-BoundaryHistograms 
To measure the similarity of two sets of luminance-boundary histograms from two raster map, we utilize a nearest-neighbor classiﬁer that employs a common histogram comparison met.ric, the L1-distance. Given two map and their luminance-boundary histograms HLBH1 , LLBH1 and HLBH2 , LLBH2 

255
X 
L1 = |HLBH1i . HLBH2i | + |LLBH1i . LLBH2i | (5) 
i=0 
A smaller distance indicates that the spatial relationships between luminance levels in one map are similar to the ones in the other map. 
4. EXPERIMENTS 
We compared our luminance-boundary histogram with 
three traditional color-based features: the color histogram, 
the color moments, and the color-coherence vectors. We fol.
lowed the steps in [18] to implement the color histogram and 
color moments. Color-coherence vectors were implemented 
as described in [14] with the region threshold tuned to 5%. 
The raster map were all quantized in the RGB color space 
 16 color buckets for R, 16 color buckets for G, and 
16 color buckets for B ( a total of 4,096 buckets) be.
fore we generated the color histogram, color moments and 
color-coherence vectors. 
In one experiment, we evaluated the robustness of the 
four features  image retrieval queries. In a second ex.
periment, we simulated a map classiﬁcation task ( the 
map classiﬁcation component shown in  2) to clas.
sify raster map based on their luminance usage by compar.
ing the map’ image content  the four features. In 
both experiment settings, we used 60 test map and an 
image repository with raster map collected from various 
sources. We collected the 60 test map from 11 map sources 
on the Internet, namely Google map, Live map, Yahoo 
map,7 MapQuest map,8 USGS topographic map, Rand 
McNally,9 Map24,10 TIGER/Line map, OpenStreetMap,11 
Streetmap.co.uk,12 and ViaMichelin.13 Table 2 shows the 
map sources, map types, and map counts of each source. We 
combined two existing image repositories to create a large 
map repository for the two experiments. One image reposi.
tory contained 1,112 raster map identiﬁed in [13]. The other 
image repository has 383 images wrapped from Yahoo Im.
age Search and Google Image Search  search keywords 
such as “street map” or “Los Angeles map.” 
For the ground truth of our experiments, we manually 
separated the test map into 12 classes based on manually 
trained map proﬁles to extract the map’ foreground pixel. 
Initially we created a class for every map source and assigned 
every map to a class according to the map’s source since the 
raster map from the same source are most likely to share 
a set of thresholds to extract their foreground pixel. Next, 
within each class, we manually identiﬁed a luminance inter.
val to separate the foreground pixel from a map and tested 
the luminance interval on every map in the class. For ex.
ample,  14 shows two map from Rand McNally and 
their foreground pixel after we applied the same thresh.
old setting of 0 to 190. The fact that the two map share 
7http://map.yahoo.com/ 
8http://www.mapquest.com/ 
9http://www.randmcnally.com/ 10http://www.map24.com/ 11http://www.openstreetmap.org/ 12http://streetmap.co.uk/ 13http://www.viamichelin.com/ 
Table 2: Test map and the ground truth 

the L1 distance between the two sets of luminance-boundary histograms is deﬁned as: 

Map Source  Map Type  Map Counts  Intensity Interval  
Google map  Digital  5  0–230  
Live map  Digital  5  0–225  
Yahoo map  Digital  5  0–200  
MapQuest map  Digital  5  0–220  
USGS topographic map  Scanned  5  0–36  
USGS topographic map  Scanned  5  0–184  
Rand McNally  Digital  5  0–190  
Map24  Digital  5  0–215  
TIGER/Line  Digital  5  0–110  
OpenStreetMap  Digital  5  0–238  
Streetmap.co.uk  Digital  5  0–175  
ViaMichelin  Digital  5  0–234  

the same threshold setting indicates that the two map be.long to the same class in our ground truth. If one set of thresholds cannot be applied to every map in one class, we further divided the class into smaller classes and manually identiﬁed a set of thresholds for each class. Eventually the map in one class all shared one set of thresholds to extract their foreground pixel. For example, we initially assigned all ten topographic map from USGS to one class. Then, we further separated the ten map into two classes since the ten map did not share the same threshold setting to extract their foreground pixel.  13 shows the foreground pix.els of the topographic map after we applied the luminance interval of 0 to 184 on  4(a) and the luminance in.terval 0 to 36 on  5(a). The manually trained map proﬁles for the test map are shown in the luminance inter.val column in Table 2. For the map repository, we manually examined each map image. We removed the non-map im.ages and map images that have similar color usage as the test map or generated from the same map sources as the 11 test map sources. Therefore, besides the test map, the map images in the repository should not be classiﬁed into any of our test classes  the four test features. 
4.1 Experiments on Image Retrieval 
We ﬁrst tested the robustness of the luminance-boundary histogram and the three traditional color-based features as follows: First, after every test map was inserted into the image repository, we removed one class of test map from the repository and reinserted one test map from the removed class into the image repository as the target map. Then, we used the remaining test map from the removed class to query the image repository in turn. The repository returned the query results by ranking the images in the repository based on their similarity to the query image. The similarity was computed by comparing the four test features in turn. Finally, we recorded the rank of the target map in the query results. For example, as shown in Tabel 2, there were ﬁve test map from Google map, namely G1, G2, G3, G4, and G5. After we inserted G1 into the image repository, we ﬁrst used G2 as the query image. We then recorded the rank of G1 in the returned query results. Next, we used G3 as the query image and recorded the rank of G1 in the returned query results. After G4 and G5 were both used as the query image, we removed G1 from the repository and inserted G2 into the repository. Then, we used the other four images as the query images to record the rank of G2. This process 


Table 3: Experimental results  the four image comparison features in 240 queries 
Feature  Average Ranks  .  
Luminance-Boundary Histogram  5.95  24.15  
Color-Coherence Vectors  15  52.14  
Color Histogram  28.17  116.85  
Color Moments  232.87  239.52  


(a) 
The original map (b) The original map 

(c) 
The foreground pixel (d) The foreground pixel 



 12: Example Rand McNally test map and their foreground pixel 

(a) El Segundo, CA (b) St. Louis, MO 
 13: The foreground pixel of the USGS topo.graphic map shown in  5(a) and  4(a) 
was repeated until every test map had been inserted into the image repository. As a result, for the 60 images ( 5 map in a class and there are 12 classes), we conducted 240 queries ( 20 queries for each of the 12 classes) for the four test features in the image retrieval experiments. 
By conducting the image query for every pair of images in each class, a robust image comparison feature should have a low average rank and low variation between the ranks from all queries. Table 3 shows the average ranks and standard deviations of the results for the four test features.  14 to  16 show sample queries, where the ranks in the captions are presented in the order of  the luminance-boundary histogram (LBH), color-coherence vectors (CCV), the color histogram (CH), and color moments (CM). 
Overall, the luminance-boundary histogram did well com.pared with other color-based features on both the average rank and the standard deviation. In particular, the luminance-boundary histogram and color-coherence vectors are much better and robust (both have low variances) than the color histogram and color moments since the ﬁrst two features both utilize more than just color or luminance. Between the luminance-boundary histogram and color-coherence vectors, the average rank  the luminance-boundary histogram 
was better, and the di.erence is statistically signiﬁcant.14 
In addition, the luminance-boundary histogram had lower 
variance and no tuning was required. 
As shown in  14, the luminance-boundary histogram 
handled cases where color-coherence vectors did not work 
well. Because the luminance levels of the background used 
in  14(a) and  14(b) are very di.erent, color-
coherence vectors that rely on the consistency of region sizes 
of the same color showed poor result on classifying these two 
images (other color features show even worse results). On 
the other hand, since the foreground pixel in the Rand Mc-
Nally map have a strong luminance level against the back.
ground in both map, the luminance-boundary histogram 
worked well on ﬁnding the target map. For a more complex 
set of map shown in  15, the two map share sim.
ilar size of color regions but di.erent number of colors, so 
color-coherence vectors did better than the color histogram 
and color moments. Luminance-boundary histograms had 
the best rank since the high and low luminance-boundary 
values are designed to capture the spatial relationships be.
tween adjacent luminance levels, which are similar between 
the two map. 
 16 shows a case where the luminance-boundary 
histogram did not have the best result. This is because 
there are only a few luminance levels used in the map, and 
the extra luminance levels used to draw the highways and 
major roads in the query image have strong high and low 
luminance-boundary values. The strong luminance-boundary 
value from the extra luminance levels lowered the normalized 
luminance-boundary values for the luminance levels shared 
by the two map such as the ones used to draw the road 
boarders and the characters. This issue could be resolved if 
we further apply a threshold on the luminance-boundary his.
togram to compare only the luminance levels that have pixel 
counts larger than a preset percentage. However, the perfor.
mance will then depends on the threshold tuning, which is 
similar to color-coherence vectors. Color-coherence vectors 
and the color histogram both did well on this query since 
the colors that exist in only one of the images have a small 
number of pixel compared to the colors shared by the two 
images. 
4.2 Experiments on Map Classiﬁcation 
In the second experiment, we simulated a map classiﬁca.tion task to test the classiﬁcation of raster map  the luminance-boundary histogram and the three color-based features with a nearest neighbor classiﬁer. For an input map, the map classiﬁcation component searched the image repos.itory to ﬁnd a target map that shared a trained map proﬁle with the input map to extract their foreground pixel. The experiments worked as follows: First, after we inserted every 

14 a one-tailed distribution paired t-test (p=0.006) 

(a) The query map 	(b) The target map with 
(a) The query map 	(b) The target map with ranks: 1/289/713/275 ranks: 3/1/1/231 

 14: A query with Rand McNally map (ranks 
 16: A query with OpenSteetMap map are listed as LBH/CCV/CH/CM) 
(ranks are listed as LBH/CCV/CH/CM) 
Table 4: Classiﬁcation results  the four image comparison features 


Feature  Accuracy  
Luminance-Boundary Histogram  95%  
Color-Coherence Vectors  86.67%  
Color Histogram  88.33%  
Color Moments  13.33%  


(a) The query map 	(b) The target map with ranks: 1/15/269/724 
 15: A query with Streetmap.co.uk map (ranks are listed as LBH/CCV/CH/CM) 
test map into the image repository, we removed only one test map from the repository and used the removed test map as the query image. If the ﬁrst returned map belonged to the same class as the query image, the classiﬁcation was success.ful. Then, we reinserted the query image into the repository and removed another test map to test the map classiﬁcation until every test map had served as the query image. For ex.ample, we ﬁrst removed G1 to query the repository ( G1 represents a new input map). If the ﬁrst returned map was G2, G3, G4, or G5, then we had a correct classiﬁcation ( we successfully identiﬁed an applicable trained map proﬁle for G1); otherwise, the classiﬁcation failed. We report the classiﬁcation accuracy for each feature. The accuracy is de.ﬁned as the number of successful classiﬁcations divided by the total number of tested classiﬁcations. 
In the previous image retrieval experiments, there was only one target map in the repository. In the map clas.siﬁcation experiments, there were four target map in the repository, and if the test feature ranked any of the tar.get map as the ﬁrst map in the returned query results, the classiﬁcation was successful. The enlarged pool was a simulation of a real map classiﬁcation application since in practice, the size of any map class grows after we have seen more map. In the classiﬁcation results shown in Table 4, the luminance-boundary histogram had the highest accuracy of 95% among the features on classifying the raster map in our experiments. We missed three map in the map classiﬁcation experiments  the luminance-boundary histogram. Al.though the enlarged pool contained more diverse map, the three map still su.ered from the same issue as the query shown in  16. We expect this problem to be resolved when there are more map in each class as the training set becomes larger. The color histogram also beneﬁted from the enlarged pool. Despite the fact that the color histogram had a lower average rank in the previous test, it had similar accuracy as color-coherence vectors in the map classiﬁca.tion experiments. However, the accuracy of color-coherence vectors and the color histogram were both lower than the luminance-boundary histogram. 
4.3 Efﬁciency 
We implemented our experiments  Microsoft .Net running on a Microsoft Windows 2003 Server powered by a 
3.2 GHz Intel Pentium 4 CPU with 4GB RAM. We recorded the computation time for generating the two luminance-boundary histograms and color-coherence vectors as the com.parison for e.ciency. Both features can be generated us.ing a single-pass on the image. The smallest test image in pixel is 130-by-350 and the largest image is 3000-by.2422. With 1,949 images, it took 428 seconds to generate the luminance-boundary histograms and 805 seconds to gen.erate color-coherence vectors. One of the dominant factors for the time di.erences was that our luminance-boundary histograms used 256 buckets while color-coherence vectors used 4,096 buckets (16 R, 16 G, 16 B). The implementations of these two features were not optimized and improvements can still be done to speed up the processes. 
5. CONCLUSION AND FUTURE WORK 
Identifying the foreground color or luminance in the raster map is labor intensive since the process requires user input on specifying foreground pixel in the raster map, especially for scanned map that have numerous colors. Instead, we present an approach to classify raster map based on their luminance usage for re existing trained map proﬁles to extract foreground pixel from new input map. The classiﬁ.cation task is achieved by  the luminance-boundary his.togram and a nearest-neighbor classiﬁer to compare the im.age content of two raster map. In the image retrieval experi.ments, the luminance-boundary histogram produced robust results compared with other traditional color features on ﬁnding map with similar luminance usage. In the map clas.siﬁcation experiments, the luminance-boundary histogram achieved 95% accuracy compared with the traditional color features with average ranks from 13.33% to 88.33%. In other words, we can identify an applicable trained map proﬁle for 95% of the test map by  the luminance-boundary his.togram for comparing the input map with existing map in the repository and hence make automatic map processing work practical. In addition, the generation of the luminance-boundary histogram is e.cient without parameter tunings. 

In the future, we plan to incorporate modern classiﬁers or o.-the-shelf CBIR systems with the luminance-boundary histogram to explore the possibility of enhancing the map classiﬁcation. Moreover, we intend to integrate the map classiﬁcation component with our current map processing system that extracts geographic feature from raster map. In our previous work, we presented a map processing sys.tem with an automatic technique for extracting the road pixel from simpler map (e.g., digitally generated map) [6] and a supervised technique for more complex map or map with poor image quality (e.g., scanned map, a metro map contains various types of lines, etc.) [5]. The map pro.cessing system can process the road pixel to extract road-intersection templates [3] and then utilize the road inter.section templates to extract road vectors from the raster map [4]. By integrating the map classiﬁcation component described in this paper with the map processing system, we will be able to automate the processes for extracting geo.graphic features from more diversiﬁed raster map and fuse the map with imagery and other geospatial data. 
6. ACKNOWLEDGMENTS 
This research is based upon work supported in part by the University of Southern California under the Viterbi School Doctoral Fellowship, in part by the United States Air Force under contract number FA9550-08-C-0010. 
The U.S. Government is authorized to reproduce and dis.tribute reports for Governmental purposes notwithstanding any copyright annotation thereon. The views and conclu.sions contained herein are those of the authors and should not be interpreted as necessarily representing the o.cial policies or endorsements, either expressed or implied, of any of the above organizations or any person connected with them. 
7. REFERENCES 
[1] R. Cao and C. L. Tan. Text/graphics separation in 
map. In Proceedings of the Fourth International 
Workshop on Graphics Recognition Algorithms and 
Applications, pages 167–177, 2002. 

[2] Y. Chen, R. Wang, and J. Qian. Extracting contour 
lines from common-conditioned topographic map. 

IEEE Transactions on Geoscience and Remote Sensing, 44(4):1048–1057, 2006. 
[3] Y.-Y. Chiang and C. A. Knoblock. Automatic extraction of road intersection position, connectivity, and orientations from raster map. In Proceedings of the 16th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, pages 1–10, 2008. 
[4] Y.-Y. Chiang and C. A. Knoblock. Automatic road 
vectorization of raster map. In Proceedings of the 
Eighth International Workshop on Graphics 
Recognition, 2009. 

[5] Y.-Y. Chiang and C. A. Knoblock. A method for automatically extracting road layers from raster map. In Proceedings of the Tenth International Conference on Document Analysis and Recognition, 2009. 
[6] Y.-Y. Chiang, C. A. Knoblock, C. Shahabi, and C.-C. Chen. Automatic and accurate extraction of road intersections from raster map. GeoInformatica, 13(2):121–157, 2008. 
[7] N. Dalal and B. Triggs. Histograms of oriented 
gradients for human detection. In International 
Conference on Computer Vision & Pattern 
Recognition, volume 2, pages 886–893, 2005. 

[8] J. Gelernter. Data mining of map and their 
automatic region-time-theme classiﬁcation. 
SIGSPATIAL Special, 1(1):39–44, 2009. 

[9] A. Khotanzad and E. Zink. Contour line and 
geographic feature extraction from USGS color 
topographical paper map. IEEE Transactions on 
Pattern Analysis and Machine Intelligence, 
25(1):18–31, 2003. 

[10] L. Li, G. Nagy, A. Samal, S. C. Seth, and Y. Xu. Integrated text and line-art extraction from a topographic map. International Journal of Document Analysis and Recognition, 2(4):177–185, 2000. 
[11] B. S. Manjunath and W. Y. Ma. Texture features for browsing and retrieval of image data. IEEE Transactions on Pattern Analysis and Machine Intelligence, 18(8):837–842, 1996. 
[12] 
B. S. Manjunath, J. R. Ohm, V. V. Vasudevan, and 

A. 
Yamada. Color and texture descriptors. IEEE Transactions on Circuits and Systems for Video Technology, 11(6):703–715, 2001. 


[13] M. Michelson, A. Goel, and C. A. Knoblock. Identifyinig map on the world wide web. In Proceedings of the Fifth International Conference on Geographic Information Science, 2008. 
[14] G. Pass, R. Zabih, and J. Miller. Comparing images  color coherence vectors. In Proceedings of the Fourth ACM International Conference on Multimedia, pages 65–73, 1996. 
[15] S. Salvatore and P. Guitton. Contour line recognition from scanned topographic map. In Proceedings of the Winter School of Computer Graphics, 2004. 
[16] N. Sebe, Q. Tian, M. S. Lew, and T. S. Huang. Similarity matching in computer vision and multimedia. Computer Vision and Image Understanding, 110(3):309–311, 2008. 
[17] G. Sharma and H. J. Trussell. Digital color imaging. IEEE Transactions on Image Processing, 6:901–932, 1997. 
[18] M. Stricker and M. Orengo. Similarity of color images. In In Storage and Retrieval of Image and Video Databases III, volume 2420, pages 381–392, 1995. 
[19] H. Tamura, S. Mori, and T. Yamawaki. Textual features corresponding to visual perception. IEEE Transactions on Systems, Man and Cybernetics, 8(6):460–472, 1978. 

Strabo: A System for Extracting Road Vector Data from 
Raster map (Demo Paper) 

Yao-Yi Chiang and Craig A. Knoblock
University of Southern California 
Department of Computer Science and Information Sciences Institute 
4676 Admiralty Way, Marina del Rey, CA 90292, USA 
[yaoyichi, knoblock]@isi.edu 

ABSTRACT 
Raster map contain valuable road information, which is es.pecially important for the areas where road vector data are otherwise not readily accessible. However, converting the road information in raster map to road vector data usu.ally requires signiﬁcant user e.ort to achieve high accuracy. In this demo, we present Strabo, which is a system that extracts road vector data from heterogeneous raster map. We demonstrate Strabo’s fully automatic technique for ex.tracting road vector data from raster map with good im.age quality and the semi-automatic technique for handling raster map with poor image quality. We show that Strabo requires minimal user input for extracting road vector data from raster map with varying map complexity ( over.lapping features in map) and image quality. 
Categories and Subject Descriptors 
H.2.8 [Database Management]: Database Applications— 
Spatial Databases and GIS 
General Terms 
Algorithms, Design 
Keywords 
GIS, raster map, road vectorization, map processing 
1. INTRODUCTION 
Raster map are easily accessible than other geospatial data, such as imagery, and provide geographic features that are di.cult to ﬁnd elsewhere, such as landmarks in historical map. Since roads commonly exist across various geospatial data (e.g., satellite imagery), by extracting the road vector data from a raster map and matching the road geometry with the road geometry from a georeferenced data set, we can identify the geospatial extent of the raster map and align the raster map with other georeferenced data [1, 5]. Fur.ther, we can provide road vector data for the areas where 
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. ACM GIS ’10 San Jose, CA USA Copyright 2010 ACM 0-12345-67-8/90/01 ...$10.00. 
such data are otherwise not available. Extracting accurate road vector data from raster map is challenging because of the varying image quality, the complexity of map ( overlapping features in map), and the typical lack of meta-data (e.g., map geocoordinates, map source, original vec.tor data). To overcome these di.culties, in this demo, we present Strabo, which is a general road-vectorization sys.tem for processing heterogeneous raster map with mini.mal user input [2, 3, 5]. For raster map with good image quality, Strabo automatically extracts the foreground pixel and then generates the road geometry and the road vector data [2, 5]. For complex raster map, or map with poor image quality, Strabo employs a semi-automatic technique that requires a few examples of road areas to ﬁrst extract the road pixel from the map [3]. Then, Strabo automati.cally produces the road vector data from the extracted road pixel [2]. 
2. RELATED WORK 
The research for vectorizing raster map in the earlier years [7] was highly labor-intensive and the research focused on hardware design and the techniques to help human oper.ators to speed up the whole processes. For example, Leberl and Olson [7] describe techniques to aid the operator to move the cursor quickly, such as the line-following technique to automate the cursor movement. In recent work, Itonaga et al. [6] work on non-scanned map to extract the road vector data with manually speciﬁed parameters, such as the road width. In comparison, Strabo handles heterogeneous map types and extracts road vector data from raster map with minimal user input  dynamically generated parameters. 
In addition to the research work, a commercial product called R2V from Able Software is an automated raster-to.vector conversion software package specialized for digitizing raster map. R2V requires signiﬁcant user e.ort on labeling road areas and parameter tuning to achieve accurate results compared to Strabo, especially for map with numerous col.ors, such as scanned map [2]. 
3. STRABO ARCHITECTURE 
Strabo consists of two major components: a fully auto.matic and a semi-automatic road vectorization components. To vectorize the roads in a raster map, the user ﬁrst invokes the fully automatic road vectorization function of Strabo. The user then reviews the vectorization results and decides if the semi-automatic approach is required to improve the re.sults. We brieﬂy explain the key ideas of the automatic and semi-automatic components in the following subsections. 


(a) The user-labeling function 	(b) The extracted road vector data 
 1: Snapshots of Strabo 
3.1 Automatic Road Vectorization 
First, to identify the road pixel from raster map auto.matically, Strabo utilizes the heuristic that the background colors of raster map have a dominant number of pixel and the foreground colors have high contrast against the back.ground colors to remove the background pixel. Next, to separate the overlapping features in the foreground pixel, the automatic technique exploits the distinctive geometric properties of the road lines to detect the road width and road format ( single line or double lines) and identiﬁes the road pixel with dynamically generated parameters, such as a road line should be longer than the detected road width. Finally, Strabo links the identiﬁed road pixel to extract and vectorize the centerlines of the linked road pixel for produc.ing the road vector data. 
3.2 Semi-Automatic Road Vectorization 
The automatic road vectorization approach of Strabo does not work well for map with poor image quality since map with poor image quality generally contain numerous colors and hence the background colors do not always have a dom.inant number of pixel. Therefore, Strabo provides a semi.automatic technique with a supervised user-labeling func.tion to ﬁrst extract the road pixel [3]. 
To start the user-labeling function, Strabo ﬁrst generates a set of quantized map with reduced numbers of colors. The user then selects a quantized image that contains road lines shown in di.erent colors compared with other map features in order to label the road areas. The goal of the user-labeling task is to provide a user label for each road color in the quantized map. For example, for the quantized map shown in  1, the user needs to provide two user labels, one for the white roads and one for the yellow roads. A user label is a rectangle that should be large enough to cover a road intersection or a road segment. The user-labeling function of Strabo provides a dialog interface with a slide bar for the user to indicate the size of a user label, as shown in  1(a). The user label should be centered at a road intersection or at the center of a road line (approximately), which is the property that Strabo exploits to identify the road colors from the user labels automatically. After the user-labeling task is ﬁnished, Strabo analyzes the user labels to extract the road pixel and then generates the road vector data from the extracted road pixel.  1(b) shows the extracted road vector data from the raster map shown in  1(a). 
4. DISCUSSION AND FUTURE WORK 
We presented Strabo, which is a system that requires min.imal user e.ort for extracting accurate road vector data from heterogeneous raster map. The average processing time for Strabo to vectorize the road pixel for an 800x550-pixel map was 5 seconds and for a 2084x2756-pixel map was 2 minutes. In the future, we plan to incorporate into Strabo the functionality for recognizing text labels in raster map from our previous work [4]. 
5. ACKNOWLEDGMENTS 
This research is based upon work supported in part by the University of Southern California under the Viterbi School Doctoral Fellowship. 
References 
[1] Chen, C.-C., Knoblock, C. A., and Shahabi, C. (2008). Automatically and accurately conﬂating raster map with orthoimagery. GeoInformatica, 12(3):377–410. 
[2] Chiang, Y.-Y. and Knoblock, C. A. (2009a). 	Extracting road vector data from raster map. In GREC, LNCS, 6020, pages 93–105, New York. Springer. 
[3] Chiang, Y.-Y. and Knoblock, C. A. (2009b). 	A method for automatically extracting road layers from raster map. In Proceedings of the Tenth ICDAR. 
[4] Chiang, Y.-Y. and Knoblock, C. A. (2010). An approach for recognizing text labels in raster map. In Proceedings of the 20th ICPR. 
[5] Chiang, Y.-Y., Knoblock, C. A., Shahabi, C., and Chen, C.-C. (2008). Automatic and accurate extraction of road intersections from raster map. GeoInformatica, 13(2):121–157. 
[6] Itonaga, 	W., Matsuda, I., Yoneyama, N., and Ito, S. (2003). Automatic extraction of road networks from map images. Electronics and Communications in Japan (Part 
II: Electronics), 86(4):62–72. 

[7] Leberl, F. W. and Olson, D. (1982). 	Raster scanning for operational digitizing of graphical data. Photogrammetric Engineering and Remote Sensing, 48(4):615–672. 
Automatic Text Recognition from Raster map 
Yao-Yi Chiang and Craig A. Knoblock 
University of Southern California 
Department of Computer Science and Information Sciences Institute 
4676 Admiralty Way, Marina del Rey, CA 90292 
yaoyichi, knoblock@isi.edu 

Abstract 

Text labels in raster map provide valuable geospa.tial information by associating geospatial locations with geographical names. Although present commer.cial optical character recognition (OCR) products can achieve a high recognition rate on documents, text recognition on raster map is still challenging due to the varying text orientations and the overlapping be.tween text labels. This paper presents an automatic text recognition approach which focuses on locating indi.vidual text labels in the map and detecting their ori.entations and leverages horizontal text recognition of commercial OCR software. We show that our approach detects all strings in the test map and achieves 96.8% precision and 95.7% recall on character recognition. 
1. Introduction 
map are easily accessible compared to other geospatial data, such as vector data, databases of geo.graphical names, geospatial information systems (GIS), etc. Due to the popularity of high quality scanners and the Internet, we can now obtain various map in raster format for areas around the globe. By converting the text labels in a raster map to machine-editable text, we can produce geospatial knowledge for understand.ing the map covering area where other geospatial data is not ready accessible. Moreover, we can register other geospatial data (e.g., imagery) to a raster map [3] and exploit the recognized text from the map for indexing and retrieval of the geospatial data. 
Text recognition from raster map is a challenging task. First, the image quality of the raster map usually suffers from compression and scanning noise. Second, the text labels can have various font types and font sizes and very often overlap with each other or even other fea.tures in the map, such as road lines. Third, classic OCR research focuses on documents containing text lines in the same direction (usually horizontal text lines); how.ever, the text labels within a map do not follow a ﬁxed orientation. 
In this paper, we present a general approach to over.come these difﬁculties for recognizing text labels from raster map. We ﬁrst quantize color space of the raster map and generate a color palette for extracting the pix.els of text labels ( the text layer)  user spec.iﬁed colors. Then, we perform connected-component analysis on the text layer to identify characters, group characters into strings, and split overlapping text labels into individual strings. Finally, we detect the orienta.tion of the multi-oriented strings and rotate the image of each individual strings to the horizontal direction. The string images of horizontal direction then can be pro.cessed  commercial software for recognizing the text. We tested on two map with 1,655 characters and 308 words of varying text fonts and sizes and show that we can achieve accurate recognition rates automatically. 
The remainder of this paper is organized as follows. Section 2 discusses related work on text recognition from raster map. Section 3 presents our approach to detect and prepare string labels for commercial OCR software. Section 4 reports on our experimental results and Section 6 presents the discussion and future work. 
2. Related Work 
Text recognition from raster map has been an ac.tive research area. One type of research separates from classic OCR research ( working on text with the same orientation) and builds speciﬁc character recog.nition components for handling multi-oriented text la.bels. Both Deseilligny et al. [4] and Adam et al. [1] uses rotation-invariation features to compare the target string with trained character samples for recognizing text la.bels from raster map. These methods require intensive training, such as providing sample characters for each test map. 
For the techniques that employ classic OCR method as their character recognition components, Li et al. [6] identify the graphics layer and text labels  connected-component analysis and extrapolate the graphics layer to remove the lines that overlap with characters within each identiﬁed text label. Then, a template-matching based OCR component is used to recognize characters from the text labels. Cao and Tan [2] also analyze the geometry properties of the connected component to ﬁrst separate text labels from graphics. The separated graphics layer is then decom.posed into line segments and a size ﬁlter is used to re.cover the character strokes that touch the lines. Finally, an OCR software from HP is then used to recognize the text labels. In both [6] and [2], the identiﬁed text labels are manually rotated to the horizontal direction for the ﬁnal character recognition tasks. 

Pouderoux et al. [8] use component analysis and string analysis with dynamic parameters generated from the geometry of the connected components to identify strings in the raster map. The strings are then rendered horizontally for character recognition  the aver.age angle connecting the centroids of the components in a string. However, the average angle can vary much when the characters have very different height or width. For example, considering the substring ‘afa’ from one of our test map, the angle of the line connecting the centroid of the ﬁrst ‘a’ and the centroid of ‘f’ is almost perpendicular to the line connecting ‘f’ and the second ‘a’. On the other hand, we adopt the skew detection method in [7] to identify the orientation of each string automatically. 
3. Text Labels Recognition 
This section describes our techniques to locate groups of text labels in the map, separate overlapping labels, and detect the text orientation of each label. 
3.1 Extracting Text pixel 
Raster map usually contain numerous colors due to the scanning or compression processes. To generate a color a color palette for extracting the text pixel us.ing user speciﬁed colors, we apply color segmentation techniques to reduce the number of colors in the map. We ﬁrst apply the Mean-Shift ﬁltering algorithm, which merges two colors into one by considering the distance in the color space and the image space. The Mean-Shift ﬁltering algorithm preserves the object edges in the map and prevents pixel of two different objects to have the same color. Next, we utilize a common color quantiza.tion method called the median-cut [5] to generate a im.age with maximum 1,024 colors. The quantized image is present to the user for selecting a set of colors that represents text in the map; however, if the same color 

(a) 
An example text layer (b) Dilated text layer 

(c) 
Identiﬁed strings (d) Split strings 



 1. Locating strings in the map 
is used on both text and other features, text separation techniques such as [2] can be used to remove graphics and preserve text pixel.  1(a) shows an example of the extracted text layer. 
3.2 Identifying Strings 
With the extracted text layer, the user provides a sample string for each of the font size used in the text layer as follows: the user select a bounding box of a horizontal string and indicate how many characters are in the string. We then compute the character width and character spacing of each font size  the width of the bounding box and the number of characters. If more than one font size is used in the text layer, we sepa.rate the text layer into sub-text layers by performing a connected-component analysis with a size threshold on every connected component. Therefore, every sub-text layer should contain characters with similar size. 
To group characters in each sub-text layer into string, we use the dilation operator as in [2] to merge nearby characters. The iteration of the dilation operator is de.termined by the character spacing.  1(b) shows the results of merged characters and  1(c) shows the identiﬁed strings in red rectangles. As note in [2], the dilation operator has the beneﬁt on identifying curved strings, but it also merged two nearby strings. 


 2. Splitting merged strings 
To separate merge strings, we ﬁrst perform a connected-component analysis within each of the merged strings and use the distance transformation to calculate the pixel distance between each connected component in a merged string. Two connected com.ponents are linked if the distance in pixel between them is smaller than a threshold, which is also determined by the character spacing. Next, we start to trace the con.nected component following the links in each merged string and identify individual strings.  2 shows a merged string, where ‘W’ overlaps ‘n’ and ‘T’ over.laps with ‘y’. To split the merged string, we identify two types of connected component as splitting points: First, the ones that have more than three links, such as ‘Ty’. Second, the ones those constitute an angle smaller than a threshold with their neighbors, such as ‘Wn’ and its neighbors ‘a’ and ‘A’. The angle between three con.nected component are calculated  the centroid of each connected component. In the case that three con.nected components of the same height constituting a straight label, the angle is 180 degree. However, since the connected components have various heights, we use an angle threshold of 145 degree to prevent breaking a continuous string. For the map with curvy labels, we use an angle threshold of 125 degree to preserve curvy labels. After we identify the splitting points, we can produce individual strings as shown in the right side of  2. 
3.3 Detecting String Orientation 
Skew correction is very well developed in modern OCR techniques; however, classic skew correction can only apply on documents with multiple lines since the space between lines are exploited to detect the tilt an.gle [7]. To detect the orientation of each string, we mod.ify the morphological based skew correction method for multi-line document in [7] by select different sizes of structure elements for each string image. We ﬁrst ap.ply the closing operator  a structure element of size equal to the character width plus character spacing. Then, we rotate the string image from 0 degree to 179 degree and we use the maximum horizontal width of the rotated string to determine the length of the structure elements of the erosion operator. Therefore, the ero-
 3. Detecting orientation  mor.
phological operators 
sion operator is able to erase a portion of the foreground pixel when the string is not in the horizontal direction while not overly elongate to erase every foreground pix.els in every rotated image.  3 shows examples of intermediate results of the orientation detection where the horizontal strings has the most remaining pixel af.ter we apply the erosion operator. 
We only apply the orientation detection technique on the strings with more than three connected components since the orientation of short strings can be dominated by the height of the components. To assign orientation for the short strings, we search from the centroid of a short string for nearby strings and use the orientation of the nearby strings as the short string’s orientation. This is because short strings in a raster map are usually part of a longer label. For example, the most common short strings in our test map are ‘Av’ as avenue, ‘Dr’ as drive, ’Cir’ as circle, which are all part of a road name. 
4. Experimental Setup and Results 
We tested our approach on map from two sources. The ﬁrst test map is a digital map (850x850 pixel each) from Rand McNally (RM map). In addition to the digital map, we tested our technique on process.ing commonly accessible scanned map  a map tile (2750x2372 pixel) cropped from a scanned map (350dpi) published from by International Travel map (ITM map). We applied our techniques in this paper to identify text labels from the test map and detect the orientation of the labels. We generate two images for one text label by rotating the text label clockwise and counterclockwise to the horizontal direction according to its orientation. Then, we selected the correctly ro.tated string image ( not the upside down one) for the character recognition task, for which we used a com.mercial OCR software called ABBYY FineReader 10. 
For the RM map, we identiﬁed 54 strings and 29 of them have more than 3 characters and hence are sent to detect their orientation. After manual veriﬁcation, we detected accurate orientation for 28 of the 29 strings and the orientation offset for the only inaccurate string are 2 degrees. For the ITM map, we identiﬁed 254 

Table 1. Character recognition results 
Map  # of Char.  Precision  Recall  
RM map  258  96.4%  94.9%  
ITM map  1,397  96.9%  95.7%  

strings and 196 of them are sent to detect their orien.tation. After manual veriﬁcation, we detected accurate orientation for 183 of the 197 strings and the average orientation offset for the 5 inaccurate strings are only 
5.4 degrees. The orientation offset came from shorter strings or string with symbols such as a quotation mark. 
There are 25 strings and 58 strings in the RM map and the ITM map, respectively, have less or equal to 3 characters and hence we search nearby strings to as.sign their orientation. Among the 25 strings in the RM map, 5 of them we cannot ﬁnd a nearby string to in.herit the correct orientation. This is because four of the strings are near the boarder of the map and hence the short strings do not follow any of the nearby string ori.entation in the map. The other one of the incorrect ori.entation is the string ‘Av’ shown in the left-upper part of  1(a), where a road name, ‘BEND’ is in between the string ‘Av’ and its counter part ‘man’. Among the 58 short strings in the ITM map, 6 of them we cannot ﬁnd a nearby string to inherit the correct orientation. This is because three of the short strings are near the boarder of the map and the other three are isolated characters. 
Table 1 shows the OCR results on the character level. The errors were resulting from: 1. We do not have the orientation of the strings. 2. Overlapping charac.ters, such as the ‘n ’ and ‘w’ shown in  2. The problem of missing orientation detection can be done by performing OCR on multiple degrees to recognize the characters since there are only a few of strings we do not detect correct orientation. For the overlapping text, additional knowledge such as geographical name database could help as a dictionary to improve the re.sults. For the string level accuracy, we extracted every string from both map. 43 of 54 extracted strings in the RM map (80%) have all of their characters successfully recognized. 218 of 254 extracted strings in the ITM map (86%) have all of their characters successfully rec.ognized. 
5. Discussion and Future Work 
In this paper, we present an approach to automati.cally recognize text labels from raster map. Our ap.proach focuses on locating individual strings and de.tecting string orientation while leverages the advance of horizontal text recognition of commercial OCR soft.ware. By doing so, our approach beneﬁts from future improvement on commercial OCR software. Our ex.periments show accurate results on detecting the orien.tation of the identiﬁed text labels and recognizing the text labels. In the future, we plan to include additional knowledge of the map covering area to build a database of geographic names and use the database as a dictio.nary for improving the OCR accuracy on overlapping characters. 
6 	Acknowledgments 
The author would like to thank Dr. Chia-Hsiang Yang for his input on the article. This research is based upon work supported in part by the University of South.ern California under the Viterbi School Doctoral Fel.lowship. 
References 
[1] S. Adam, J. Ogier, C. Cariou, R. Mullot, J. Labiche, and J. Gardes. Symbol and character recognition: application to engineering drawings. IJDAR, 3(2): 89–101, 2000. 
[2] R. Cao and C. L. Tan. 	Text/graphics separation in map. In Proceedings of the 4th GREC Workshop, pages 167–177, 2002. ISBN 3-540-44066-6. 
[3] C.-C. Chen, C. A. Knoblock, and C. Shahabi. Au.tomatically and accurately conﬂating raster map with orthoimagery. GeoInformatica, 12(3):377– 410, 2008. 
[4] M. P. Deseilligny, H. L. Mena, and G. Stamonb. Character string recognition on map, a rotation-invariant recognition method. Pattern Recognition Letters, 16(12):1297–1310, 1995. 
[5] P. Heckbert. 	Color image quantization for frame buffer display. SIGGRAPH, 16(3):297–307, 1982. 
[6] L. Li, G. Nagy, A. Samal, S. C. Seth, and Y. Xu. Integrated text and line-art extraction from a topo.graphic map. IJDAR, 2(4):177–185, 2000. 
[7] L. Najman. 	 mathematical morphology for document skew estimation. SPIE DRR IX, pages 182–191, 2004. 
[8] J. Pouderoux, J. C. Gonzato, A. Pereira, and P. Gui.tton. Toponym recognition in scanned color topo.graphic map. In Proceedings of the 9th ICDAR, volume 1, pages 531–535, 2007. 

HARVESTING GEOGRAPHIC FEATURES FROM HETEROGENEOUS 
RASTER map 

by 


Yao-Yi Chiang 

A Dissertation Presented to the 
FACULTY OF THE USC GRADUATE SCHOOL 
UNIVERSITY OF SOUTHERN CALIFORNIA 
In Partial Fulﬁllment of the 
Requirements for the Degree 
DOCTOR OF PHILOSOPHY 
(COMPUTER SCIENCE) 

December 2010 

Copyright 2010 Yao-Yi Chiang 

Dedication 
n

To my father, Chi´ukok-liˆang, and my mother, Hˆau l¯e-hˆoa, in-¯ui l´ın-ˆe `ai k¯a chi-chhˆı, chiah-¯u chit-p´un phok-s¯u l¯un-bˆun. 
Acknowledgments 

I would like to ﬁrst and foremost thank my advisor, Professor Craig A. Knoblock. Craig has made my graduate study a wonderful journey. He gave me freedom and support in doing my research. He gave me opportunities to present in conferences and showed me how to interact with people in the research community. He spent numerous hours with me discussing research ideas and reading my papers. He has made me a better researcher (and a better person) than I had ever hoped. The ﬁrst time after I gave Craig my draft of a conference paper, he said to me, and I quote: “Yao-Yi, where are your paragraphs?” And now, I can ﬁnish a whole Ph.D. thesis with meaningful paragraphs. I would like to also thank Craig on a personal level. He was (and still is) always willing to help on things not limited to the research. He taught me how to play ping-pong, showed me tricks on bidding on hotels and ski cabins, explained English pronunciations, and much more. Most importantly, he is fun to talk with and is very knowledgeable on all sorts of interesting things. 
I would like to thank my other dissertation committee members, Professors Cyrus Shahabi and John P. Wilson. Cyrus has been a great source of ideas and inspiration on solving research problems and a great person for discussions. John gave me valuable advise on doing practical computer science research from the view of a geographer. Both John and Cyrus helped to make the thesis possible from the very beginning. I would like to thank Professors C.C. Jay Kuo and G´erard G. Medioni for serving on my guidance committee and advising me on the research direction. I would like to thank my undergrad advisor Professor Frank Y.-S. Lin for his encouragement and advice that inspired my to pursue my own Ph.D. I would like to thank Phyllis O’Neil for her excellent comments and editing. 
I would like to thank everyone in our group. Thank you, Jos´e Luis Ambite, for those lengthy (and fun) discussions of world history (in particular, the history of Spain, Taiwan, and China). Thank you, Mark Carman, for being a caring friend and helping me on the research. Thank you, Matt Michelson and Martin Michalowski, for your humor that made the o.ce a fun working place. Thank you, Anon Plangprasopchok, for proofreading my papers and discussing homework assignments and tough math problems. Thank you, Alma Nava, for helping me on those administration issues at ISI. 
I cannot live such a great life in Los Angeles without my friends; thank you, Chih-Kuei Sung, Yen-Ming Lee, Jimmy Pan, Sean Lo, John Wu, and Hui-Ling Hsieh. Thank you, Jason Chen and Shou-de Lin, for those discussions and advise in my earlier research life at USC. Thank you, Ling Kao, for always being around sharing those stressful moments. Thank you Dan Goldberg for showing me how to surf. Thank you, Chia-Hsiang Yang, for those late night 5k and 10k runs and being a great companion for overnight studies. Thank you, Chih-Wei Chang, you know how much I want to play StarCraft II with you, and it was truly a motivation for ﬁnishing my dissertation early. Thank you, Hung Yuan Su, you are like a brother to me. 
I would like to thank my family for their love and support. My parents were always there for me when I had the most frustrating moments. Thank you my sister, Weili, for helping me on those tedious experiments. 
Last, thank you, my dearest girl friend, Han-Chun, for your patience, caring, and love. 
This research is based upon work supported in part by the University of Southern California under the Viterbi School Doctoral Fellowship, in part by the National Science Foundation under Award No. IIS-0324955, in part by the Air Force O.ce of Scientiﬁc Research under grant number FA9550-04-1-0105, and in part by the United States Air Force under contract number FA9550-08-C-0010. 
The U.S. Government is authorized to reproduce and distribute reports for Gov.ernmental purposes notwithstanding any copyright annotation thereon. The views and conclusions contained herein are those of the author and should not be interpreted as necessarily representing the o.cial policies or endorsements, either expressed or implied, of any of the above organizations or any person connected with them. 
Table of Contents 

Dedication ii 
Acknowledgments iii 
List Of Tables ix 
List Of s x 
Abstract xv 
Chapter 1: Introduction 1 
1.1 MotivationandProblemStatement...................... 1 

1.2 ThesisStatement ................................ 6 

1.3 Approach .................................... 6 

1.4 PreviousWorkonMapImageryConﬂation . . . . . . . . . . . . . . . . . 9 
1.5 ContributionsoftheResearch ......................... 10 

1.6 OutlineoftheThesis .............................. 11 

Chapter 2: Automatic Decomposition and Alignment of Raster map 12 
2.1 AutomaticDecompositionofRastermap . . . . . . . . . . . . . . . . . . 13 
2.1.1 Automatically Extracting Foreground pixel from Raster map . . 13 
2.1.2 Automatically Identifying Road and Text pixel in Foreground pixel 20 
2.1.2.1 Text/GraphicsSeparation. . . . . . . . . . . . . . . . . . 21 
2.1.2.2 Parallel-Pattern Tracing (PPT) . . . . . . . . . . . . . . 24 
2.2 AutomaticDetectionofRoadIntersections . . . . . . . . . . . . . . . . . 30 
2.2.1 Automatically Generating Road Geometry . . . . . . . . . . . . . 34 
2.2.1.1 BinaryDilationOperator .................. 34 

2.2.1.2 BinaryErosionOperator .................. 35 

2.2.1.3 ThinningOperator...................... 38 

2.2.2 Automatically Detecting Road Intersections from Road Geometry 40 
2.2.2.1 Detecting Road-Intersection Candidates . . . . . . . . . . 40 
2.2.2.2 Filtering Road-Intersection Candidates . . . . . . . . . . 41 
2.2.2.3 Localized Template Matching (LTM) . . . . . . . . . . . 44 
2.3 Experiments................................... 46 

2.3.1 ExperimentalSetup .......................... 48 

vi 

2.3.2 	EvaluationMethodology ........................ 48 

2.3.3 	ExperimentalResults ......................... 51 

2.3.4 	ComputationTime ........................... 56 

2.4 Conclusion ................................... 57 

Chapter 3: Automatic Extraction of Road Vector Data 	59 
3.1 SupervisedSeparationofRoadLayers . . . . . . . . . . . . . . . . . . . . 63 
3.1.1 	ColorQuantization........................... 63 

3.1.2 	UserLabeling .............................. 67 

3.1.3 	Automatically Identifying Road Colors  Example Labels . . . 67 
3.2 AutomaticExtractionofRoadVectorData . . . . . . . . . . . . . . . . . 73 
3.2.1 	Single-Pass Parallel-Pattern Tracing (SPPT) . . . . . . . . . . . . 75 
3.2.2 	Automatically Generating Road Geometry . . . . . . . . . . . . . 77 
3.2.3 	Automatically Extracting Accurate Road-Intersection Templates . 78 
3.2.3.1 Labeling Potential Distortion Areas . . . . . . . . . . . . 78 
3.2.3.2 Identifying and Tracing Road-Line Candidates . . . . . . 80 
3.2.3.3 Updating Road-Intersection Templates . . . . . . . . . . 84 
3.2.4 	Automatically Vectorizing Road Geometry  Accurate Road-IntersectionTemplates ......................... 89 
3.2.5 	Divide-and-Conquer Extraction of Road Vector Data . . . . . . . . 91 
3.3 Experiments................................... 94 

3.3.1 	ExperimentalSetup .......................... 96 

3.3.2 	EvaluationMethodology ........................ 98 

3.3.3 	ExperimentalResults ......................... 100 

3.3.4 	ComputationTime ........................... 111 

3.4 Conclusion ................................... 112 

Chapter 4: Automatic Recognition of Text Labels 	113 
4.1 SupervisedExtractionofTextLayers. . . . . . . . . . . . . . . . . . . . . 116 
4.1.1 	ColorQuantization........................... 118 

4.1.2 	UserLabeling .............................. 118 

4.1.3 	Automatically Identifying Text Colors  Example Labels . . . 120 
4.2 AutomaticRecognitionofTextLabels . . . . . . . . . . . . . . . . . . . . 127 
4.2.1 	Conditional Dilation Algorithm (CDA) . . . . . . . . . . . . . . . 128 
4.2.1.1 Input ............................. 130 

4.2.1.2 TheFirstPass ........................ 131 

4.2.1.3 TheVeriﬁcationPass .................... 136 

4.2.1.4 Output ............................ 136 

4.2.2 	Divide-and-Conquer Identiﬁcation of Text Labels . . . . . . . . . . 138 
4.2.3 	Automatically Detecting String Orientation . . . . . . . . . . . . . 140 
4.2.4 	Automatically Recognizing Characters  OCR Software . . . . 143 
4.3 Experiments................................... 144 

4.3.1 	ExperimentalSetup .......................... 146 

4.3.2 	EvaluationMethodology ........................ 146 

4.3.3 	ExperimentalResults ......................... 151 

vii 

4.3.4 	ComputationTime ........................... 157 

4.4 Conclusion ................................... 158 

Chapter 5: Related Work 	159 
5.1 SeparationofFeatureLayers ......................... 160 

5.2 Separation and Recognition of Feature Layers . . . . . . . . . . . . . . . . 162 
5.2.1 	Separation and Recognition of Feature Layers WithIntensiveUserInteraction.................... 162 
5.2.2 	Separation and Recognition of Feature Layers WithPriorKnowledge ......................... 163 
5.3 Road Layer Separation and Road Vectorization . . . . . . . . . . . . . . . 164 
5.4 TextLayerSeparationandTextRecognition . . . . . . . . . . . . . . . . 168 
Chapter 6: Conclusion and Future Extensions 	172 
6.1 Contributions .................................. 172 

6.2 MapProcessingHeuristics ........................... 174 

6.3 FutureExtensions ............................... 176 

6.4 Conclusion ................................... 177 

Bibliography 	179 
List Of Tables 


2.1 	Numberofpixelineachcluster ....................... 20 

2.2 	The average precision, recall, and F-measure for the various map sources . 52 
2.3 	Experimental results with respect to the map scale . . . . . . . . . . . . . 55 
3.1 	Testmap .................................... 95 

3.2 	The number of colors in the image for user labeling of each test map and the number of user labels for extracting the road pixel . . . . . . . . . . . 100 
3.3 	The average positional o.set, average orientation o.set, and connectivity o.set (the number of missed lines for each map source) . . . . . . . . . . . 101 
3.4 	Numeric results of the extracted road vector data (three-pixel-wide bu.er) StraboandR2V ............................. 104 
4.1 	Testmap .................................... 146 

4.2 	The number of extracted text layers and the number of user labels for extractingthetextlayers ........................... 152 
4.3 	Numeric results of the text recognition from test map  Strabo and ABBYY (P. is precision, R. is recall, and F. is the F-Measure) . . . . . . 153 
List Of s 


1.1 	The tourist map contains rich information that is di.cult to ﬁnd elsewhere forthecityofTehran,Iran .......................... 4 
1.2 	Exploiting geospatial information in raster map . . . . . . . . . . . . . . 5 
1.3 	The overall approach to harvesting geographic features from heterogeneous rastermap ................................... 7 
1.4 	Examplerastermap .............................. 8 

2.1 	The overall approach of the AutoMapDecomposer and AutoIntDetector . 14 
2.2 	An example USGS topographic map after converting to grayscale . . . . . 16 
2.3 	The pseudo-code of the grayscale-histogram analysis (GHA) algorithm . . 17 
2.4 	Identifying Cluster 1 ’s left boundary  the Triangle method . . . . . . 18 
2.5 	An example USGS topographic map after extracting the foreground pixel 20 
2.6 	Text/graphics separation  an example double-line map from Yahoo map....................................... 23 
2.7 	Examplesingle-lineanddouble-linemap . . . . . . . . . . . . . . . . . . 24 
2.8 	Basicsingle-lineelements ........................... 25 

2.9 	 the 45o basic element (black cells) to constitute a 30o line segment (bothblackandgraycells) .......................... 26 
2.10Basicdouble-lineelements ........................... 27 

2.11 The pseudo-code of the Parallel-Pattern Tracing algorithm . . . . . . . . . 28 
2.12 Examples of double-line roads and the parallel patterns . . . . . . . . . . 28 
2.13  the Parallel-Pattern Tracing algorithm to detect road format and roadwidth ................................... 31 
2.14 Applying the Parallel-Pattern Tracing algorithm on an example USGS to.pographicmap ................................. 32 
2.15 Examples of the Parallel-Pattern Tracing algorithm exceptions . . . . . . 32 
2.16 Separating the road layer  an example single-line map from TIGER/Line 33 
2.17 Dilation(backgroundisshowninwhitecells) . . . . . . . . . . . . . . . . 35 
2.18 The e.ect of the binary dilation operator (background is shown in white cells)....................................... 35 
2.19 Example results after applying the binary dilation operator . . . . . . . . 36 
2.20 Erosion(backgroundisshowninwhitecells) . . . . . . . . . . . . . . . . 36 
2.21 The e.ect of the binary erosion operator (background is shown in white cells)....................................... 37 
2.22 Example results after applying the binary dilation and erosion operators . 37 
2.23 The thinning operator (background is shown in white cells) . . . . . . . . 38 
2.24 Example results after applying the thinning operator with and without the erosionoperator ................................ 39 
2.25 Example results after applying the binary dilation, binary erosion, and thinningoperators ............................... 39 
2.26 Examples of salient points (background is shown in white cells) . . . . . . 41 
2.27 The intersection candidates (gray circles) of a portion of an example map fromTIGER/Line ............................... 42 
2.28 Constructing lines to compute the road orientations . . . . . . . . . . . . 43 
2.29 Example templates for the double-line and single-line road formats . . . . 45 
2.30 The Localized Template Matching algorithm . . . . . . . . . . . . . . . . 45 
xi 

2.31 An example TIGER/Line map before/after applying the Localized Tem.plateMatchingalgorithm ........................... 47 
2.32 Example results of the automatic road-intersection extraction . . . . . . . 53 
2.33 The precision and recall with respect to the positional displacements . . . 54 
2.34 An example low-resolution raster map (TIGER/Line, 8 meters per pixel) 55 
2.35 One of the dominant factors of the computation time is the number of foregroundpixel ................................ 57 
3.1 	Distorted road lines near road intersections caused by the thinning operator 60 
3.2 	The overall approach of the RoadLayerSeparator and AutoRoadVectorizer 62 
3.3 	An example map tile and the color quantization results with their red, green,andblue(RGB)colorcubes ...................... 65 
3.4 	An example of the supervised extraction of road pixel . . . . . . . . . . . 68 
3.5 	Identifying road colors  the centerline property (background is shown inblack) ..................................... 69 
3.6 	The identiﬁed Hough lines (background is shown in black) . . . . . . . . . 71 
3.7 	A road template example (background is shown in black) . . . . . . . . . 72 
3.8 	The Edge-Matching results (background is shown in black) . . . . . . . . 73 
3.9 	An example results of the RoadLayerSeparator . . . . . . . . . . . . . . . 74 
3.10 The pseudo-code of the Single-Pass Parallel-Pattern Tracing (SPPT) algo.rithm ...................................... 76 
3.11 The overall approach to extract accurate road-intersection templates from theroadlayers ................................. 79 
3.12 Generating a blob image to label the distorted lines . . . . . . . . . . . . 81 
3.13 The pseudo-code for the Limited Flood-Fill algorithm . . . . . . . . . . . 82 
3.14 Tracing only a small portion of the road lines near the contact points . . . 83 
3.15Thethreeintersectingcases .......................... 85 

3.16 Adjusting the road-intersection position  the 	centroid point of the intersectionsoftheroad-linecandidates . . . . . . . . . . . . . . . . . . . 86 
3.17Mergednearbyblobs .............................. 88 

3.18 Example results of  the thinning operator only and the AutoRoad-Vectorizer .................................... 88 
3.19 Extracting road vector data from an example map . . . . . . . . . . . . . 90 
3.20 Thepseudo-codefortracinglinepixel . . . . . . . . . . . . . . . . . . . . 91 
3.21Overlappingtiles ................................ 92 

3.22 Merging two sets of road vector data from neighboring tiles . . . . . . . . 93 
3.23Examplesofthetestmap........................... 97 

3.24Qualitycomparison............................... 102 

3.25 Example results  Strabo and R2V of a cropped area from the ITM map103 
3.26 Examples of the road vectorization results of the ITM and GECKO map 106 
3.27 Examples of the road vectorization results of the GIZI and UNAfg map . 107 
3.28 Examples of the road vectorization results of the UNIraq map . . . . . . . 108 
3.29 Examples of the road vectorization results of the RM map . . . . . . . . . 109 
3.30 Examples of the road vectorization results from map of the MapQuest andOSMmap ................................. 110 
4.1 	The overall approach of the TextLayerSeparator and AutoTextRecognizer 117 
4.2 	An example map tile and the color quantization results with their red, green,andblue(RGB)colorcubes ...................... 119 
4.3 	Example text labels for characters of various colors . . . . . . . . . . . . . 120 
4.4 	The user labeling interface and the selected text label . . . . . . . . . . . 121 
4.5 	ThedecomposedimagesandtheRLSAresults . . . . . . . . . . . . . . . 122 
4.6 	An example of a text label with uniform background . . . . . . . . . . . . 124 
4.7 	Anexampleofnon-solidcharacters ...................... 125 

4.8 	 non-text label to identify text colors from raster map with non-solid characters .................................... 125 
4.9 	Example results of the supervised text-layer extraction . . . . . . . . . . . 126 
4.10 The pseudo-code for the conditional dilation algorithm (CDA) . . . . . . 129 
4.11 Generating a binary text layer from a text layer with non-solid characters 131 
4.12 The shortest distance between two character components does not depend onthecharacters’sizesandorientations . . . . . . . . . . . . . . . . . . . 133 
4.13 Comparing the angle baseline with the connecting angle to test the straight stringcondition ................................. 135 
4.14  the 	veriﬁcation pass to determine actual expansion pixel (back.groundisshowninwhite) ........................... 137 
4.15TheCDAoutput ................................ 139 

4.16Thedivide-and-conquerprocessing ...................... 140 

4.17 The pseudo-code for the single-string orientation detection algorithm . . . 141 
4.18 Detecting string orientation  the morphological-operators-based RLSA 143 
4.19 ExamplesoftheITM,GECKO,GIZImap . . . . . . . . . . . . . . . . . 147 
4.20 ExamplesoftheRM,UNAfg,Googlemap . . . . . . . . . . . . . . . . . 148 
4.21 ExamplesoftheLive,OSM,MapQuestmap . . . . . . . . . . . . . . . . 149 
4.22ExamplesoftheYahoomap .......................... 150 

4.23 The string “Zubaida” can be mis-identiﬁed as “epieqnz” . . . . . . . . . . 155 
4.24 Examples of the curved strings and their rotated images  the detected orientations ................................... 155 
4.25 The string “Hindu Kush” has a wide character spacing . . . . . . . . . . . 156 
4.26 An example string with a non-text object and the rotated image  the detectedorientation .............................. 157 
xiv 
Abstract 

Raster map o.er a great deal of geospatial information and are easily accessible compared to other geospatial data. However, harvesting geographic features locked in heterogeneous raster map to obtain the geospatial information is challenging. This is because of the varying image quality of raster map (e.g., scanned map with poor image quality and computer-generated map with good image quality), the overlapping geographic features in map, and the typical lack of metadata (e.g., map geocoordinates, map source, and original vector data). 
Previous work on map processing is typically limited to a speciﬁc type of map and often relies on intensive manual work. In contrast, this thesis investigates a general approach that does not rely on any prior knowledge and requires minimal user e.ort to process heterogeneous raster map. This approach includes automatic and supervised techniques to process raster map for separating individual layers of geographic features from the map and recognizing geographic features in the separated layers ( detecting road intersections, generating and vectorizing road geometry, and recognizing text labels). 
The automatic technique eliminates user intervention by exploiting common map properties of how road lines and text labels are drawn in raster map. For example, the road lines are elongated linear objects and the characters are small connected-objects. 
The supervised technique utilizes labels of road and text areas to handle complex raster map, or map with poor image quality, and can process a variety of raster map with minimal user input. 
The results show that the general approach can handle raster map with varying map complexity, color usage, and image quality. By matching extracted road intersections to another geospatial dataset, we can identify the geocoordinates of a raster map and further align the raster map, separated feature layers from the map, and recognized features from the layers with the geospatial dataset. The road vectorization and text recognition results outperform state-of-art commercial products, and with considerably less user input. The approach in this thesis allows us to make use of the geospatial information of heterogeneous map locked in raster format. 
Chapter 1 
Introduction 

In this chapter, I present the motivation for this thesis. I explain the challenges, outline my contributions, and brieﬂy describe my approach and previous work that my approach is based on. 
1.1 Motivation and Problem Statement 
Humans have a long history of  map. In particular, paper map have been widely used for documenting geospatial information and conveying the information to viewers. Because of the widespread use of Geographic Information Systems (GIS) and the avail.ability of low cost and high-resolution scanners, we can now obtain a huge numbers of scanned map in raster format from various sources. For instance, the United States Geological Survey (USGS)1 has been mapping the United States since 1879. The USGS topographic map cover the entire country with informative geographic features, such as contour lines, buildings, and road lines. The georeferenced USGS topographic map in raster format ( the digital raster graphic, DRG) are now accessible from the USGS 
1http://www.usgs.gov/ 

and the TerraServer-USA2 websites. Other map sources, such as online map repositories like the University of Texas Map Library,3 also provide information-rich map and histor.ical map for many countries. Websites such as OpenStreetMap4 and MultiMap5 provide high quality digital map generated directly from vector data ( computer-generated map) with valuable geospatial information, such as business locations. Moreover, we can harvest images from image search engines (e.g., Yahoo Image Search) and identify raster map among the images [Desai et al., 2005; Michelson et al., 2008]. 
Raster map not only are readily accessible compared to other geospatial data (e.g., vector data and imagery), but are also an important source of geospatial information. First, raster map provide the most complete set of data for certain geographic features, such as the USGS topographic map, which contain the contour lines of the entire United States. By f the publicly available USGS topographic map with imagery, we can provide terrain information for any location in the U.S. inexpensively and fast. In ad.dition, we can align parcel map to obtain the georeferenced parcel data for building an accurate geocoder [Goldberg et al., 2009] for the areas where the parcel data are otherwise not easily accessible. Second, raster map contain particular information that is di.cult to ﬁnd elsewhere. For example, the tourist map found  an image search engine on the Internet shown in  1.1(a) contains location information such as gas stations, 
2http://terraserver-usa.com/ 
3http://www.lib.utexas.edu/map/ 
4http://www.openstreetmap.org/ 
5http://www.multimap.com/ 


hotels, and road names of Tehran, Iran, while the hybrid view from Google map shown in  1.1(b) shows only the major roads and their labels. 
We can exploit the information in raster map in several ways: ﬁrst, we can fuse the raster map with other geospatial data as the example shown in  1.2(a). By aligning the tourist map to the imagery, we can create an enhanced integrated representation of the two geospatial datasets and provide additional information, which cannot be viewed by the imagery alone. Second, we can extract image layers of individual geographic features ( feature layers) from raster map and fuse the extracted layers to other geospatial data. For example, we can extract the text layer from the raster map and align the text layer to imagery to annotate the imagery as shown in  1.2(b). Third, we can recognize the features in the extracted feature layers to generate map context and improve understanding. For example, we can extract the road-intersection templates ( the positions of road intersections, the number of roads intersecting at each intersection, and the orientations of the intersecting roads) and road vector data from the road layers of raster map. The extracted road-intersection templates then can be used to extract roads from imagery [Koutaki and Uchimura, 2004].  1.2(c) shows that the aligned road-intersection templates can be used as seed templates to extract the roads from the imagery of Tehran, Iran, where we have limited access to the vector data. The extracted road topology and road vector data can be used as a matching feature to align the raster map, separated feature layers, and recognized features to other geospatial data that contain roads [Chen et al., 2008; Wu et al., 2007]. Similarly, we can recognize the text labels in the text layer to generate context for the imagery and produce map metadata for the indexing and retrieval of the raster map and imagery. 

(a) A tourist map on the Internet 

(b) The hybrid view of Tehran from Google map (a) F a tourist map with imagery 


(b) Labeling roads in imagery with a text layer separated from a map 

(c) Extracting roads from imagery  road-intersection templates extracted from a map 
 1.2: Exploiting geospatial information in raster map 
Harvesting geographic features from raster map is challenging for a number of rea.sons: ﬁrst, map are complex and contain overlapping layers of geographic features, such as roads, contour lines, labels, etc. Second, the image quality of raster map is sometimes poor due to the scanning and/or image compression processes for preparing the map in raster format. Third, the metadata of raster map is often not available, such as the vector data used to produce the raster map, map geocoordinates, legend information, etc. To overcome these di.culties, this thesis presents a general approach for harvesting geographic features from raster map. This approach can process raster map with vary.ing map complexity and image quality, and does not rely on any prior knowledge of the input map. 
1.2 Thesis Statement 

We develop a general approach to exploit the information in heterogeneous raster map. This approach decomposes the map into raster layers of geo.graphic features, recognizes features in the layers, and aligns the raster map, extracted layers, and recognized features to other geospatial data. 
1.3 Approach 

The overall approach to harvesting geographic features from heterogeneous raster map is shown in  1.3.  1.4 shows examples of input raster map. The input is either a computer-generated map, such as the TIGER/Line map shown in  1.4(a), or a scanned map, such as the USGS topographic map and Thomas-Brothers map shown in  1.4(b) and  1.4(c). Since the geographic features in raster map generally overlap, the ﬁrst step is to separate the raster map into individual layers of geographic fea.tures, which is called the map decomposition step. To process raster map with varying map complexity ( overlapping features) and image quality, the map decomposition step includes a fully automatic approach and a supervised approach with user train.ing. For raster map with good image quality (e.g., computer-generated raster map), I present a fully automatic technique that exploits the distinctive geometry of the desired geographic features (e.g., road lines) to decompose raster map into feature layers, namely the road layer and the text layer [Chiang et al., 2005, 2008]. For complex raster map, such as the map that contain non-road linear features, or map with poor image quality, I present supervised techniques including user labeling that require minimal user input to separate the road and text layers from raster map [Chiang and Knoblock, 2009c]. The supervised techniques can be used on scanned and compressed map that are otherwise di.cult to process automatically and tedious to process manually. 


(a) An example of TIGER/Line (b) An example of USGS topo-(c) An example of scanned Thomas-map graphic map Brothers map 
 1.4: Example raster map 
With the separated feature layers from the map decomposition step, the next step is to recognize geographic features in the feature layers. For the road layer, I present an au.tomatic technique to extract the road-intersection templates from the raster layer [Chiang and Knoblock, 2008; Chiang et al., 2005, 2008]. Since the road is a common geographic feature across many geospatial data sources, the set of road-intersection templates of a raster map can serve as a reference feature in a map conﬂation system [Chen et al., 2006a, 2008] to compute a transformation matrix for georeferencing the map and aligning the map with other geospatial data, such as imagery. Further, I present an automatic technique to extract the road vector data from the road layer based on the accurately extracted road-intersection templates [Chiang and Knoblock, 2009a,b]. Therefore, we can produce road vector data from raster map for areas where we have a limited access to such data. 
For the text layer, I present an automatic technique that ﬁrst identiﬁes individual text labels that are multi-oriented and in various font types and sizes in the text layer. Then, the automatic technique rotates each identiﬁed text labels to the horizontal direction and employs an optical character recognition (OCR) component to translate the labels into machine-editable text. 
1.4 Previous Work on Map Imagery Conﬂation 
Chen et al. [2004b] and Wu et al. [2007] present techniques to align road vectors to imagery. Chen et al. [2004b] utilize road-intersection templates extracted from road vector data to search a local area for identifying corresponding road intersections in imagery and then align the road vector data to the imagery  the identiﬁed intersection correspondences. Wu et al. [2007] use the road lines from road vector data to match the road areas in the imagery, which allows their approach to handle suburban areas where only a few road intersections exist. 
In our previous work [Chen et al., 2004a], we present an approach to automatically align raster map with orthoimagery. We ﬁrst exploit our previous work [Chen et al., 2004b] to identify the locations of road intersections in imagery, and we detect the road-intersection templates in raster map by  the technique described in this thesis (Chapter 2). Finally, we compute the alignment between the two point sets and use the conﬂation technique [Saalfeld, 1993] to align the map with the imagery. 
This thesis presents a general technique that takes a raster map as input, decomposes the map into layers of geographic features, and recognizes the geographic features ( the road-intersection templates, road vector data, and text labels) in the separated layers. By exploiting the previous work [Chen et al., 2004a,b; Wu et al., 2007] with the extracted road-intersection templates or road vector data, we can georeference the raster map, the extracted feature layers, and the recognized features and align them to other geospatial data. 
1.5 Contributions of the Research 
The key contribution of this thesis is a method for separating and recognizing geographic features from raster map and aligning the raster map, separated layers, and recognized features to other geospatial data. This thesis o.ers four major contributions: 
• 	
An automatic approach that separates road and text layers from raster map (Chap.ter 2) 

• 	
An automatic approach that extracts road-intersection templates from road layers for the alignment of raster map and other geospatial data (Chapter 2) 

• 	
A general approach for extracting and vectorizing road geometry from raster map (Chapter 3), which includes: 


– 	
A supervised approach that handles complex raster map or map with poor image quality for separating road layers from the map 

– 	
An automatic approach for extracting road vector data from road layers 


• 	A general approach for recognizing text labels in raster map (Chapter 4), which includes: 
– 	
A supervised approach that handles complex raster map or map with poor image quality for separating text layers from the map 

– 	
An automatic approach for recognizing text labels in text layers 


1.6 Outline of the Thesis 

The remainder of this thesis is organized as follows: Chapter 2 describes the automatic approach to separate road and text layers from raster map and to detect the road-intersection templates for map alignment. Chapter 3 presents the road vectorization technique, which includes a supervised approach to separate road layers from raster map and an automatic approach to extract the road vector data from the separated road layers. Chapter 4 explains the text recognition technique, which includes a supervised approach to separate text layers form raster map and an automatic approach to recognize the text labels in the separated text layers. 
Chapters 3 and 4 ﬁrst present the supervised techniques for handling complex raster map or map with poor image quality for separating the feature layers. Together with the automatic map decomposition technique described in Chapter 2, this approach pro.cesses heterogeneous raster map to separate their road and text layers with minimal user input. The two chapters then describe general techniques for automatically converting the geographic features in the separated road and text layers into a machine-editable format. 
Chapter 5 reviews related work. It ﬁrst presents the map processing research on separating or recognizing general geographic features, and then focuses on work closely related to this thesis. Chapter 6 concludes with the contributions of this thesis and discusses future research. 
Chapter 2 

Automatic Decomposition and Alignment of Raster map 
This chapter describes my ﬁrst and second contributions, which include my automatic map decomposition technique called the Automatic Map Decomposer (AutoMapDecom.poser) for separating the road and text layers from raster map and my automatic road-intersection extraction technique called the Automatic Intersection Detector (AutoInt-Detector) for extracting the road intersections from the separated road layer [Chiang et al., 2005, 2008]. Chen et al. [2008] utilize the techniques in this chapter to detect road-intersection templates ( the positions of road intersections, the number of roads intersecting at each intersection, and the orientations of the intersecting roads) from a set of raster map from various sources. They match the extracted road-intersection tem.plates to georeferenced orthoimagery to identify the geospatial extent of the map and align the map to the orthoimagery. By  their technique, we can further align the separated feature layers and recognized features with other geospatial data to generate a hybrid view and create context for the integrated data, such as annotating roads by aligning an extracted text layer from a street map to imagery. 
 2.1 shows the overall approach of the AutoMapDecomposer and AutoIntDe.tector. To separate the overlapping feature layers from raster map, the AutoMapDecom.poser exploits the distinctive geometric properties of the road lines and text characters to automatically separate the road and text pixel from raster map. The AutoIntDetector then links the extracted road pixel and traces the centerlines of the linked road objects to generate the road geometry for detecting the road intersections. The output of the AutoIntDetector is a set of road-intersection positions, the number of roads that meet at each intersection, and the orientation of each intersecting road. Both the AutoMapDe.composer and AutoIntDetector do not assume any prior knowledge of the input map, such as the color of the roads, vector data, or legend information. 
2.1 Automatic Decomposition of Raster map 
The AutoMapDecomposer includes two distinctive steps: the ﬁrst step is to extract the binary map, which contains the foreground pixel of the raster map. With the binary map, the second step separates the road pixel from the text pixel by removing the foreground pixel that do not hold the properties that constitute road lines. 
2.1.1 Automatically Extracting Foreground pixel from Raster map 
Since the foreground pixel of the raster map contain the feature layers ( road and text layers), the ﬁrst step for extracting the layers is to extract the foreground pixel, which is a problem that has no universal solution on all types of images [Henderson et al., 2009; Lacroix, 2009; Leyk and Boesch, 2010; Sezgin, 2004]. The AutoMapDecomposer utilizes a technique that analyzes the grayscale histogram to classify the histogram values 

( the luminosity levels) into the background and foreground clusters for extracting the foreground pixel. An edge detector [Ziou and Tabbone, 1998], such as the Canny edge detector [Canny, 1986], is sometimes used to extract the foreground features instead of histogram analysis; however, the edge detector is sensitive to noise, which makes it di.cult to use for common raster map without careful manual tuning [Habib et al., 1999]. My grayscale-histogram analysis (GHA) algorithm is based on the heuristics of the luminosity usage of raster map and generates a better result of the feature layers for further feature recognition tasks. The heuristics of the luminosity usage of raster map are: 
• 	
The background luminosity levels of a raster map have a dominant number of pixel; and the number of foreground pixel is signiﬁcantly smaller than the number of background pixel. 

• 	
The foreground luminosity levels have high contrast against the background lumi.nosity levels. 


To generate the grayscale histogram of the input raster map, the AutoMapDecomposer ﬁrst converts the original input raster map to an 8-bit grayscale ( 256 luminosity levels) image by assigning the average value of the red, green, and blue strength of each map pixel as the pixel’s luminosity level. For the color map shown in  2.2(a),  2.5(a) shows the converted grayscale map. 
With the grayscale histogram, the GHA ﬁrst partitions the histogram into luminos.ity clusters. Then, the GHA classiﬁes each of the clusters as either a background or foreground cluster.  2.3 shows the pseudo-code of the GHA. 

(a) An example of USGS topographic map (b) The example map in grayscale 
 2.2: An example USGS topographic map after converting to grayscale 
 2.4 shows the grayscale histogram of  2.5(a). In the grayscale histogram, the X-axis represents the luminosity levels, from black (0) to white (255), and the Y-axis represents the number of pixel for each luminosity level. For a peak in the histogram, the GHA exploits the triangle method by Zack et al. [1977] to ﬁnd the boundaries of a luminosity cluster that contains the peak. To ﬁnd the left/right boundary of a luminosity cluster, the GHA ﬁrst constructs a line called the triangle line from the peak to the origin/end point in the histogram. Then, the GHA computes the distance between each Y-value and the triangle line. The GHA identiﬁes the luminosity level that has its Y-value under the triangle line and has the maximum distance to the triangle line as the cluster’s boundary. The dashed line in  2.4 indicates the identiﬁed left boundary of Cluster 1. If no luminosity level exists with its Y-value under the triangle line, such as when constructing the triangle line between Peak 3 and the origin point to ﬁnd the 

 2.3: The pseudo-code of the grayscale-histogram analysis (GHA) algorithm 


left boundary of Cluster 3, then the GHA selects the origin or end points as the cluster boundary. For example, the left boundary of Cluster 3 is the origin point. 
Since the background luminosity levels have a dominant number of pixel, the GHA starts searching for the global maximum of the histogram values to identify the ﬁrst background cluster. As shown in  2.4, the GHA ﬁrst ﬁnds Peak 1 and then checks whether Peak 1 is closer to white (255) or black (0) in the histogram to determine which portion of the histogram ( the luminosity levels to the left or right of Peak 1 ) contains the foreground luminosity levels. If the global maximum is closer to white, such as Peak 1, the GHA uses white as the right boundary of the ﬁrst background cluster. This is because the foreground generally has high contrast against the background. For example, if the background contains a light gray of luminosity level 200, we often ﬁnd the foreground luminosity levels spreading in the histogram from 0 to 200 instead of 200 to 255. As a result, for Cluster 1 in  2.4, the GHA identiﬁes the end point (white) as the cluster’s right boundary and then locates the left boundary  the triangle method. If Peak 3 is the global maximum, the GHA selects the origin point (black) as the cluster’s left boundary and then uses the triangle method to locate the right boundary. 
After the GHA identiﬁes the ﬁrst background cluster, the algorithm searches for the next peak and uses the triangle method to locate the cluster boundaries until every luminosity level in the histogram belongs to a cluster. In the example, the GHA ﬁrst identiﬁes Cluster 1 as the ﬁrst background cluster. Then, since the foreground colors usually have a high contrast against the background colors, the cluster that is the farthest from the ﬁrst background cluster in the histogram is the ﬁrst foreground cluster ( Cluster 3 in the example). 
After the GHA identiﬁes the ﬁrst foreground cluster, for the remaining clusters, the algorithm classiﬁes them as either the foreground or background clusters based on the number of pixel in each cluster. This is because if a cluster uses a number of pixel similar to the ﬁrst background cluster, the cluster could contain some of the luminosity levels of the map background or the luminosity levels used to paint homogeneous-region features, such as parks, lakes, etc. Therefore, if the number of pixel of a cluster is closer to the number of pixel of the ﬁrst background cluster than the ﬁrst foreground cluster, the GHA classiﬁes it as a background cluster; otherwise the cluster is a foreground cluster. Formally, the the GHA calculates the pixel ratio for the classiﬁcation, which is deﬁned as: 
ClusterP ixels 

P ixelRatio = (2.1)
T otalNumbersOfP ixels 
Cluster 1  Cluster 2  Cluster 3  
Cluster pixel  316,846  237,876  85,278  
Pixel Ratio  50%  37%  13%  

Table 2.1: Number of pixel in each cluster 
Table 2.1 shows that the number of pixel in Cluster 2 is closer to Cluster 1 (back.ground) than Cluster 3 (foreground), so Cluster 2 is a background cluster. After the GHA classiﬁes each of the clusters, the algorithm removes the background clusters and the result is a binary map containing foreground pixel only, such as the example shown in  2.5(b). 

(a) The example map in grayscale 	(b) The binary map 
 2.5: An example USGS topographic map after extracting the foreground pixel 
2.1.2 	Automatically Identifying Road and Text pixel in Foreground pixel 
After the AutoMapDecomposer extracts the foreground pixel, the result is a binary map that is a union of overlapping feature layers. To separate the text layer from the road layer, the AutoMapDecomposer exploits the geometric properties of road lines and characters in a raster map with a text/graphic separation algorithm to separate text and line pixel. Text/graphics separation algorithms [Bixler, 2000; Cao and Tan, 2002; Fletcher and Kasturi, 1988; Li et al., 1999, 2000; Myers et al., 1996; Nagy et al., 1997; Tang et al., 1996; Vel´azquez and Levachkine, 2004] are designed for grouping small connected components ( small CCs) and separating them from linear structures ( elongated lines). Then, the AutoMapDecomposer automatically detects the road format and the road width  the set of identiﬁed line pixel to remove linear structures that are not road lines. 
2.1.2.1 Text/Graphics Separation 

The AutoMapDecomposer utilizes the text/graphics separation algorithm from Cao and Tan [2002] to ﬁrst separate the text layer and linear structures from the raster map based on the following geometric properties of road lines and text labels in a raster map: 
• 	
Road lines are connected to each other ( road network) so that a road layer usu.ally has a small number of connected components or even only one large connected component when the entire road layer is connected. 

• 	
Characters are isolated small connected components that are close to each other. 

• 	
Character strokes are generally shorter than lines that represent roads. 


The text/graphics separation algorithm ﬁrst removes solid areas  morphological operators and then removes small connected components in the binary map  a size threshold.  2.6(b) shows the remaining foreground pixel after the solid areas and small connected components are both removed. Then, from each removed small connected component, the text/graphics separation algorithm searches and groups neighboring con.nected components to identify string objects. The gray boxes in  2.6(c) show the identiﬁed string objects, and  2.6(d) shows the remaining foreground pixel after the string objects are removed. Finally, the remaining foreground pixel are divided into line segments and the length of each line segment is checked to determine if the line segment belongs to a linear structure (e.g., road lines) or a text object. Since the line segments that constitute characters ( strokes) are usually smaller than the ones that constitute lines, a length ﬁlter can be used to separate the characters that overlap with linear structures. 
The AutoMapDecomposer adds up the identiﬁed strings and the line segments of the characters that overlap with linear structures to produce the text layer as shown in  2.6(e). There are objects that are not text in the text layer, and these non-text objects will be ﬁltered out  an optical character recognition component during the text recognition. 
 2.6(f) shows the road layer after the text layer is removed from the binary map. The road layer might contain linear features that are not roads, such as contour lines. The AutoMapDecomposer removes the linear features that are not roads in the next subsection. Moreover, the extracted road layer contains broken lines since the characters that overlap with the road lines are removed. The broken lines are reconnected to generate the road geometry during the detection of road intersections in Section 2.2.1 of this chapter. 

(a) An example binary map (b) Removing solid areas and small CCs 

(c) 
Identifying strings (d) Remaining lines and overlapping char.acters 

(e) 
The extracted text layer (f) The extracted road layer 



2.1.2.2 Parallel-Pattern Tracing (PPT) 
To further remove the linear structures that are not roads in the road layer, the Au.toMapDecomposer utilizes the Parallel-Pattern Tracing (PPT) algorithm that traces the parallel patterns of lines to detect the road format and road width based on the following geometric properties of road lines in a raster map: 
• 	
The majority of the roads share the same road format: single-line or double-line roads, as the examples shown in  2.7, and the majority of the roads have the same road width within a map. 

• 	
In a double-line map, the linear structures in single-line format are not roads but can be other geographic feathers, such as contour lines. 



(a) A single-line map from TIGER/Line (b) A double-line map from Google map 
 2.7: Example single-line and double-line map 
In this subsection, I ﬁrst explain the parallel patterns that the PPT identiﬁes and then describe how the AutoMapDecomposer exploits the PPT to detect the road format and road width. 
For a pixel C in the grid domain ( the raster format), there are four possible one-pixel width straight lines constituted by C and C ’s eight connecting pixel, which represent the lines of 0 degrees, 45 degrees, 90 degrees, and 135 degrees respectively, as shown in  2.8. These four lines are the basic elements that constitute straight lines with various slopes passing through the pixel C. In  2.9, for example, to represent a 30-degree line, one of the basic elements, the 45-degree line-segment, is drawn in black, then additional gray pixel are added to tilt the line. 

 2.8: Basic single-line elements 
Based on the four basic single-line elements, if a pixel C is on a double-line road, there are eight possible parallel patterns of double-line roads as shown in  2.10 (the dashed cells are the possible corresponding parallel lines). Hence, if the PPT detects any of the eight parallel patterns for a given foreground pixel, the PPT classiﬁes the pixel as a double-line road pixel. 

 2.11 shows the pseudo-code of the PPT. To detect the parallel pattern, for a given road width (RW ), the PPT searches for the corresponding foreground pixel at a distance of RW in the vertical and horizontal directions. For example,  2.10 shows the detection of the parallel patterns  the crosses at the pixel C with the length of RW. The crosses locate at least two road-line pixel for each of the eight patterns; one is in the horizontal direction and the other one is in the vertical direction. 
 2.12 shows two examples of double-line roads. If a foreground pixel is on a horizontal or vertical road line, such as the pixel C in  2.12(a), the PPT can ﬁnd two foreground pixel along the road line within a distance of RW and at least another foreground pixel on the corresponding parallel road line in a distance of RW. If the orientation of the road line is neither horizontal nor vertical as shown in  2.12(b), the PPT can ﬁnd one foreground pixel in each of the horizontal and vertical directions on the corresponding road lines at a distance of RW as shown in  2.12(b). 

 2.10: Basic double-line elements 



(a) Road width is three pixel (b) Road width is four pixel 
 2.12: Examples of double-line roads and the parallel patterns 
By detecting the parallel pattern, the PPT determines if a foreground pixel is a double-line road pixel or a single-line road pixel at a given RW. To detect the road layer format, the AutoMapDecomposer applies the PPT on the binary map varying the road width from one to K pixel as the main function shown in the pseudo-code in  2.11. K is the maximum width in pixel of a double-line road in the map. For example, to detect a double-line road of 20 meters wide in a 2 meters-per-pixel map, K needs to be at least 10 pixel. A bigger K ensures that the PPT can ﬁnd wider roads, but it requires more processing time. 
After the AutoMapDecomposer applies the PPT, the algorithm computes the ratio of the number of double-line road pixel to the number of total foreground pixel in the binary map (double-line ratio) at each RW.  2.13 shows the double-line ratios of various double-line and single-line map. When the RW is one pixel, the double-line ratios are close to one (100%). After RW increases, the double-line ratios start to decline. This is because the foreground pixel tend to be near each other, and it is easier to ﬁnd corresponding pixel even if the PPT does not have the correct road width or the map is not a double-line map. 
If the map is a double-line map and the PPT applies on the map  the correct road width, a peak appears in the chart as shown in  2.13(a) since most of the road pixel have one of the parallel pattens detected by the PPT. For example, the double-line high-resolution map from ESRI map has a peak at two pixel. The double-line high-resolution map from MapQuest map and Yahoo map and the double-line map from USGS topographic map have peaks at four pixel in the chart. The map from TIGER/Line and the map from ESRI map and MapQuest map that are not high resolution are all single-line map, which do not have any peaks in the chart as shown in  2.13(b). 
 this method, the AutoMapDecomposer determines the road format automati.cally and also obtains the road width by searching for a peak. For example, from Fig.ure 2.13(a), the AutoMapDecomposer determines that the USGS topographic map is a double-line map with road width equal to four pixel. Hence, the AutoMapDecomposer applies the PPT setting RW to four pixel on the map to remove the foreground pixel that do not have any of the parallel patterns detected ( single-line pixel) as shown in  2.14, where the contour lines are removed. 
There are some exceptions to  the PPT to trace the parallel patterns. In Fig.ure 2.15, the PPT detects Pixel 1 to Pixel 8 as double-line pixel since the gray pixel are the corresponding pixel of Pixel 1 to Pixel 8 in the horizontal and/or vertical di.rections. For the Pixel A to Pixel D, the PPT cannot detect a parallel pattern for any of the pixel, so they are removed. Although the removal of Pixel A to Pixel D results in gaps between the line segments, the PPT detects the majority of road pixel and the gaps will be reconnected in the next steps to generate the road geometry and detect the road intersections. 
2.2 Automatic Detection of Road Intersections 
After applying the AutoMapDecomposer, we have a road layer that contains broken lines because of the removal of overlapping features, such as characters or contour lines.  2.16 shows an example TIGER/Line map after the text/graphics separation and 

(a) Double-line format road layers 

(b) Single-line format road layers 
 2.13:  the Parallel-Pattern Tracing algorithm to detect road format and road width 

(a) Before applying the PPT 

(b) After applying the PPT 
 2.14: Applying the Parallel-Pattern Tracing algorithm on an example USGS to.pographic map 

the road lines are broken. Moreover, if the road layer is in double-line format, such as the example in  2.6, we need to merge the parallel lines to produce the road geometry. The AutoIntDetector utilizes the binary morphological operators [Pratt, 2001] to reconnect the lines and build the road geometry for detecting the road intersections in the road layer. 

(a) An example binary map (b) Removing solid areas and small CCs 

(c) Identifying strings and overlapping characters  (d) The extracted road layer  
  2.16:  Separating  the  road  layer    an  example  single-line  map  from  
TIGER/Line  

2.2.1 Automatically Generating Road Geometry 
The binary morphological operators are based on the hit-or-miss transformation with vari.ous size masks [Pratt, 2001], which are often used in various document analysis algorithms as fundamental operations [Agam and Dinstein, 1996]. The hit-or-miss transformation ﬁrst uses a 3x3 pixel binary mask to scan over the input binary image. If the mask matches the underlying pixel, it is a “hit”; otherwise, it is a “miss”. Each morpholog.ical operator uses a di.erent mask to perform the hit-or-miss transformation and has a di.erent action as the result of a “hit” or “miss”. 
2.2.1.1 Binary Dilation Operator 

The e.ect of the binary dilation operator is to expand the region of foreground pix.els [Pratt, 2001] and the AutoIntDetector uses it to thicken the lines and reconnect adja.cent pixel. As shown in  2.17, if a background pixel has any foreground pixel in its eight adjacent pixel ( a “hit”), the dilation operator converts the background pixel to a foreground pixel ( the action resulting from the “hit”). For example,  2.18 shows that after two iterations, the binary dilation operator ﬁxes the gap between the two lines. Moreover, if the roads are in double-line format, the binary dilation operator combines the two parallel lines to a single line. The number of iterations determines the maximum gap size that the AutoIntDetector needs to ﬁx. For example, the AutoIntDe.tector can connect the gaps smaller than six pixel with three iterations of the binary dilation operator. For double-line raster map, the AutoIntDetector selects the number of iterations based on the width of the roads in order to merge parallel lines.  2.19(a) shows the result after performing the binary dilation operator on  2.6(f) where the parallel road lines are merged into single lines, except on intersection area that is larger than the dynamically generated dilation iterations. For single-line map, the AutoIntDe.tector utilizes three iterations to ﬁx gaps smaller than six pixel, which is usually enough to merge most of the gaps since the gaps result from removing small overlapping com.ponents.  2.19(b) shows the result after performing three iterations of the binary dilation operator on  2.16(d) where the gaps are connected. 

 2.17: Dilation (background is shown in white cells) 
2.2.1.2 Binary Erosion Operator 

The idea of the binary erosion operator is to reduce the region of foreground pixel [Pratt, 2001]. The AutoIntDetector uses the binary erosion operator to reduce the area of each thickened line and maintain an orientation similar to the original orientation prior to 


(a) An example result of a double-line road (b) An example result of a single-line road layer layer from Yahoo map from TIGER/Line 
 2.19: Example results after applying the binary dilation operator 
applying the binary dilation operator. If a foreground pixel has any background pixel in its eight adjacent pixel ( a “hit”), the binary erosion operator converts the fore.ground pixel to a background pixel ( the action resulting from the “hit”) as shown in  2.20. For example,  2.21 shows that after two iterations, the binary erosion operator reduces the width of the thickened lines.  2.22(a) and  2.22(b) show the results after applying the binary erosion operator on  2.19(a) and  2.19(b) respectively. 



(a) An example result of a double-line road (b) An example result of a single-line road layer layer from Yahoo map from TIGER/Line 
2.2.1.3 Thinning Operator 

After applying the binary dilation and erosion operators, the road lines are in various widths. To extract the centerlines of the roads to represent the road geometry, the Au.toIntDetector utilizes the thinning operator to produce one-pixel-wide roads.  2.23 shows the e.ect of the thinning operator. The AutoIntDetector does not use the thinning operator immediately after the binary dilation operator because the binary erosion opera.tor has the opposite e.ect of the binary dilation operator, which prevents the orientations of the road lines near the intersections from being signiﬁcantly distorted as shown in Fig.ure 2.24. The AutoIntDetector utilizes a generic thinning operator that is a conditional erosion operator with an extra conﬁrmation step [Pratt, 2001]. The ﬁrst step of the thin.ning operator is to mark every foreground pixel that connects to one or more background pixel ( the same idea as the binary erosion operator) as a candidate to convert to the background. Then, the conﬁrmation step checks if the conversion of a candidate causes any disappearance of original line branches to ensure that the basic structures of the original objects are not compromised.  2.25(a) and  2.25(b) show the results after performing the thinning operator on  2.22(a) and  2.22(b), respectively. 


(a) After the dilation (b) Without the erosion (c) With the erosion 
 2.24: Example results after applying the thinning operator with and without the erosion operator 

(a) An example result of a double-line road (b) An example result of a single-line road layer layer from Yahoo map from TIGER/Line 
 2.25: Example results after applying the binary dilation, binary erosion, and thinning operators 
2.2.2 Automatically Detecting Road Intersections from Road Geometry 
In this step, the AutoIntDetector automatically extracts the road intersections, connec.tivity ( the number of roads that meet at an intersection), and orientations of the roads intersecting at each intersection from the road geometry. 
2.2.2.1 Detecting Road-Intersection Candidates 
A road intersection is a point at which more than two lines meet with di.erent tangents. To detect possible intersection points, the AutoIntDetector starts by  an interest operator. The interest operator detects salient points in an image as starting points for image recognition algorithms. The AutoIntDetector uses the interest operator proposed by Shi and Tomasi [1994] and implemented in OpenCV1 to ﬁnd the salient points as the road-intersection candidates. 
The interest operator checks the color variation around every foreground pixel to identify salient points and assigns a quality value based on the color variation to each identiﬁed salient point. For example,  2.26 shows the salient points of Pixel 1 to Pixel 5. Pixel 1 and Pixel 3 have higher quality values than other salient points since Pixel 1 and Pixel 3 have more intersecting lines resulting from higher color variations around the pixel. 
The AutoIntDetector discards the salient point that lies within a predeﬁned radius R of some salient points with higher quality values and uses the remaining salient points as the road-intersection candidates. With the radius R deﬁned as ﬁve pixel, the AutoInt-Detector discards Pixel 2 and keeps Pixel 1 as a road-intersection candidate since Pixel 
1http://sourceforge.net/projects/opencvlibrary, GoodFeaturesToTrack function 

1 has a higher quality value. Similarly, the AutoIntDetector discards Pixel 4 because it lies within the ﬁve-pixel radius of Pixel 3. The AutoIntDetector keeps Pixel 5 as a road-intersection candidate since it is not close to any other salient points with a higher quality value. The radius R of ﬁve pixel is selected through experimentation. Consider.ing the fact that road intersections are not generally near each other in a raster map, the radius of ﬁve pixel is reasonable for detecting road-intersection candidates. 
2.2.2.2 Filtering Road-Intersection Candidates 
The identiﬁed road-intersection candidates are salient points, which can be a road pixel on a road line where the slope of the road suddenly changes, such as Pixel 5 in Fig.ure 2.26. Since every road intersection is a point where two or more line segments meet, the AutoIntDetector uses the connectivity of a salient point to determine actual road intersections among the identiﬁed road-intersection candidates. 
The AutoIntDetector draws a rectangle around each road-intersection candidate, as shown in  2.27, to determine the connectivity. The connectivity of a road-intersection candidate is the number of foreground pixel that intersects with this rect.angle since the road lines are all one pixel wide. If the connectivity is less than three, the AutoIntDetector discards the candidate; otherwise the AutoIntDetector identiﬁes the candidate as a road intersection. Subsequently, since the road lines in a raster map are straight within a small distance ( several meters), the AutoIntDetector links the road intersection to the intersected foreground pixel on the rectangle boundaries to compute the slopes ( orientations) of the intersecting road lines as shown in  2.28. 

(a) Intersection candidates 

(b) Not an intersection (c) An intersection 
 2.27: The intersection candidates (gray circles) of a portion of an example map from TIGER/Line 
The rectangles around road-intersection candidates should cover only straight road lines for the road orientation to be accurate. In the example shown in  2.27, 
the AutoIntDetector uses an 11x11-pixel rectangle on the raster map with resolution 2 meters per pixel assuming the road lines are straight within ﬁve pixel or ten meters (e.g., the width of the rectangle is a line of length 11 pixel divided into ﬁve pixel to the left, one center pixel, and ﬁve pixel to the right). Although the e.ective rectangle size can vary depending on the raster map and the map scale, the AutoIntDetector uses a small rectangle size to assure that even with raster map of lower resolutions, the assumption that the rectangle should cover only straight road lines is still valid. 

 2.28: Constructing lines to compute the road orientations 
The AutoIntDetector does not trace the pixel between the road intersection and the intersected pixel at the rectangle boundaries, which could introduce errors if the intersected pixel are from other road lines that do not intersect at the road intersection or if the road lines within the rectangle are not straight. This usually happens in low-resolution map; however, in general, the rectangle is much smaller than the size of a street block, and it is unlikely to contain non-intersecting or non-straight road lines. Moreover, the AutoIntDetector saves signiﬁcant computation time by avoiding the tracing of every possible road line between each of the road intersections and the rectangle boundaries. 
2.2.2.3 Localized Template Matching (LTM) 
Because of the usage of the binary morphological operators, the AutoIntDetector might shift the road lines in the road geometry from their original positions or even create false branches. Therefore, after the AutoIntDetector extracts the road intersections, the algorithm uses the Localized Template Matching (LTM) algorithm [Chen et al., 2008] to improve the accuracy of the extracted road intersections. 
For every extracted road intersection, the AutoIntDetector constructs a road tem.plate based on the road format, connectivity, and road orientations. For example, Fig.ure 2.29(a) shows a road intersection with connectivity equal to four, and the orientations of the intersected roads are 0, 90, 180, and 270 degrees, respectively. If the raster map is a double-line map, the AutoIntDetector uses the road width detected by the PPT with one-pixel-wide parallel lines to construct the template as shown in  2.29(b); other.wise, the AutoIntDetector uses one-pixel wide lines to construct a single-line template as shown in  2.29(c). After constructing the templates, the AutoIntDetector utilizes the LTM to search locally from the positions of the extracted road intersection, as shown in  2.30. The LTM locates regions in the binary raster map that are most similar in terms of the geometry to the templates. The outputs of the LTM are the positions of the matched templates and a set of similarity values. For a road intersection, if the similarity is larger than a similarity threshold, the AutoIntDetector adjusts the position of the in.tersection to the matched point determined by the LTM; otherwise the AutoIntDetector discards the road intersection. 

(a) Extracted intersection (b) Double-line format (c) Single-line format 
 2.29: Example templates for the double-line and single-line road formats 

(a) Searching within a local area (b) Identiﬁed a match 
 2.30: The Localized Template Matching algorithm 
As shown in  2.31, the AutoIntDetector adjusts the circled intersections to the precise location in the original raster map, and the arrows show the directions of the adjustments. The di.erences are only a few pixel, so the ﬁgures need to be studied carefully to see the di.erences. For example, in the upper-left circle of  2.31(a), AutoIntDetector adjusts the intersection several pixel lower to match the exact intersec.tion location in the original map; for the three circles on the bottom, AutoIntDetector moves the intersections to their right for exact matches. 
2.3 Experiments 

In this section, I report my experiments on the techniques described in this chapter  raster map from various sources. I experimented with computer-generated map and scanned map from 12 sources, including ESRI map, MapQuest map, TIGER/Line, Yahoo map, A9 map, MSN map, Google map, Map24 map, ViaMichelin map, Multimap map, USGS topographic map, and Thomas Brothers map, covering cities in the United States and some European countries. 
I arbitrarily selected 80 detailed street map with scales range from 1.85 to 7 meters per pixel.2 In addition, I deliberately selected seven low-resolution map with scales ranging from 7 to 14.5 meters per pixel to test my approach on complex raster map that have signiﬁcant overlaps between lines and characters. 
2Some of the sources do not provide scale information. 

(a) Before applying the LTM 

(b) After applying the LTM 
 2.31: An example TIGER/Line map before/after applying the Localized Template Matching algorithm 
2.3.1 Experimental Setup 

To demonstrate the capability for handling a variety of map, I did not use any prior information of the input map in the experiment. Instead, I used a set of default parame.ters for all input map based on an initial experiment on a small set of data (disjoint with my test dataset in this experiment). I could optimize these parameters for one particular source to produce the best results if prior knowledge of the sources were available. 
In the text/graphics separation step, the size of a small connected component was 20x20 pixel. In the step to extract the road geometry  the morphological operators, the number of iterations for the binary dilation and erosion operators for a single-line map were three and two, respectively ( a gap smaller than six pixel could be ﬁxed). For a double-line map, I dynamically generated the number of iterations based on the detected road width. In the step to identify actual intersections, connectivity, and road orientations, the rectangle for calculating the connectivity was a 21x21-pixel box (10 pixel to the left, 10 pixel to the right, and one center pixel). In the LTM step, the similarity threshold was 50%. 
2.3.2 Evaluation Methodology 

The output of the AutoMapDecomposer was the separated road and text layers. The evaluation of a successful layer separation result was carried out by the feature recogni.tion results: the AutoIntDetector could proceed on detecting road intersections from the separated road layer if and only if the AutoMapDecomposer successfully separated the road and text layer from raster map. 
The output of the AutoIntDetector was a set of road-intersection positions, the num.ber of roads that meet at each intersection, and the orientation of each intersecting road. The AutoIntDetector output is the key for allowing a map conﬂation system ( a conﬂation system that aligns a map to other geospatial data, such as orthoimagery) to determine the map extent and align the map to other geospatial data. Therefore, the following evaluation metrics of the road intersections are based on supporting an accurate map-alignment result, which are discussed in turn in this subsection. 
The ground truth of a road intersection is deﬁned as the intersection points of two or more road lines for single-line map or any pixel inside the intersection areas where two or more roads intersect for double-line map. A correctly extracted road intersection is deﬁned as follows: if there exists a road intersection in the original raster map within a N -pixel radius to the extracted road intersection, the extracted road intersection is a correctly extracted road intersection. I report the results varying N from zero to ﬁve for a subset of test data in Section 2.3.3. For any other places in this section, I report the results  N equal to ﬁve pixel. From my observation, if an extracted intersection is within ﬁve-pixel radius to any road intersections in the original map, it is usually an intersection shifted during the extraction; otherwise it is more likely a false positive. 
I report the precision (correctness) and recall (completeness)3 for the accuracy of the extracted road intersections. The precision is given by: 
NumberofCorrectlyExtractedRoadIntersections 
P recision = (2.2)
NumberofExtractedRoadIntersections 
3The terms precision and recall are common evaluation terminologies in information retrieval and correctness and completeness are often used alternatively in geomatics and remote sensing [Heipke et al., 1997]. 
The recall is given by: 

NumberofCorrectlyExtractedRoadIntersections 
Recall = (2.3)
NumberofGroundT ruthRoadIntersections 
The precision and recall of the intersection results represent the capability of extracting an abstract representation of the road geometry from a raster map. To achieve the map alignment in a map conﬂation system, high precision is required to support the matching process to identify the corresponding road network in the other geospatial dataset, such as imagery Chen et al. [2006a, 2008]. Chen et al. [2006a] report that an average of 76% precision can support the map system to align a set of 60 map to the imagery. 
After the map conﬂation system ﬁnds a corresponding road network in the other geospatial dataset for the map alignment, the next step is to transform the map to the geospatial dataset according to the corresponding locations of the road intersections in the map and the geospatial dataset. Therefore, I report the displacement quality of the road-intersection locations, which is the distance between the extracted road intersections and the ground truth. I randomly selected two map from each source to examine the positional displacement. 
The intersection positions, connectivity, and road orientations help to reduce the search space in a map conﬂation system for ﬁnding a match between the map and the other geospatial data [Chen et al., 2008]. Therefore, I evaluate the geometric similarity to evaluate the similarity between the ground truth in the binary map and the extracted intersection positions, connectivity, and road orientations. 
For an intersection template T with an image size of w x h pixel and the binary raster map B, the geometric similarity is deﬁned as: 
∑∑ 

=1 T (x, y)B(X + x, Y + y)
=1 

∑∑
=1 T (x, y)2 =1 B(X + x, Y + y)2 
=1 
wxhy
wxhywx
GS(T )= √∑
=1 
hy
(2.4)

∑ 

where T (x, y) is one, if the pixel at (x, y) in the image of the intersection template is a foreground pixel; otherwise T (x, y) is zero. B(x, y) is one, if the pixel at (x, y) in the binary map is a foreground pixel; otherwise B(x, y) is zero. In other words, the geometric similarity is a normalized cross correlation between the template and the ground truth, which ranges from zero to one [Chen et al., 2006a]. 
2.3.3 Experimental Results 

The AutoMapDecomposer successfully separated the road and text layers from every test map in the experiments and then the AutoIntDetector processed the separated road layers for detecting the road intersections. Table 2.2 shows the numerical results of the average numbers of the precision, recall, and F-measure of my experiments. The average precision was 95% and the recall was 75%  the set of parameters discussed in Section 2.3.1. In particular, for map sources like the A9 map, Map24 map, and ViaMichelin map, the precision numbers were 100% because of the ﬁne quality of their map (e.g., less noise and roads of the same width). In my experiments, the USGS topographic map had the lowest precision and recall besides the low-resolution raster map. This was because the USGS topographic map had more feature layers than other map sources and the image quality of USGS topographic map was not as good as the computer-generated raster map due to the scanning and compression processes.  2.32 shows two example results from my experiments. In these ﬁgures, an “X” marks one extracted road intersection and the numbers were only for manually verifying the results. 
Map Source  Map Count  Precision  Recall  F-Measure  
ESRI map  10  93%  71%  81%  
MapQuest map  9  98%  66%  79%  
TIGER/Line map  9  97%  84%  90%  
Yahoo map  10  95%  76%  84%  
A9 map  5  100%  93%  97%  
MSN map  5  97%  88%  92%  
Google map  5  98%  86%  91%  
Map24 map  5  100%  82%  90%  
ViaMichelin map  5  100%  98%  99%  
Multimap map  5  98%  85%  91%  
USGS topographic map  10  82%  60%  69%  
Thomas Brothers map  2  98%  65%  79%  

Table 2.2: The average precision, recall, and F-measure for the various map sources 
The average positional displacements for the 24 randomly selected map (two from each source) was were pixel and the Root Mean Squared Error (RMSE) was 0.82, which shows that the majority of the extracted intersections were within one-pixel radius to the actual intersections in the map.  2.33 shows the recall and precision  the positional displacement, N, which varies from zero pixel ( the extracted intersection was at the exact location of the ground truth) to ﬁve pixel. Intuitively, the precision and recall were higher with a larger N since more extracted intersections were considered as correctly extracted ones, which means that for the applications that do not require the exact locations of the intersections, higher numbers for precision and recall could be achieved. For example, for an application that utilizes the extracted intersections as seed templates to search for road pixel in aerial imagery [Koutaki and Uchimura, 2004], 

(a) An example TIGER/Line map 

(b) An example USGS topographic map 
the application needs as many intersections as possible and has a low requirement for positional accuracy ( N can be larger). For a map conﬂation system, the require.ment for positional accuracy is high ( a smaller N ) since the conﬂation system needs accurate intersection positions to match another set of road intersections from a second source [Chen et al., 2008]. 

 2.33: The precision and recall with respect to the positional displacements 
The average geometric similarity between the extracted intersection and the ground truth is 72%. I could further improve the similarity by tracing the line pixel to generate the templates when identifying the connectivity, but pixel tracing would require more computation time. In the map conﬂation system built by Chen et al. [2008], they utilized the techniques described in this chapter and successfully used the road connectivity and orientations to reduce the time of ﬁnding a matching between two sets of road intersections from a raster map and imagery. 
For comparison, I selected seven low-resolution map (scales range from 7 meters per pixel to 14.5 meters per pixel) to conduct experiments on more complex raster map with signiﬁcant overlaps between lines and characters. Table 2.3 shows the experimental results of the seven low-resolution map compared to the set of 80 high-resolution map. The low-resolution map ( scales lower than 7 meters per pixel) had a signiﬁcantly lower recall number. This was because the characters and symbols in the low-resolution map overlap the lines more frequently, as shown in  2.34. Since the text/graphics separation algorithm is used to remove characters and labels, the algorithm also removed many of the road lines in the low-resolution map. Moreover, the size of street blocks in the low-resolution map were sometimes smaller than the rectangle size used to determine the connectivity, which could lead to inaccurate identiﬁcation of the road orientations. 
map  Precision  Recall  
Scales higher than 7 meters per pixel (80 map)  95%  75%  
Scales lower than 7 meters per pixel (7 map)  83%  27%  

Table 2.3: Experimental results with respect to the map scale 


 2.34: An example low-resolution raster map (TIGER/Line, 8 meters per pixel) 

In comparison to the road-intersection-extraction results from closely related work [Habib et al., 1999; Henderson et al., 2009], Habib et al. [1999] work on map that contain only road lines to extract the road intersections and their paper does not o.er numeric results. Henderson et al. [2009] work on the road layer extracted  user speciﬁed colors from USGS topographic map. The approach of Henderson et al. [2009] achieved 93% recall and 66% precision in an experiment  the USGS topographic map. In comparison, my approach achieved 82% recall and 60% precision on USGS topographic map. My recall number was lower because I did not employ any prior knowledge to separate the road layer from the USGS topographic map and hence some of the road pixel might be missing during the automatic road-layer separation. In addition, the morphological operators could generate distorted road geometry during the process of merging parallel road lines. Although my experiments on the USGS topographic map had a lower recall number, my approach is not limited to a speciﬁc type of map. 
2.3.4 Computation Time 

I built the experiment platform  Microsoft Visual Studio 2003 running on a Microsoft Windows 2003 Server powered by a 1.8 GHz Intel Xeon 4 Dual Processors CPU with one GB RAM. The USGS topographic map were the most informative raster map ( more foreground pixel) in my experiments, which took less than one minute to extract the road intersections from an 800x600 pixel topographic map with around 120k foreground pixel. Other map sources required fewer than 20 seconds for an image size less than 500x400 pixel. The LTM algorithm took about 0.25 seconds to match an intersection point. The dominant factors for the computation time were the image size and the number of foreground pixel in the raster map.  2.35 shows the computation time with respect to the number of foreground pixel in a raster map. The implementation was not fully optimized and improvements could still be made to speed up the processes, such as multi-threading for processing individual road intersections. 

2.4 Conclusion 

This chapter presented the AutoMapDecomposer and AutoIntDetector techniques, which automatically separate the road and text layers from raster map and detect the road intersections from the separated road layer. My experiments on 87 map from 12 sources show accurate results for extracting the positions and geometry of the road intersections. We utilize the road intersection results from a set of raster map from various sources and successfully identify the geospatial coordinates of the map and align the map to the orthoimagery [Chen et al., 2006a]. 
Chapter 3 

Automatic Extraction of Road Vector Data 
In Chapter 2, I described automatic techniques to separate the road and text layers from raster map (AutoMapDecomposer) and extract road intersections from the sepa.rated road layers (AutoIntDetector). The AutoMapDecomposer employs an automatic grayscale-histogram-analysis algorithm to extract the foreground pixel from raster map, which is based on the hypothesis that the background colors of raster map have a dom.inant number of pixel and the number of foreground pixel is signiﬁcantly smaller than the number of background pixel. However, for raster map with poor image quality, the map usually contain numerous colors due to the noise introduced in producing the map in raster format (e.g., scanning). Therefore, the automatic grayscale-histogram-analysis algorithm does not work well on raster map with poor image quality; even manual la.beling requires signiﬁcant e.ort to identify the colors that represent the road pixel. 
In addition to the di.culty in processing raster map with poor image quality, the AutoIntDetector utilizes the binary dilation, binary erosion, and thinning operators for generating the road geometry, which can cause distorted distorted lines. In particular, the thinning operator does not require any parameter tuning; however, the thinning operator can produce distorted lines around intersections and hence the extracted road geometry is not accurate without manual adjustment [Bin and Cheong, 1998]. As shown in  3.1, if we apply the thinning operator directly on the thick lines shown in  3.1(a), the lines that are near intersections are seriously distorted as shown in  3.1(b). To reduce the extent of the line distortion, the AutoIntDetector ﬁrst erodes the lines  the erosion operator [Pratt, 2001] with a dynamically generated number of iterations and then applies the thinning operator. s 3.1(c) and 3.1(d) show that the extent of the line distortion is smaller after the AutoIntDetector applies the erosion operator; however, the erosion operator did not completely eliminate the distortion. 

(a) 
Thickened road lines (b)  only the thinning operator 

(c) 
Eroded road lines 	(d)  the erosion and thinning opera.tors 



 3.1: Distorted road lines near road intersections caused by the thinning operator 
This chapter presents a general approach for extracting road vector data from raster map. I ﬁrst present a supervised technique called the Road Layer Separator (RoadLay.erSeparator) that requires only minimal user input for separating road layers from the map with poor image quality. Then, I describe an automatic technique called the Au.tomatic Road Vectorizer (AutoRoadVectorizer) for extracting accurate road vector data from the road layers. Together with the automatic techniques described in Chapter 2, the AutoMapDecomposer and AutoIntDetector, I can process raster map with varying map complexity (e.g., overlapping features) and image quality to separate their road layers with minimal user input and automatically detect the road intersections and vectorize the road geometry from the separated road layers. 
 3.2 shows the overall approach of the RoadLayerSeparator and the AutoRoad-Vectorizer. RoadLayerSeparator identiﬁes the road colors to extract road pixel from raster map by analyzing user labels automatically. The user label does not have to contain only road pixel, which makes the user-labeling task easier and practical. With the separated road layer, the AutoRoadVectorizer utilizes an enhanced Parallel-Pattern-Tracing algorithm to detect the road width and format, which can process large road layers more e.ciently since scanned map are usually large images (a typical 350 dot.per-inch (DPI) scanned map can be larger than 6000x6000 pixel). Then, the AutoRoad-Vectorizer automatically generates the road geometry  the morphological operators as described in Chapter 2. Finally, the AutoRoadVectorizer automatically corrects the distorted lines around road intersections in the road geometry to produce accurate road vector data. 

3.1 Supervised Separation of Road Layers 
The RoadLayerSeparator has three major steps. The ﬁrst step is to quantize the color space of the input image. Then, the user labels road areas of every road color in the quantized image. Since the quantized image has a limited number of colors, the Road-LayerSeparator can reduce the manual e.ort in this user-labeling step. Finally, the Road-LayerSeparator automatically identiﬁes a set of road colors from the user labels and gen.erates a color ﬁlter to extract the road pixel from the raster map. I describe the details of each step and the labeling criteria in the following subsections. 
3.1.1 Color Quantization 

Distinct colors commonly represent di.erent layers ( a set of pixel representing a particular geographic feature) in a raster map, such as roads, contour lines, and text labels. By identifying the colors that represent roads in a raster map, we can extract the road pixel from the map. However, raster map usually contain numerous colors due to the scanning and/or compression processes and the poor condition of the original documents (e.g., color variation from aging, shadows from folding lines). For example,  3.3(a) shows a 200x200 pixel tile cropped from a scanned map. The tile has 20,822 distinct colors, which makes it di.cult to manually select the road colors. To overcome this di.culty, the RoadLayerSeparator applies color quantization algorithms to group the colors of individual feature layers into clusters based on the assumption that the color variation within a feature layer is smaller than the variation between feature layers. 
Therefore, after applying the color quantization algorithms, the RoadLayerSeparator can extract individual feature layers by selecting speciﬁc color clusters. 
The RoadLayerSeparator ﬁrst applies the Mean-shift algorithm [Comaniciu and Meer, 2002], which helps to preserve the edges of map features (e.g., road lines) while reducing noise. The Mean-shift algorithm works in a multi-dimensional space, which is a com.bination of the spatial space ( the image coordinates, X and Y) and the HSL color space (hue, saturation, and luminance). The RoadLayerSeparator uses the HSL color space because the HSL space is a uniform color space. For a pixel in the raster map, P (x, y), the corresponding node in the ﬁve-dimensional space ( X and Y from the image coordinates plus H, S, and L from the color space) is N(x, y, h, s, l), where h, s, and l represent the color of P . 
To reduce the noise in the raster map, for a pixel, P (x, y), the Mean-shift algorithm starts from computing the mean node, M(xm,ym,hm,sm,lm), from N(x, y, h, s, l)’s neigh.boring nodes. The mean node’s position consists of the mean values on each of the axes X, Y, H, S, and L of N’s neighboring nodes within a local area (the RoadLayerSeparator uses a spatial distance of 3 pixel and a color distance of 25 to deﬁne the local area). If the distance between M(xm,ym,hm,sm,lm) and N(x, y, h, s, l) is larger than a small threshold (the RoadLayerSeparator uses the small threshold to limit the running time for the Mean-shift algorithm to converge), the Mean-shift algorithm shifts N(x, y, h, s, l) to M(xm,ym,hm,sm,lm) and recalculates the mean node within the new local area. After the Mean-shift algorithm converges (the distance between the mean node and N is no longer larger than the threshold), the H, S, and L values of N is used as P (x, y)’s color. In 

(a) An example tile (b) The Mean-shift result 

(c) The K-means result, K=8 (d) The K-means result, K=16 
 3.3: An example map tile and the color quantization results with their red, green, and blue (RGB) color cubes 
the example shown in  3.3, the Mean-shift algorithm reduces the number of colors in  3.3(a) by 72% as shown in  3.3(b). 
To further merge similar colors in the raster map, the RoadLayerSeparator applies the K-means algorithm [Forsyth and Ponce, 2002] to generate a quantized image with at most K colors. The K-means algorithm can signiﬁcantly reduce the number of colors in a raster map by maximizing the inter-cluster color variance; however, since the K-means algorithm considers only the color space, it is very likely that the resulting map has merged features with a small K. For example,  3.3(c) shows the quantized map with K as eight and the text labels have the same color as the road edges. Therefore, the user would need to select a larger K to separate di.erent features, such as in the quantized map in  3.3(d) with K as 16. 
For large map with thousands of colors, the results after the Mean-shift algorithm can still have numerous numbers of colors and hence the K-means algorithm will require signiﬁcant processing time to classify the colors. In this case, the RoadLayerSeparator utilizes a common color quantization method called the Median-cut [Heckbert, 1982] after the Mean-Shift algorithm to generate an image with at most 1,024 colors as the input to the K-means algorithm, which helps to reduce the processing time of the K-means algorithm. The Median-cut algorithm cannot replace the K-means algorithm because the design of the Median-cut algorithm is to be able to keep image details in the quantized image, such as the image texture, and the goal of my color quantization is to have a single color representing a single feature in the map ( eliminating the image texture). 
3.1.2 User Labeling 

In this user-labeling step, the RoadLayerSeparator ﬁrst generates a set of quantized map in di.erent quantization levels  various K in the K-means algorithm. Then, the user selects a quantized map that contains road lines in di.erent colors from other features and provides a user label for each road color in the quantized map. A user label is a rectangle that should be large enough to cover a road intersection or a road segment. To label the road colors, the user ﬁrst selects the size of the label. Then, the user clicks on the approximate center of a road line or a road intersection to indicate the center of the label. The user label should be (approximately) centered at a road intersection or at the center of a road line, which is the constraint the RoadLayerSeparator exploits to identify the road colors in the next step. For example,  3.4(a) shows an example map and  3.4(b) shows the quantized map and the labeling result to extract the road pixel. The two user labels cover one road intersection and one road segment, which contain the two road colors in the quantized map ( yellow and white). 
3.1.3 Automatically Identifying Road Colors  Example Labels 
Each user label contains a set of colors, and some of the colors represent roads in the raster map. The RoadLayerSeparator exploits two geometric properties of the road lines in a user label to identify the road colors of a given user label, namely the centerline property and the neighboring property. 
The centerline property is: because the user labels are centered at a road line or a road intersection, the pixel of a road color will be a portion of one or more linear objects that are near the image center. For example, the RoadLayerSeparator decomposes a user label 

(a) An example scanned map 

(b) The quantized map and user labels 
shown in the top-right of  3.4(b) into a set of six images shown in  3.5(a) (background is shown in black) so that every decomposed image each contains only one color from the user label. The pixel in the decomposed images, Image 3, 4, and 5, constitute the road lines in the user label, and their pixel constitute a portion of several linear objects that pass through or near the image centers. For example, in Image 3 and 4, the pixel are a portion of four linear objects that pass near the image center as the four lines A, B, C, and D shown in  3.5(b). 

(a) The decomposed images 

(b) Four linear objects 
 3.5: Identifying road colors  the centerline property (background is shown in black) 
The neighboring property is: the pixel of road colors should be spatially near each other. For example, the majority of pixel in Image 3 can ﬁnd an immediate neighboring pixel in Image 4 and 5 and vice versa, but the majority of pixel in Image 0 cannot ﬁnd an immediate neighbor in Image 3, 4, and 5. 
To exploit the centerline property, for each decomposed image, the RoadLayerSepa.rator extracts the skeletons of every connected object in the image  the thinning operator and applies the Hough transformation [Forsyth and Ponce, 2002] to identify a set of Hough lines from the skeletons. The Hough transformation is a technique to iden.tify lines ( the Hough lines) that are constituted from imperfect objects. In our case, the imperfect objects are the non-background pixel in the decomposed images. Since the image center of a user label is the center of a road line or a road intersection, if the detected Hough lines are near the image center, the lines are most likely to be part of the road lines. Hence, the RoadLayerSeparator detects the Hough lines in each decom.posed image and computes the average distance between the detected Hough lines to the image center as a measure to determine if the foreground pixel (non-black pixel) in a decomposed image represent roads in the raster map. 

 3.6 shows the detected Hough lines of each decomposed image, where the Hough lines that are within a distance threshold to the image centers are drawn in red and others are drawn in blue (the colors are only used to help explain the idea). In  3.6, the decomposed images that contain road pixel (Image 3, 4, and 5 ) have more red lines than blue lines and hence the average distances between their Hough lines to their image centers are smaller than the other decomposed images. Therefore, the decomposed image that has the smallest average distance is classiﬁed as the road-pixel image ( the color of the foreground pixel in the decomposed image represents roads in the raster map). The other decomposed images with average distances between one pixel to the smallest average distance are also classiﬁed as road-pixel images. This criterion allows the user label to be a few pixel o. (depending on the size of the user label) from the actual center of the road line or road intersection in the map, which makes the user labeling easier. In our example, Image 5 has the smallest average distance, so the RoadLayerSeparator ﬁrst classiﬁes Image 5 as a road-pixel image. Then, since Image 4 is the only image with its average distance within a one-pixel distance to the smallest average distance, the RoadLayerSeparator also classiﬁes Image 4 as a road-pixel image. 

 3.6: The identiﬁed Hough lines (background is shown in black) 
The road-pixel image classiﬁcation method based on the detected Hough lines relies on the numbers of detected Hough lines. So if a color that is used on roads has a smaller number of pixel compared to the major road colors, such as the Image 3 shown in  3.5(a), the image will not be classiﬁed as a road-pixel image  the Hough-line method. Therefore, I present the Edge-Matching algorithm for the RoadLayerSeparator to exploit the neighboring property to determine if any of the decomposed images that are not classiﬁed as road-pixel images  the Hough-line method ( Image 0 to 3 ) is a road-pixel image. 
The Edge-Matching algorithm utilizes a road template generated  the already classiﬁed road-pixel images and compares the unclassiﬁed images with the road template to identify road-pixel images. In our example, the road template is the combination of Image 4 and 5 as shown in  3.7 (background is shown in black). Next, the Edge-Matching algorithm uses the road template to evaluate Image 0 to 3 in turn. For a color pixel, C(x, y), in a given decomposed image to be evaluated, the Edge-Matching algorithm searches a 3x3 pixel neighborhood centered at (x, y) in the image of the road template to detect if there exists any road pixel. If one or more neighboring road pixel exist, the Edge-Matching algorithm marks the pixel C(x, y) as a road pixel since it is spatially near one or more road pixel. After the Edge-Matching algorithm examines every foreground pixel in a given decomposed image, if more than 50% of the foreground pixel in that image are marked as road pixel, then the decomposed image is identiﬁed as a road-pixel image. 

 3.7: A road template example (background is shown in black) 
 3.8 shows an example of the Edge-Matching algorithm. The ﬁrst row shows the foreground pixel of the Image 0 to 3 (background is shown in black) and the second row is the match with the road template. The bottom row shows the results after the Edge-Matching algorithm processes each of the images, where the non-black pixel are the matched pixel. Only Image 3 has more than 50% of its foreground pixel identiﬁed as matched pixel, so that Image 3 is a road-pixel image and others are discarded. 
The RoadLayerSeparator processes every user label and identiﬁes a set of road-pixel images from each label. Then, the algorithm uses the foreground colors in the road-pixel images ( the identiﬁed road colors) to generate a color ﬁlter for scanning the quantized map to extract the road pixel. The RoadLayerSeparator ﬁlls up the pixel in the quantized map where colors in the color ﬁlter are shown with black pixel ( the road pixel) and the other pixel with white pixel.  3.9(b) shows the extracted road pixel ( the separated road layer) of  3.9(a). 

3.2 Automatic Extraction of Road Vector Data 
In this section, I present the AutoRoadVectorizer, which automatically generates the road geometry from the separated road layer and then vectorizes the road geometry. Since scanned map are usually large images, the AutoRoadVectorizer includes an en.hanced Parallel-Pattern-Tracing algorithm for detecting the road width and road format of large map more e.ciently. Then, the AutoRoadVectorizer exploits the techniques from Chapter 2 to identify the road centerlines from the extracted road pixel as the road geometry automatically. The AutoRoadVectorizer corrects the distortions near road intersections by extracting accurate road-intersection templates and then generates ac.curate road vector data. Finally, instead of processing the entire map at once, I present 

(a) An example scanned map 

(b) The separated road layer (background is shown in white) 
 3.9: An example results of the RoadLayerSeparator 
the divide-and-conquer approach of the AutoRoadVectorizer that divides the map into overlapping tiles and extracts and merges the road vector data from each tile. 
3.2.1 Single-Pass Parallel-Pattern Tracing (SPPT) 
In Chapter 2, I described the Parallel-Pattern-Tracing algorithm (PPT) for identifying the road format and road width. The PPT checks each foreground pixel to determine if there exists any corresponding pixel in the horizontal and vertical directions at a certain road width. If the PPT ﬁnds a corresponding pixel in each direction, the algorithm classiﬁes the pixel as a parallel-pattern pixel of the given road width. By applying the PPT iteratively from one-pixel wide road width to K-pixel wide, we can identify the road width and road format of the majority of the roads in the map by analyzing the number of parallel-pattern pixel at each of the tested road widths. 
The implementation of the PPT contains two convolution masks. One convolution mask works in the horizontal direction and the other one works in the vertical direction to ﬁnd the corresponding pixel. The sizes of the convolution masks are based on the road width. For example, if the road width is one-pixel wide, the size of the convolutions mask is 3x3 pixel. If the road width is two-pixel wide, the size of the convolution masks is 5x5 pixel. Therefore, for N foreground pixel, to iteratively apply the PPT on the road width from one-pixel wide to K-pixel wide, the number of steps is: 
K
∑ 

PPT (N, K)= N . (2r + 1)2 (3.1) r=1 
The time complexity is O(NK3), which requires signiﬁcant computing time when we have a large map (a large N) and/or we run the PPT with more iterations (a large K). 
To improve the time complexity, I present the Single-Pass Parallel-Pattern-Tracing algorithm (SPPT), which only requires a single-pass scan on the image.  3.10 shows the pseudo-code of the SPPT. The SPPT starts from the upper-left pixel in the image and scans the image one row at a time from left to right. 

 3.10: The pseudo-code of the Single-Pass Parallel-Pattern Tracing (SPPT) algo.rithm 
To determine the number of parallel-pattern pixel in the image at the road width from one-pixel wide to K-pixel wide, the SPPT scans every foreground pixel’s neighboring pixel in the horizontal and vertical directions. For the horizontal direction, the SPPT starts from the neighboring pixel to the left of a foreground pixel and scans K pixel. For the vertical direction, the SPPT starts from the neighboring pixel below a foreground pixel and scans K pixel. For a foreground pixel, P , if the SPPT detects a corresponding pixel, C, at P ’s Cth neighbor to the left or towards the bottom, the SPPT records that P and C both have at least one corresponding neighbor in the horizontal/vertical direction at the road width equal to C, which eliminates searching in the pixel’s right/top direction. Therefore, for N foreground pixel, to iteratively apply the SPPT on the road width from one-pixel wide to K-pixel wide, the number of steps is: 
SPPT (N, K)=2 . N . K (3.2) 
The time complexity is O(NK), which is signiﬁcantly less than  the convolution masks and enables e.cient processing of large map. 
3.2.2 Automatically Generating Road Geometry 
With the detected road width and road format, the AutoRoadVectorizer exploits the techniques from Chapter 2 to automatically identify the road centerlines by ﬁrst applying the dilation operator  the iteration as half of the detected road width to generate thickened road lines. If the road format is double-line format, the iteration of the dilation operator is half of the detected road width plus three for merging the parallel lines into one line. The AutoRoadVectorizer applies more iterations of the dilation operator for the roads in double-line format because the algorithm needs to expand the road areas farther for ﬁlling up areas within road intersections of the parallel lines. Three more iterations ﬁll areas smaller than 6x6 pixel, which is generally large enough to cover the holes in the intersection areas of parallel lines in all orientations after applying the dilation operator  half of the detected road width as the number of iterations. Then, the AutoRoadVectorizer applies the erosion operator with the number of iterations as the detected road width to thin the road lines for minimizing the distortion after applying the thinning operator next. Finally, the AutoRoadVectorizer uses the thinning operator to generate the road centerlines ( the thinned road lines) as the road geometry. 
3.2.3 Automatically Extracting Accurate Road-Intersection Templates 
A road-intersection template is the position of a road intersection, the orientations of the roads intersecting at an intersection, and the connectivity of a road intersection.  3.11 shows the overall approach to automatically extract accurate road-intersection templates, where the gray boxes indicate the results from  the SPPT algorithm and the Automatic Intersection Detector (AutoIntDetector) technique from Chapter 2. 
3.2.3.1 Labeling Potential Distortion Areas 
Since the thinning operator produces distorted road geometry near the road intersections and the road width determines the extent of the distortion, the AutoRoadVectorizer can utilize the extracted road intersections and the road width to label the locations of the potential distorted lines. The AutoRoadVectorizer ﬁrst generates a blob image 

with the detected road-intersection points labeled as single foreground pixel. Then, the AutoRoadVectorizer applies the dilation operator to grow a blob for each of the road-intersection points  the road width as the number of iterations. For example,  3.12(a) shows the blob image after the AutoRoadVectorizer applies the dilation operator where the size of each blob is large enough to cover the road area of each road intersection in the original map. Finally, the AutoRoadVectorizer overlaps the blob image with the thinned-line image shown in  3.12(c) to label the extent of the potential distorted lines as show in  3.12(d). 
3.2.3.2 Identifying and Tracing Road-Line Candidates 
To extract accurate road vector data around the intersections, the AutoRoadVectorizer uses the labeled image shown in  3.12(d) to detect possible road lines intersecting at each road intersection ( road-line candidates) and traces the thinned-line pixel to compute the line orientations. The AutoRoadVectorizer ﬁrst identiﬁes the contact points between each blob and the thinned-lines by detecting the thinned-line pixel that have any neighboring pixel labeled by the gray boxes. These contact points indicate the starting points of a road-line candidate associated with the blobs. In the example shown in  3.12(d), the road intersection in the second top-left blob has three road-line candidates starting from the contact points that are on the top, right, and bottom of the blob. 
For each road-intersection blob, the AutoRoadVectorizer utilizes the Limited Flood-Fill algorithm to trace the thinned-lines from their contact points.  3.13 shows the pseudo-code for the Limited Flood-Fill algorithm. The Limited Flood-Fill algorithm 

(a) An example raster map (b) Road-intersection blobs 

(c) Thinned road lines (d) Labeling distortion areas 
ﬁrst labels a contact point as visited and then checks the eight neighboring pixel of the contact point to ﬁnd unvisited thinned-line pixel. If one of the eight neighboring pixel is not labeled as visited nor labeled as a potential distorted line pixel, the neighboring pixel is set as the next visit point for the Limited Flood-Fill algorithm to walk through. 

 3.13: The pseudo-code for the Limited Flood-Fill algorithm 
When the Limited Flood-Fill algorithm walks through a new pixel, it records the position of the pixel for later computing the road orientation. The AutoRoadVectorizer limits the number of pixel that the Limited Flood-Fill algorithm can trace from each contact point  a parameter called MaxLinePixel. The Limited Flood-Fill algorithm counts the number of pixel that it has visited and stops when the counter is larger than the MaxLinePixel variable (the AutoRoadVectorizer uses ﬁve pixel in my method). As shown in  3.14, instead of tracing the whole curve starting from the two contact points ( the one on the right and the one on the bottom), the AutoRoadVectorizer utilizes the MaxLinePixel to ensure that the Limited Flood-Fill algorithm traces only a small portion of the thinned-lines near the contact points assuming that roads near the contact points are straight within a small distance ( several meters within a street block in the raster map). 

After the Limited Flood-Fill algorithm walks through the lines from each contact point and records the positions of the line pixel, the AutoRoadVectorizer utilizes the Least-Squares Fitting algorithm to ﬁnd the linear functions of the lines. Assuming a linear function L for a set of line pixel traced by the Limited Flood-Fill algorithm, by minimizing the sum of the squares of the vertical o.sets between the line pixel and the line L, the Least-Squares Fitting algorithm ﬁnds the straight line L that most represents the traced line pixel. The algorithm works as follows:1 The target line function for L: 
Y = m . X + b (3.3) 
where 

∑ ∑∑ 
n xy . xy 
m = ∑∑ (3.4) 
nx2 . ( x)2 
and 

∑∑ 
y . mx 
b = (3.5) 
n 

1The proof of the Least-Squares Fitting algorithm can be found in most statistics textbooks or on the web at http://mathworld.wolfram.com/LeastSquaresFitting.html. 
The AutoRoadVectorizer then uses the computed line functions in the next step of up.dating road-intersection templates to identify actual intersecting road lines and reﬁne the positions of the road intersections. 
3.2.3.3 Updating Road-Intersection Templates 
There are three possible intersecting cases for the road-line candidates of one intersection as shown in  3.15, where the left images for the three cases are the original map, the middle images are the thinned lines with the locations of the potential distorted lines labeled by the blob images, and the right images are the traced line functions ( the line functions computed  the Least-Squares Fitting algorithm) drawn on a Cartesian coordinate plane. 
The top row of  3.15 shows the ﬁrst case where all the road-line candidates inter.sect at one point. The middle row of  3.15 shows the second case where the road-line candidates intersect at multiple points and the intersecting points are within a distance threshold to the initially detected road-intersection position. The bottom row of Fig.ure 3.15 shows the third case where the road-line candidates intersect at multiple points and some of the intersecting points are not near the initially detected road-intersection position. 
For the ﬁrst case, the AutoRoadVectorizer adjusts the position of the road intersection to the intersecting point of the road-line candidates. The AutoRoadVectorizer keeps all road-intersection candidates as the intersecting roads of this road-intersection template. The road orientations of this road template are 0 degrees, 90 degrees, and 270 degrees, respectively. 

 3.15: The three intersecting cases 

For the second case,  3.16(a) shows a detail view where the solid red dot is the initially detected road-intersection position, the green, blue, red, and orange lines are the road-line candidates, the solid black dots are the candidates’ intersecting points, and the semi-transparent red circle implies a local area with radius as the detected road width. Since the extent of the distortion depends on the road width, the positional o.set between any intersecting point of the road-line candidates and the initially detected road-intersection position should not be larger than the road width. Therefore, for case two, since every intersecting point of the road-line candidates is in the semi-transparent red cir.cle, the AutoRoadVectorizer adjusts the position of the road-intersection template to the centroid of the intersecting points of all road-line candidates. The AutoRoadVectorizer keeps all road-intersection candidates as the intersecting roads of this road-intersection template. The road orientations of this road template are 80 degrees, 172 degrees, 265 degrees, and 355 degrees, respectively. 

(a) Without outliers (b) Two outliers 
 3.16: Adjusting the road-intersection position  the centroid point of the intersections of the road-line candidates 
For the third case, when two road intersections are very close to each other, the road geometry between them is totally distorted as shown in  3.17. In this case, the blobs of the two road intersections merge into one big blob as shown in  3.17(d), and the AutoRoadVectorizer associates both road intersections with the four thinned-lines linked to this blob.  3.16(b) shows a detail view of the third case of the merged blobs. Since the two intersecting points where the dashed road-line candidate intersects with two other road-intersection candidates are more than a road width away from the initially detected road-intersection position, the AutoRoadVectorizer discards the dashed road-line candidate. The AutoRoadVectorizer uses the centroid of the re.maining two intersecting points as the position of the road-intersection template. Since the AutoRoadVectorizer discards the dashed road-line candidate, the connectivity of this road-intersection template is three and the road orientations are 60 degrees, 150 degrees, and 240 degrees, respectively. This third case shows how the blob image helps to extract correct road orientations even when an intersecting road line is totally distorted by the thinning operator. This case holds when the distorted road line is short and hence likely to have the same orientation as the other intersecting lines. For example, in the third case in  3.15, the distorted line is part of a straight line that goes through the intersection, so it has the same orientation as the 240-degree line. 
 3.18 shows example results of the accurately extracted road-intersection tem.plates and the results of  the thinning operator only. By utilizing the knowledge of the road width and road format, the AutoRoadVectorizer automatically detects and corrects the distorted lines around road intersections caused by the thinning operator and generates accurate road-intersection templates. 

(a) Original roads (b) Thickened lines 

(c) Distorted lines (d) Merged blobs 
 3.17: Merged nearby blobs 

(a)  the thinning operator only (b)  the AutoRoadVectorizer 
 3.18: Example results of  the thinning operator only and the AutoRoadVec.torizer 
3.2.4 	Automatically Vectorizing Road Geometry  Accurate Road-Intersection Templates 
With the accurate positions of the road intersections and the knowledge of potential distorted areas as shown in  3.19(a) to  3.19(b), the AutoRoadVectorizer starts to trace the road pixel in the thinned-line image to generate the road vector data. The thinned-line image contains three types of pixel: the non-distorted road pixel, distorted road pixel, and background pixel.  3.19(a) shows the three types of pixel, which are the black pixel not covered by the gray boxes, black pixel in the gray boxes, and white pixel, respectively. The AutoRoadVectorizer creates a list of connecting nodes (CNs) of the road vector data. A CN is a point where two lines meet at di.erent angles. The AutoRoadVectorizer ﬁrst adds the detected road intersections into the CN list. Then, The AutoRoadVectorizer identiﬁes the CNs among the non-distorted road pixel  a 3x3-pixel-window to check if the pixel has any of the straight-line patterns shown in  3.19(c). The AutoRoadVectorizer adds the pixel to the CN list if the algorithm does not detect a straight-line pattern since the road pixel is not on a straight line. 
To determine the connectivity between the CNs, the AutoRoadVectorizer traces the road pixel  an eight-connectivity Flood-Fill algorithm shown in  3.20. The Flood-Fill algorithm starts from a CN, travels through the road pixel (both non-distorted and distorted ones), and stops at another CN. Finally, for the CNs that are road inter.sections, the AutoRoadVectorizer uses the previously updated road intersection positions as the CNs’ positions as the main function, shown in  3.20. The CN list and their 

(a) 
Marking distortions and tracing (b) Accurate road-intersection tem.roads plates 

(c) 
Straight-line patterns (d) Extracted road vector data 



 3.19: Extracting road vector data from an example map 
connectivity are the results of the extracted road vector data.  3.19(d) shows the extracted road vector data. The road vector data around the road intersections are ac.curate since the AutoRoadVectorizer does not generate any CN  the distorted lines except the road intersections ( does not record the geometry of the distorted lines) and the intersection positions are updated  the accurate road orientations. 

 3.20: The pseudo-code for tracing line pixel 
3.2.5 Divide-and-Conquer Extraction of Road Vector Data 
The AutoRoadVectorizer utilizes a divide-and-conquer technique (DAC) that divides a raster map into overlapping tiles and processes each tile for extracting its road vector data.  3.21 shows an input scanned map and the DAC divides the desired map region into four overlapping tiles. After the DAC processes all the tiles, the algorithm combines the extracted road vector data from each tile as one set of road vector data for 
the entire raster map. 



For the extracted road vector data of each tile, the DAC cuts the vector data at the center of the overlapping areas as the dashed lines shown in  3.22 and discards the vector data located in the areas between the dashed line and the tile borders. This is because the extracted road vector data near the image borders are usually inaccurate from  the image processing operators (e.g., the morphological operators). 
The DAC merges the road vector data from two neighboring tiles by matching the intersections of the dashed lines and the road lines ( the cutting points) of the two neighboring sets of road vector data. For example,  3.22(a) shows two sets road vector data from two neighboring tiles. The DAC ﬁrst generates a set of cutting points for the left set of road vector data  the intersections of the vertical dashed line and the road lines of the left tile’s road vector data. Then, the DAC generates the set of cuttings points for the right set of road vector data. Finally, the DAC merges the two sets of road vector data by connecting two road lines in the two tiles ending at the cutting points of 

(a) Cutting the overlapping area (gray) 

(b) 
Connecting the cutting points 

(c) 
The merged road vector data 



 3.22: Merging two sets of road vector data from neighboring tiles 
the same location as shown in  3.22(b) where each of the horizontal arrows point to a matched pair of cutting points shown as the cross marks.  3.22(c) shows the merged road vector data. 
The DAC ﬁrst merges the road vector data of tiles on the same row from left to right and then it merges the integrated vector data of each row into the ﬁnal results from top to bottom. The divide-and-conquer approach to process raster map scales to large images and can take the advantage of multi-core or multi-processor computers to process the tiles in parallel. 
3.3 Experiments 

I have implemented the supervised road-layer-separation and automatic road-vectorization approaches (the RoadLayerSeparator and RoadVectorizor) described in this chapter as the road vectorization component in a map processing system called Strabo. In this sec.tion, I report my experiments on the extraction of road vector data from heterogeneous raster map  Strabo. I tested Strabo on 16 map from 11 sources. Table 3.1 shows the information of the test map and their abbreviations that I use in this section. The set of test map includes four scanned map and 12 computer-generated map. 
For the scanned map, we purchased three paper map covering the city of Baghdad, Iraq from the International Travel map, Gecko map, and Gizi Map. I scanned the Bagdad paper map in 350 DPI to produce the raster map. I downloaded another scanned map covering the city of Samawah, Iraq from the United Nations Assistance Mission for Iraq website.2 The United Nations website does not include the map metadata other than the map scale, which is 15,000:1. 
Map Source (abbr.)  Map  Map Type  Image Dimension  
Count  (pixel)  

International Travel map (ITM)  1  Scanned  4000x3636  
Gecko map (GECKO)  1  Scanned  5264x1923  
Gizi Map (GIZI)  1  Scanned  3344x3608  
UN Iraq (UNIraq)  1  Scanned  4000x3636  
Rand McNally (RM)  1  Computer Generated  2084x2756  
UN Afghanistan (UNAfg)  1  Computer Generated  3300x2550  
Google map (Google)  2  Computer Generated  800x550  
Live map (Live)  2  Computer Generated  800x550  
OpenStreetMap (OSM)  2  Computer Generated  800x550  
MapQuest map (MapQuest)  2  Computer Generated  800x550  
Yahoo map (Yahoo)  2  Computer Generated  800x550  
Table 3.1: Test map 


For the computer generated map, we purchased a map in PDF format covering the city of St. Louis, Missouri from the Rand McNally website.3 I downloaded a map covering Afghanistan in PDF format from the United Nations Assistance Mission in Afghanistan website.4 The Afghanistan map shows the main and secondary roads, cities, political boundaries, airports, and railroads of the nation. I cropped ten computer-generated map from ﬁve web-mapping-service providers, Google map, Microsoft Live map, OpenStreetMap, MapQuest map, and Yahoo map. The ten computer generated map cover two areas in the U.S. One area is in Los Angeles, California, and the other one is in St. Louis, Missouri.  3.23 shows examples of the test map, where the 
2http://www.uniraq.org 
3http://www.randmcnally.com/ 
4http://unama.unmissions.org/ 

95 scanned map show poor image quality, especially the ones from Gecko map and Gizi map with the shadows caused by the fold lines. 

3.3.1 Experimental Setup 

I utilized Strabo to automatically extract road vector data from the 16 test map. Strabo successfully generated accurate road vector data for 10 map among the 16 test map automatically, which are the map from the last ﬁve sources shown in Table 3.1. The fully automatic road-vectorization function in Strabo did not work well for six of the 16 test map, which include four scanned map since the automatic function cannot separate the foreground pixel from the background. The other two map contain non-roads linear features, which are drawn  the same single-line format as the roads, and hence the automatic function could not separate the road lines from the other map features. Therefore, to achieve the best results, I utilized the supervised road-pixel.extraction function in Strabo to extract the road pixel and then Strabo automatically generated the road geometry and extracted the road vector data. 
For comparison, I tested the 16 test map  the automatic road-vectorization function in R2V from Able Software. R2V allows the user to use one set of color thresholds to extract the road pixel from the map for the automatic road-vectorization function. For raster map that require more than one set of color thresholds (all of my test map except the UNAfg map require more than one set of color thresholds), the user has to manually specify sample pixel for each of the road color and the sample pixel, which requires signiﬁcant user e.ort in R2V. Therefore, for the raster map that require more than one set of color thresholds to extract their road pixel, I utilized Strabo to ﬁrst extract the 

(a) ITM map (b) GECKO map 

(c) 
GIZI map (d) UNIraq map 

(e) 
RM map (f) UNAfg map 



 3.23: Examples of the test map 
road pixel and then utilized the “Auto Vectorize” function in R2V to vectorize the roads without manual pre-processing or post-processing. 
3.3.2 Evaluation Methodology 

To evaluate my supervised road-pixel-extraction approach, I report the number of user labels that were required for extracting road pixel from a test map  Strabo. For the six map that I tested for the supervised approach (ITM, GECKO, GIZI, UNIraq, RM, and UNAfg), four map are scanned map, which contain numerous colors (ITM, GECKO, GIZI, and UNIraq). Strabo automatically quantized the four scanned map  the Mean-shift, Median-cut, and K-means algorithms to generate four quantized images in di.erent quantization levels for each map (the four quantization levels have the number of colors as 32, 64, 128, and 256, respectively). Strabo did not apply the color segmentation algorithms on the two computer generated map (RM and UNAfg) before user labeling. This is because the computer-generated map contain a smaller number of colors. The UNAfg map has 90 unique colors and there is only one color representing both the major and secondary roads in the map. The RM map has 20 unique colors, with ﬁve colors representing roads. The user started  Strabo for the user-labeling task from the quantized image of the highest quantization level ( the quantized image that has the smallest number of colors). If the user could not distinguish the road pixel from other map features (e.g., background) in the quantized image, the user would need to select a quantized image containing more colors (a lower quantization level) for user labeling. 
I evaluated the quality of the extracted road-intersection templates  10 raster map of the last ﬁve sources shown in Table 3.1. I report the quality of the extraction re.sults  the positional o.set, orientation o.set, and connectivity o.set. The positional o.set is the average number of pixel between the extracted road intersection templates and the actual road intersections on the raster map. The actual road intersections in the raster map are deﬁned as the centroid points of the intersection areas of two or more intersecting road lines. The orientation o.set is the average number in degrees between the extract road orientations and the actual road orientations. The connectivity o.set is the total number of missed road lines. I manually examine each road intersection in the raster map to obtain the ground truth of the positions of the road intersections, the connectivity, and the road orientations. 
For evaluating the extracted road vector data, I report the accuracy of the extraction results  the road extraction metrics proposed by Heipke et al. [1997], which include the completeness, correctness, quality, redundancy, and the root-mean-square (RMS) di.erence. I manually drew the centerline of every road line in the map as the ground truth. The completeness and correctness represent how complete/correct the extracted road vector data are. The completeness is the length of true positives divided by the summation of the lengths of true positives and false negatives, and the optimum is 100%. The correctness is the length of true positives divided by the summation of the lengths of true positives and false positives, and the optimum is 100%. The quality is a combination metric of the completeness and correctness, which is the length of true positives divided by the sum of the lengths of true positives, false positives, and false negatives, and the optimum is 100%. The redundancy shows the percentage of the matched ground truth that is redundant ( more than one true positive line matched to one ground truth line), and the optimum is 0. The RMS di.erence is the average distance between the extracted lines and the ground truth, which represents the geometrical accuracy of the extracted road vector data. To generate these metrics, Heipke et al. [1997] suggest  a bu.er width as half of the road width in the test data so that a correctly extracted line is inbetween the road edges. In my test map, the roads range from ﬁve to twelve pixel wide. I used a bu.er width of three pixel, which means a correctly extracted line is no farther than three pixel to the road centerlines. 
3.3.3 Experimental Results 

Table 3.2 shows the number of colors in the images used for user labeling and the number of user labels used for extracting the road pixel. For all the scanned map, only one to nine labels were needed for Strabo to extract the road pixel. 
Map Source  Original Map Colors  Quantized Map Colors  User Labels  
ITM  779,338  64  9  
GECKO  441,767  128  5  
GIZI  599,470  64  9  
UNIraq  217,790  64  5  
RM  20  N/A  5  
UMAF  90  N/A  1  

Table 3.2: The number of colors in the image for user labeling of each test map and the number of user labels for extracting the road pixel 
From the 10 raster map examined for the accuracy of the road-intersection templates, Strabo correctly extracted all the 139 road intersections in the 10 raster map and ex.tracted 438 of the 451 road lines intersecting at the road intersections. Table 3.3 shows the results. The average positional o.set was 0.4 pixel and the average orientation o.set was 0.24 degrees, which shows that the extracted road-intersection templates are very close to the ground truth in these test map. In order to achieve higher accuracy in the positional and orientation o.sets, Strabo needed to discard the lines that are outliers in the step to compute the position of the road-intersection template; Strabo missed 13 lines from a total of 451 lines in the process of tracing the lines to extract the road intersection templates. These 13 lines all belong to the road intersections that are near the boundaries of the map and hence Strabo could not ﬁnd road lines long enough to compute the cor.rect orientations. I also compare the extracted templates  Strabo with the extracted template  the one-pixel road lines directly generated by the thinning operator. The comparison results of the positional o.set and orientation o.set are shown in  3.24. My approach in this chapter had a signiﬁcant improvement on the results of every map source for both of the positional and orientation o.sets. 
Map Source  Total Ints.  Extracted Lines  Missed Lines  Avg. Position O.set (pixel)  Avg. Orientation O.set (degrees)  
Google  28  87  3  0.12  0  
Live  28  91  2  0.52  0.37  
OSM  31  95  3  0.37  0.29  
MapQuest  25  83  0  0.69  0.55  
Yahoo  27  82  5  0.35  0.24  

Table 3.3: The average positional o.set, average orientation o.set, and connectivity o.set (the number of missed lines for each map source) 
Table 3.4 shows the numeric results from  Strabo and R2V to extract road vector data from the 16 test map. The average completeness, correctness, quality, redundancy, and RMS di.erences of Strabo were 96.53%, 97.61%, 94.41%, 0.19%, and 2.79 pixel compared to R2V’s 94.9%, 87.4%, 79.73%, 42.81%, and 16.12 pixel. Strabo produced more accurate road vector data with small RMS di.erences. I emphasize the numbers 

(a) Positional o.set comparison (in pixel) 

(b) Orientation o.set comparison (in degrees) 
where R2V generated a better result than Strabo. R2V produced better completeness numbers for ﬁve test map, which is because R2V generated highly redundant lines, while Strabo eliminated small branches, such as the highway ramps shown in  3.25. R2V could achieve better results if I tuned R2V with manually speciﬁed pre-processing and post-processing functions (e.g., manually specify the gap size for reconnecting two lines). 

(a) 
ITM map (portion) (b) Extracted road pixel 

(c) 
Strabo results (d) R2V results 



 3.25: Example results  Strabo and R2V of a cropped area from the ITM map 
 3.26 to  3.30 show some example results where the geometry of the extracted road vector data are very close to the road centerlines for both straight and curved roads, especially the computer generated map from the web-mapping-service providers. The ITM, GECKO, GIZI, UNIraq, and RM map had lower than average 
Map Source  Comp.  Corr.  Quality  Redundancy  RMS  
ITM (Strabo)  90.02%  93.95%  85.08%  0.85%  3.59  
ITM (R2V)  96.00%  66.91%  65.09%  117.33%  13.68  
GECKO (Strabo)  93.75%  94.75%  89.12%  0.61%  2.95  
GECKO (R2V)  96.52%  76.44%  74.39%  52.64%  8.27  
GIZI (Strabo)  92.90%  96.18%  89.59%  0.00%  2.46  
GIZI (R2V)  93.43%  95.03%  89.08%  39.42%  11.17  
UNIraq (Strabo)  88.31%  96.01%  85.19%  0.00%  6.94  
UNIraq (R2V)  94.92%  78.38%  75.22%  18.82%  5.19  
RM (Strabo)  96.03%  84.72%  81.85%  1.60%  2.79  
RM (R2V)  92.74%  68.89%  65.36%  33.56%  16.03  
UNAfg (Strabo)  86.02%  99.92%  85.96%  0.00%  3.68  
UNAfg (R2V)  88.26%  99.92%  88.20%  12.36%  3.98  
Google 1 (Strabo)  99.64%  99.74%  99.38%  0.00%  0.85  
Google 1 (R2V)  85.67%  80.16%  70.68%  17.80%  19.78  
Google 2 (Strabo)  99.60%  100.00%  99.60%  0.00%  0.77  
Google 2 (R2V)  81.22%  83.70%  70.13%  19.77%  18.54  
LM 1 (Strabo)  99.41%  96.61%  96.06%  0.00%  15.14  
LM 1 (R2V)  79.58%  66.67%  56.93%  25.38%  29.39  
LM 2 (Strabo)  99.52%  100.00%  99.52%  0.00%  1.02  
LM 2 (R2V)  87.26%  76.05%  68.45%  33.12%  18.31  
OSM 1 (Strabo)  100.00%  100.00%  100.00%  0.00%  0.82  
OSM 1 (R2V)  85.26%  87.63%  76.11%  7.33%  12.23  
OSM 2 (Strabo)  99.61%  100.00%  99.61%  0.00%  0.71  
OSM 2 (R2V)  95.67%  99.78%  95.47%  6.30%  9.46  
MapQuest 1 (Strabo)  100.00%  100.00%  100.00%  0.00%  0.73  
MapQuest 1 (R2V)  84.58%  86.81%  74.95%  6.80%  12.27  
MapQuest 2 (Strabo)  99.69%  100.00%  99.69%  0.00%  0.76  
MapQuest 2 (R2V)  99.43%  100.00%  99.43%  8.25%  1.17  
Yahoo 1 (Strabo)  100.00%  99.94%  99.94%  0.00%  0.69  
Yahoo 1 (R2V)  85.71%  77.85%  68.91%  79.55%  25.48  
Yahoo 2 (Strabo)  99.94%  100.00%  99.94%  0.00%  0.80  
Yahoo 2 (R2V)  86.49%  76.49%  68.33%  127.03%  27.51  
Avg. (Strabo)  96.53%  97.61%  94.41%  0.19%  2.79  
Avg. (R2V)  94.90%  87.41%  79.73%  42.81%  16.12  
Table 3.4: Numeric results of the extracted road vector data (three-pixel-wide bu.er)  Strabo and R2V 


correctness numbers since some of the non-road features were also extracted  the identiﬁed road colors and those parts contributed to false positive road vector data.  3.26(a) to  3.26(c) show a portion of the ITM map where the runways are represented  the same color as the white roads and hence were extracted as road pixel.  3.28(a) to  3.28(c) show a portion of the UNIraq map where part of the building pixel were extracted since they share the same colors of the road shadows.  3.29(a) shows two grid lines in pink on the left and right portions of the RM map were also extracted since they have the same color as the major road shown at the center of the map. In addition,  3.29(b) shows the extracted road pixel  the supervised function of Strabo and many characters were extracted since they share the same color as the black lines. Although I removed the majority of the characters automatically, some of the characters were miss-identiﬁed as road lines since they touch the road lines and the connected-component analysis approach I used could not remove this type of false positive. Including a user validation step after the road pixel were extracted could further reduce this type of false positives resulting in lower correctness numbers. 
For the lower than the average completeness numbers in the ITM, GECKO, GIZI, and UNAfg map, some broken lines were not reconnected since the gaps were larger than the iterations of the dilation operator after the non-road overlapping features were removed, such as the gaps in the GECKO and GIZI map shown in  3.26 and  3.27. The broken lines could be reconnected with post-processing on the road vector data since the gaps are now smaller than they were in the extracted road layers resulting from the dilation operator. For the lower than the average completeness numbers in the scanned 

(a) ITM map (portion) (b) Road pixel of (a) (c) Road vector data of (a) 

(d) 
GECKO map (portion) (e) Road pixel of (d) (f) Road vector data of (d) 

(g) 
GECKO map (portion) (h) Road pixel of (g) (i) Road vector data of (g) 



 3.26: Examples of the road vectorization results of the ITM and GECKO map 

(a) GIZI map (portion) (b) Road pixel of (a) (c) Road vector data of (a) 

(d) 
GIZI map (portion) (e) Road pixel of (d) (f) Road vector data of (d) 

(g) 
UNAfg map (portion) (h) Road pixel of (g) (i) Road vector data of (g) 



 3.27: Examples of the road vectorization results of the GIZI and UNAfg map 

(a) UNIraq map (portion) 

(b) Road pixel of (a) 

(c) Road vector data of (a) 
 3.28: Examples of the road vectorization results of the UNIraq map 

(a) RM map (portion) 

(b) Road pixel of (a) 

(c) Road vector data of (a) 
 3.29: Examples of the road vectorization results of the RM map 

(a) MapQuest map (b) Road pixel of (a) 

(c) 
Road vector data of (a) (d) OSM map 

(e) 
Road pixel of (d) (f) Road vector data of (d) 



 3.30: Examples of the road vectorization results from map of the MapQuest and OSM map 
UNIraq map, some of the road lines as shown in  3.28(a) are dashed lines and the ground truth were drawn as solid lines. 
Strabo’s redundancy numbers are generally low since I correctly identify the center.lines for extracting the road vector data. The average RMS di.erences are under three pixel, which shows that the thinning operator and my approach to correct the distortion result in good quality road geometry. For example, in the results shown in  3.30(a) to  3.30(f), although the extracted road lines are thick, Strabo extracted accurate road vector data around the intersections. The high redundancy numbers of R2V resulted from no manual pre-processing before R2V’s automatic function to extract the centerlines of the roads, and the automatic function is sensitive to wide road lines. 
3.3.4 Computation Time 

I built Strabo  Microsoft Visual Studio 2008 running on a Microsoft Windows 2003 Server powered by a 3.2 GHz Intel Pentium 4 CPU with 4GB RAM. The average process.ing time for the entire process on vectorizing the road pixel for an 800x550-pixel map was 5 seconds, for a 2084x2756-pixel map was 2 minutes, and for a 4000x3636-pixel map was 3.5 minutes. The dominant factors of the computation time are the image size, the number of road pixel in the raster map, and the number of road intersections in the road layer. The implementation was not fully optimized and improvements could still be made to speed the processes, such as multi-threading on processing map tiles of an input map. 
3.4 Conclusion 

This chapter presents the RoadLayerSeparator and AutoRoadVectorizer techniques. The RoadLayerSeparator separates road layers from raster map with poor image quality or complex map, and the AutoRoadVectorizer extracts accurate road vector data from the separated road layer. My experiments show that together with the AutoMapDecomposer and AutoIntDetector described in Chapter 2, the techniques separated road layers from raster map of 11 sources with varying color usages and image quality  minimal user input and extracted accurate road vector data from the separated road layers. 
Chapter 4 

Automatic Recognition of Text Labels 
Text labels in raster map provide valuable geospatial information by associating geo.graphical names with geospatial locations. By converting the text labels in a raster map to machine-editable text, we can produce geospatial knowledge, such as generating a gazetteer for the map coverage area  the recognized text, for understanding the map region when other geospatial data are not ready available. Moreover, we can register a raster map to other geospatial data (e.g., imagery) [Chen et al., 2008; Wu et al., 2007] and exploit the recognized text from the map for indexing and retrieval of the other geospatial data. 
The technique of converting the printed text in document images to machine-editable text is called optical character recognition (OCR), which is an active area in both academia research [Mori et al., 1995] and software development [Nagy et al., 2000], such as the com.mercial products like the ABBYY FineReader,1 NUANCE OminiPage,2 and open source 
1http://www.abbyy.com/ 
2http://www.nuance.com/ 


software, such as the OCRopus,3 Tesseract OCR,4 etc. In general, the ﬁrst step of an OCR system is “zoning,” which analyzes the layout of an input document (e.g., single column and tables) for locating and ordering the text regions ( zones), each containing text lines of the same orientation (usually horizontal or vertical text) [Kanai et al., 1995; Mao et al., 2003]. Then, the OCR system processes each identiﬁed zones for recognizing the text. 
Current OCR techniques produce a high recognition rate on documents with simple layout ( documents that can be successfully separated into zones). From 1992 to 1996, the Information Science Research Institute (ISRI) of the University of Nevada, Las Vegas conducted ﬁve annual tests of OCR accuracy  various state-of-art systems with a set of test images from government reports, legal document samples, magazines, and newspapers. In ISRI’s last publication of the annual test of OCR accuracy [Rice et al., 1996], the test data sets contained more than 2,000 pages with nearly ﬁve million characters in English, Spanish, and German and the text strings in the test images were mostly of the same orientation. The character accuracy of an OCR system in ISRI’s annual tests is deﬁned as follows: N is the number of correctly recognized characters and L is the Levenshtein distance [Levenshtein, 1966] between the recognition results and the ground truth, the character accuracy is given by: 
N . L 

CharacterAccuracy = (4.1)
N 

3http://code.google.com/p/ocropus/ 
4http://sourceforge.net/projects/tesseract-ocr/ 

Rice et al. [1996] report that ﬁve of the six tested OCR system achieved over 93% character accuracy on all types of test documents. For the legal document samples in English, which had a simpler layout compared to the magazine pages, the six OCR systems achieved even higher character accuracy numbers from 97.82% to 99.66%. ISRI’s experiments also showed that increasing the scanning resolution from 200 to 300 dot-per-inch (DPI) had a substantial improvement on the recognition results. 
Although present OCR systems can achieve a high recognition rate on documents with simple layout, text recognition from raster map is still a challenging task. First, the image quality of the raster map usually su.ers from the scanning and/or image compression processes. Second, the text labels in a raster map can have various font types and sizes and very often overlap with each other or with other features in the map, such as roads. Third, the text labels within a map do not follow a ﬁxed orientation and do not have a speciﬁc reading order. Therefore, the document structure analysis techniques for zoning generally do not work on map [Mao et al., 2003]. 
This chapter presents a general approach for recognizing text labels in raster map. . I ﬁrst present my supervised technique called the Text Layer Separator (TextLayerSepa.rator) for separating text layers from complex raster map or the map with poor image quality, which requires only minimal user input. Then, I describe my automatic technique called the Automatic Text Recognizer (AutoTextRecognizer) that focuses on locating in.dividual text labels in the map and detecting their orientations to then leverage the horizontal text recognition capability of commercial OCR software. Together with the automatic map decomposition technique (AutoMapDecomposer) described in Chapter 2, I can process raster map with varying map complexity and image quality for separating their text layers with minimal user input and automatically recognize the text labels in the separated road layers. 
 4.1 shows the overall approach of the TextLayerSeparator and AutoTextRec.ognizer. To overcome the di.culties of processing raster map with poor image quality and overlapping features, the TextLayerSeparator utilizes a supervised technique that analyzes user labels for extracting the pixel that represent text ( the text layer) from raster map. With the separated text layer, the AutoTextRecognizer employs three dis.tinct steps for recognizing the text labels. The ﬁrst step in the algorithm automatically groups characters for identifying text strings by exploiting the fact that the characters in a text string have a similar size and are spatially near each other. For the identiﬁed text strings, the second step is to automatically detect their orientations and rotate the strings to the horizontal. Finally, the last step utilizes a conventional OCR product to recognize the characters in the horizontal strings and then generates the recognized text. I describe the details of the TextLayerSeparator and AutoTextRecognizer techniques in the following sections. 
4.1 Supervised Extraction of Text Layers 
There are three major steps in the TextLayerSeparator for the supervised extraction of text layers from raster map. The ﬁrst step is to quantize the color space of the input image. Then, the user provides examples of text areas of the quantized image. The user can also provide labels of non-text areas to help the TextLayerSeparator select text colors. Finally, the last step is to automatically identify a set of text colors from the user 

examples and generate a color ﬁlter to extract the text layers from the raster map. The user interaction is designed to be consistent with the supervised road layer extraction technique (RoadLayerSeparator) described in Chapter 3. 
4.1.1 Color Quantization 

Raster map usually contain numerous colors due to the scanning or compression pro.cesses. To extract the text pixel, the TextLayerSeparator applies color segmentation techniques to reduce the number of colors in the map for generating a color palette with a limited number of colors. Similar to the the RoadLayerSeparator in Chapter 3, the TextLayerSeparator ﬁrst utilizes the Mean-shift ﬁltering algorithm [Comaniciu and Meer, 2002] to smooth the image and reduce noise. Next, the TextLayerSeparator utilizes the Median-cut [Heckbert, 1982] and K-means to generate a set of quantized images for the user to label text areas.  4.2 shows an example scanned map, the quantized im.age after the Mean-shift ﬁltering, Median-cut, and K-means algorithms. The Mean-shift ﬁltering algorithm reduces the number of unique colors in the raster map by 37% and then the Median-cut algorithm further produces a quantized map with 883 unique colors as shown in  4.2(d) and  4.2(f). Finally, the K-means algorithm generates quantized map in di.erent quantization levels  various numbers of K.  4.2(h) shows a quantized map with K equal to 16. 
4.1.2 User Labeling 

In this user-labeling step, the user selects a quantized map that contains text strings in colors distinct from other features from a set of quantized map in di.erent quantization 

(a) An example map area (80,421 unique colors) (b) RGB color cube of (a) 

(c) 
After the Mean-shift ﬁltering (51,600 unique colors) (d) RGB color cube of (c) 

(e) 
After the Median-cut algorithm (883 unique colors) (f) RGB color cube of (e) 

(g) 
After the K-means algorithm with K=16 (16 unique colors) (h) RGB color cube of (g) 




 4.2: An example map tile and the color quantization results with their red, green, and blue (RGB) color cubes 
levels. Then, the user provides an example label for each text color in the quantized map ( a text label). The text label is a rectangle that should be large enough to cover a string of at lease two characters as the examples shown in  4.3. The user can also provide non-text labels to help the TextLayerSeparator identify text colors. A non-text label is a rectangle that covers only non-text colors in the raster map. To generate the text and non-text labels, the user ﬁrst clicks on the map and then selects the size of the label and rotates the selection area to label non-horizontal text if needed.  4.4(a) shows the user interface and an example text label.  4.4(b) shows that the text label is automatically rotated to the horizontal direction if the user selects a non-horizontal text string. 

(a) Purple characters (b) Red characters (c) Black characters (d) Blue characters 
 4.3: Example text labels for characters of various colors 
4.1.3 Automatically Identifying Text Colors  Example Labels 
Each text label contains a set of colors, and some of the colors represent text in the raster map. The TextLayerSeparator exploits the fact that the character pixel in a text label are spatially near each other and constitute a horizontal string, namely the horizontal-string property, to identify the text colors in a given text label. The TextLayerSeparator ﬁrst decomposes a text label into a set of images so that every decomposed image contains only one color from the text label. For example, for the text label shown in  4.3(a), the ﬁrst row of  4.5 shows the four decomposed images (background is shown in 

(a) The user interface for user labeling 

(b) The selected text label 
black). The TextLayerSeparator discards the decomposed images where the number of foreground pixel is fewer than 15% of the total number of pixel since most of their foreground pixel do not have neighboring pixel in the horizontal direction. 

 4.5: The decomposed images and the RLSA results 
To identify the decomposed image that contains the text pixel, the TextLayerSep.arator ﬁrst expands the foreground areas in each decomposed image to grow a string blob. Since the height of a text label, H, is larger than the height of any character in the text label and the character height is usually longer than the character width, the horizontal pixel distance between two character pixel in a decomposed image should be less than H. Therefore, the TextLayerSeparator uses the closing operator with structure elements of height equal to one pixel and width equal to H to expand the foreground area for connecting character pixel and to grow a string blob. The closing operator is the di.lation operator followed by the erosion operator. For a background pixel in a decomposed image, if there exists a foreground pixel in the horizontal direction within the distance of H, the dilation operator converts the background pixel to the foreground. Then, for each foreground pixel (including the ones converted by the dilation operator), if there exists a background pixel in the horizontal direction within the distance of H, the erosion operator converts the foreground pixel to the background. The second row and third row in  4.5 shows the results after applying the dilation and erosion operators on the decomposed images shown in the ﬁrst row (background is shown in white). Finally, the TextLayerSeparator applies the erosion operator with a structure element of height equal to one pixel and width equal to H again to further eliminate false-positive branches of the string blob. The fourth row in  4.5 shows the results after applying the erosion operator (background is shown in white). 
The usage of the closing operator followed by the erosion operator is the morpholog.ical operator implementation of the run-length smoothing algorithm (RLSA), which is commonly used in document analysis techniques to identify string blocks from character pixel [Najman, 2004; Wong and Wahl, 1982]. Since the character pixel are horizontally near each other in the text label, the TextLayerSeparator uses the number of the re.maining foreground pixel after RLSA as a measure to determine if the foreground pixel (non-black pixel) in a decomposed image represent text in the raster map. For example, Image 1 in  4.5 has the most remaining foreground pixel after the RLSA and hence the color of the foreground pixel in Image 1 is identiﬁed as the text color. 
There are two exceptions to  the horizontal-string property to identify text colors in the text label. One exception is that when background with uniform color exists in the text label, the color of the foreground pixel in the decomposed image that represents the background can be mis-identiﬁed as the text color. The ﬁrst row of  4.6 shows the decomposed images of the text label shown in  4.3(c) and the second row shows the foreground pixel after the RLSA. The text label in  4.3(c) has a uniform background, which is the foreground pixel of Image 0. The foreground pixel of Image 0 are horizontally near each other. In this case, the user would need to provide a non-text label containing the color in Image 0. If a non-text label exists, the RLSA algorithm processes only the decomposed images that do not contain their foreground colors that are in a non-text label. For example, with a non-text label having the color in Image 0, my algorithm discards Image 0 and selects the color of the foreground pixel in Image 1 as the text color since Image 1 now has the most remaining foreground pixel. 

 4.6: An example of a text label with uniform background 
A second exception is when the characters in the text label are represented by multiple colors and the number of pixel of every color is less than 15% ( non-solid characters). The top-left corner of  4.7 shows a text label from Google map and the 28 rect.angles are the decomposed images from the text label (background is shown in black). Only the decomposed image on the top-right corner that represents the background of the text label has more than 15% of foreground pixel. In this case, if no text color is identiﬁed after excluding the colors in the non-text label, all the colors in the text label except the ones in the non-text label are identiﬁed as the text colors.  4.8 shows the identiﬁed text colors (background is shown in white) when the non-text label exists. 


(a) The non-text label (b) The identiﬁed text colors 
 4.8:  non-text label to identify text colors from raster map with non-solid characters 
The TextLayerSeparator processes every text and non-text label and identiﬁes a set of text colors from each text label. Then, for each pixel in the quantized map, if the pixel’s color is not one of the text colors, the TextLayerSeparator converts the pixel to the background. For example, to extract the text layer of red characters from the map in  4.2, the user selects the text label in  4.9(a) and then the TextLayerSeparator generates the text layer in  4.9(e) automatically (background is shown in white). Similarly, to extract the text layer of black pixel, the user selects the text label in  4.9(b) and then the TextLayerSeparator generates the text layer in  4.9(f) automatically. For the text layer of blue pixel, the user ﬁrst provides the text label in  4.9(c) but the uniform background is mis-identiﬁed as the text color. Therefore, the user provides the non-text label in  4.9(d) together with the text label in  4.9(c) to extract the text layer of blue characters shown in  4.9(g). 

(a) 
The text label for red (b) The text label for black (c) The text label for blue (d) The non-characters characters characters text label 

(e) 
The text layer of red characters (f) The text layer of black characters 

(g) 
The text layer of blue characters (h) The text layer of red characters after post-processing 

(i)
Thetextlayerofblackcharactersafterpost-processing







 4.9: Example results of the supervised text-layer extraction 

There are still non-text objects in the extracted text layer if the non-text objects have the same color as the text, such as the scattered small objects in the three text layers or the lines in  4.9(g). Therefore, the TextLayerSeparator applies post-processing  the connected-component analysis to ﬁlter out the non-text connected components based on the size of a connected component. For a connected component, A, and its bounding box, Abx, the size of A is given by: 
Size = Max(Abx.Height, Abx.W idth) (4.2) 
For a text layer, the text labels used to generate the text layer contain a set of hori.zontal text strings in the text layer, namely the example strings. The TextLayerSeparator computes the average size of the connected component in the example strings and then ﬁlters out the connected component in the text layer, which is smaller than half or larger than twice of the average size. This ﬁltering rule keeps most of the connected compo.nent in the text layer and ﬁlters out only extreme cases, such as a long linear objects or single-pixel connected component.  4.9(h) to  4.9(j) shows the results after post-processing. The characters that overlap a large connected component, such as the ‘S’ shown in  4.9(g), could also be removed. In this case, text separation algorithms [Cao and Tan, 2002] can be used to keep the overlapping characters. 
4.2 Automatic Recognition of Text Labels 
In this section, I present my AutoTextRecognizer technique that recognizes text labels in the separated text layers automatically. The ﬁrst step of the AutoTextRecognizer is my conditional dilation algorithm for locating individual strings in the extracted text layers, which contain multi-oriented text strings of various sized characters. The conditional dilation algorithm exploits the heuristic that the characters in a text string should have similar size and be spatially closer than the characters in two separate strings to locate the individual text strings. Similar to the AutoRoadVectorizer described in Chapter 3, instead of processing the entire map at once, the AutoTextRecognizer divides the map into overlapping tiles, utilizes the conditional dilation algorithm to locate individual text strings in each tile, and merges the identiﬁed text strings. Finally, with the identiﬁed individual strings, the AutoTextRecognizer detects their orientations and utilizes a con.ventional OCR product to recognize the characters. 
4.2.1 Conditional Dilation Algorithm (CDA) 
The conditional dilation algorithm (CDA) is an image ﬁlter that expands the foreground area of the connected components in an input image when certain conditions are met. The purpose of the expansion is to determine the connectivity between the connected components in the input image.  4.10 shows the pseudo-code for the CDA. An iteration of the CDA includes two passes on the input image. Given a set of conditions, the ﬁrst pass identiﬁes the pixel that meet the conditions as the expansion candidates. Then, the second pass, which is a veriﬁcation pass, conﬁrms that the existence of an expansion candidate does not generate a connection between two connected components that violate the conditions  the candidate’s neighboring foreground pixel and ex.pansion candidates. The CDA runs from one iteration until the ﬁrst pass cannot identify 
any more expansion candidates. 



4.2.1.1 Input 

The input of the CDA is a binary text layer, which contains characters as individual connected components ( character components). From an extracted text layer, I generate a binary text layer automatically based on the character types ( solid or non-solid characters) of the text labels used to extract the text layer. If the text labels contain only solid characters, I use every foreground pixel in the text layer as the foreground pixel of the binary text layer and everything else is the background. 
For the text labels containing non-solid characters, the AutoTextRecognizer uses the foreground pixel in the text layer that are closer to the foreground (black) than the background (white) as the foreground pixel in the binary text layer. This is because the text layers of non-solid characters are pixilated and very often contain foreground pixel that represent the shadow around characters to emphasize the visualization of the characters. If we use all foreground pixel in the text layer containing non-solid characters as foreground pixel in the binary text layer, the shadow pixel can connect two characters as a connected component and hence we do not have characters as individual connected components.  4.11(a) shows a map tile from Google map and  4.11(b) shows the extracted text layer containing non-solid characters.  4.11(c) shows the binary text layer if every foreground pixel in  4.11(b) is a foreground pixel in the binary text layer, where many of the characters are connected, especially the text strings of “Paloma St” and “E 35th ST”.  4.11(d) shows the binary text layer after we remove the shadow pixel and most of the characters are individual connected components. 

(a) An example map tile 	(b) The extracted text layer 

(c) The binary text layer with character shadow 	(d) The binary text layer without character shadow 
 4.11: Generating a binary text layer from a text layer with non-solid characters 
4.2.1.2 The First Pass 

In the ﬁrst pass, the CDA checks every background pixel in the input image. If a back.ground pixel meets all of the following conditions, the CDA sets the background pixel as an expansion candidate. These conditions do not have to be checked in a ﬁxed order. 
First, the connectivity condition. An expansion candidate needs to connect at least one and at most two connected components. Since the maximum neighboring characters that any character in a text string can have is two, an expansion candidate cannot connect to more than two connected components. 
Second, the size condition. If an expansion candidate connects to two connected components, the sizes of the two connected characters must be similar. The size of a con.nected component is deﬁned in Equation 4.2. For any connected character components, their size ratio must be smaller than two. For two connected components, A and B, their bounding boxes are Abx and Bbx, their size ratio is given by: 
Max(Size(A), Size(B))
SizeRatio = (4.3)
Min(Size(A), Size(B)) 
The size limitation guarantees that every character in an identiﬁed text string has a similar size. The limitation of size ratio equal to two is because some letters, such as the letter ‘l’ and ‘e’, do not necessarily have the exact same size, even when the same font size and type is used. 
Third, the expansibility condition. An expansion candidate needs to connect to at least one expandable character component. Before the ﬁrst CDA iteration, every character is expandable. After each iteration, the CDA checks the connectivity of each character and if the character has already connected to two other characters, the character is not expandable. Then, for the remaining character components with connectivity numbers less than two, the CDA compares the number of iterations that have been done and the size of the character component to determine the expandability of the character components. This is to control the longest distance between two characters that the CDA can connect. Since one iteration expands the area of a character by one pixel, the CDA connects two characters that are within two pixel after one iteration if the two character components satisfy other conditions. The CDA uses a dynamic distance threshold as 1/5 of the character size. For example, for a character of size equal to 20 pixel, it will not be expandable after four iterations, which means the character can only ﬁnd a connecting neighbor within the distance of four pixel plus 1/5 of the size of a neighboring character component. 
 the iteration of the CDA to limit the distance between two characters in a string is more reliable than  the distance between the bounding-box centers of the character components. This is because distance between the bounding-box centers is based on the locations of the bounding-box centers, which can vary depending on the sizes and orientations of the character components, and the shortest distance between two characters does not have this dependency. For example, as shown in  4.12, the lengths of the green lines that represent shortest distances between two neighboring characters in a text string are similar, and do not depend on the characters’ sizes and orientations. 

 4.12: The shortest distance between two character components does not depend on the characters’ sizes and orientations 
Fourth, the straight-string condition. If an expansion candidate connects to two char.acter components and at least one of the two character components has a connected neigh.bor ( a string with at least three characters), the connected character-components should constitute a straight string. This limitation guarantees that the CDA does not connect the text strings of di.erent orientations. However, to determine a straight string  only the character components without knowing how the characters are aligned is di.cult because the connecting angle between three characters in a straight string is not necessarily 180 degrees. Considering the text string “Wellington”, we can calculate the connecting angles between any three neighboring connected characters in the string by connecting the mass centers or bounding-box centers of the character components. Since the characters have various heights, the connecting angle between “Wel” is very di.erent than the one between “ell”. Therefore,  the connecting angles to determine a straight string is unreliable. 
Instead of  only the connecting angles, the CDA ﬁrst establishes a connecting-angle baseline for a text string, which represents the connecting angles between any three neighboring connected characters as if the string is in the horizontal direction. Then, the CDA compares the connecting-angle baseline to the actual connecting angles in the string to determine a straight string. To calculate the baseline, The CDA ﬁrst aligns each of the characters in the string vertically and rearranges their positions in the horizontal direction. Then, the CDA calculates the connecting angles between the characters  the connecting lines between the bounding-box centers of the characters. This connecting angle is called the normalized connecting angle and the connecting-angle baseline is a set of the normalized connecting angles. 
 4.13 shows two examples. The green dashed line in  4.13(a) shows the lines connecting the bounding-boxes centers for calculating the actual connecting angles. The blue dashed line in  4.13(b) shows the lines connecting the rearranged bounding-boxes centers for calculating the connecting-angles baseline.  4.13(c) 
shows that the .1 is similar to .1 ’ and .2 is similar to .2 ’ and hence the string “dale” is a straight string.  4.13(d) to  4.13(f) shows an example of a non-straight string where .1 and .1’ are very di.erent. 

(a) 
The actual connecting angle of “dale” (b) The connecting-angle baseline of “dale” 

(c)
Theactualconnectingangleissimilartothebaseline

(d)Theactualconnectingangleof“AvRi”


(e) 
The connecting-angle baseline of “AvRi” 	(f) The actual connecting angle is very di.erent than the baseline 






 4.13: Comparing the angle baseline with the connecting angle to test the straight string condition 
With the connecting-angle baseline, the CDA calculates the actual connecting angle between the characters  their bounding boxes and if the di.erences between the actual connecting angle and the normalized connecting angle are smaller than 30%, the string satisﬁes the straight-string condition. I use the 30% threshold to prevent breaking text strings that are slightly curved. 
4.2.1.3 The Veriﬁcation Pass 

The veriﬁcation pass checks each expansion candidate  the same conditions in the ﬁrst pass. When the CDA marks an expansion candidate in the ﬁrst pass, the CDA checks only the foreground pixel connecting to the expansion candidate. If the CDA further marks one of the expansion candidate’s connecting background pixel as an expansion candidate after the ﬁrst pass, we need to verify if the connectivity between the two expansion candidates is valid. For example,  4.14(a) shows two characters that should not be connected because of the size limitation.  4.14(b) shows the results after the ﬁrst pass, where the green pixel are the identiﬁed expansion candidates. After the ﬁrst pass, the CDA marks all background pixel that directly connect to the two characters as expansion candidates. If the distance between the two characters is two pixel, such as the areas in the red rectangle shown in  4.14(a), after the ﬁrst pass, the CDA ﬁlls up the two-pixel area with expansion candidates and the two characters are then connected. Therefore, we need the veriﬁcation pass to verify the expansion candidates.  4.14(c) shows the results after the veriﬁcation pass. 
4.2.1.4 Output 

The output of the CDA is a binary image and the individual connected components in the output image represent a text string.  4.15(a) shows an example text layer 

(a) An example map area 	(b) After the ﬁrst pass (expansion (c) After the veriﬁcation pass candidates shown in green) 
 4.14:  the veriﬁcation pass to determine actual expansion pixel (background is shown in white) 
and  4.15(a) shows the CDA results.  4.15(c) shows the string blobs; each is shown in a di.erent color. Once the CDA has identiﬁed the string blobs, it overlaps the text layer with the string blobs to group the characters into text strings as shown in  4.15(d). 
The CDA does not group the small dots in the text layer correctly to string blobs, such as the dot on top of the character ‘i’ as shown in  4.15. This is because the sizes of the dots are too small compared to their neighboring character components ( violating the size condition). The OCR system will recover the missing small parts of characters in the character recognition step, which is simpler than adopting special rules for connecting the small parts. This is because grouping a small part to a correct string blob requires additional rules on handling the size di.erences between the connected components in the binary text layer, which can lead to unreliable results. For example, if the grouping between the dot of “St.” and the character ‘t’ is allowed, it is very likely that the character ‘d’ in “Hillsdale” will connect to the character ‘A’ in “Av” before ‘d’ can connect to ‘s’ and ‘a’ in “Hillsdale”. 
4.2.2 Divide-and-Conquer Identiﬁcation of Text Labels 
The AutoTextRecognizer utilizes a divide-and-conquer (DAC) technique that divides a raster map into overlapping tiles and processes each tile to recognize their text labels.  4.16(a) shows an example text layer with two overlapping tiles. After the DAC processes all the tiles, it merges the recognized text from each tile as one set of text strings for the entire raster map. Before the processing of each tile, the DAC ﬁrst removes the connected components that touch each tile’s borders since the touching connected 

(a) An example text layer (b) The CDA results 

(c) The string blobs in color (d) Associating characters with string blobs 
 4.15: The CDA output 
components might only be a portion of a character. The overlapping area should be larger than any of the characters in the text layer so that the characters near the tile borders always exist in one of the tiles. For example,  4.16(b) and  4.16(c) shows the text identiﬁcation results of the two overlapping tiles  the conditional dilation algorithm. In the left tile, the character ‘a’ of the text string “Hillsdale” in the text layer is on the tile border and hence the DAC removes the ‘a’ before applying the conditional dilation algorithm. For the identiﬁed strings in two neighboring tiles, the DAC merges the stings that contain one or more characters that are the same. For example,  4.16(b) and  4.16(c) show that the conditional dilation algorithm identiﬁes the text string, “Hillsd”, from the left tile and the text string, “sdale”, from the right tile. Since the two characters “sd” in the original text layer exist in both text strings, the DAC merges the two strings into one as “Hillsdale”. 

(a) An example text layer (b) The left tile (c) The right tile 
 4.16: The divide-and-conquer processing 
As I described in Chapter 3, this divide-and-conquer approach to process raster map scales to large images and can take advantage of multi-core or multi-processor computers to process the tiles in parallel. 
4.2.3 Automatically Detecting String Orientation 
Skew correction is well developed in modern OCR techniques; however, classic skew correction can only apply to multi-line and multi-word documents since the line spacing and the word spacing is exploited to detect the tilt angle, such as the morphological-operator-based RLSA method [Najman, 2004]. 
The morphological-operator-based RLSA method ﬁrst uses the closing operator to generate a set of string blobs and then uses the erosion operator to reduce the areas of the string blobs. The closing operator is used to merge characters in a string so the structure-element size of the closing operator should be larger than the width of a character. The erosion operator is used to eliminate the string blobs that are shorter horizontally than the width of the erosion operator’s structure elements. For example, in a document of 500 words with 50% of the words longer than 300 pixel, if we apply RLSA with the erosion operator  a structure element of width 300 pixel, we have 250 words remaining after the RLSA if the document is in the horizontal direction. If the document is tilted by 45 degrees, the lengths of the words in the horizontal direction is the original length multiplied by cos(45.) and hence fewer than 250 words will remain after the RLSA. Therefore, by detecting the number or the areas of the remaining words, we can determine the tilt angle in a multi-word document. 
For the AutoTextRecognizer to detect the orientation of a single string, I present a single-string orientation detection algorithm (SSOD) that is based on the morphological-operator-based RLSA and uses dynamically generated structure elements for the morpho.logical operators ( the closing and erosion operators) [Chiang and Knoblock, 2010].  4.17 shows the pseudo-code of the SSOD. 

 4.17: The pseudo-code for the single-string orientation detection algorithm 

The SSOD ﬁrst calculates the average size of the connected components in the text string, namely AvgSize. Then, the SSOD rotates the string image by 0 degrees to 179 degrees and calculates the maximum width of the bounding boxes of the rotated strings, namely MaxW idth. Considering a text string in the horizontal direction, we should ﬁnd the MaxW idth when the string is rotated to the 45 degrees. Therefore, we can use the length of MaxW idth multiplied by cos(45.) to approximate the length of the string when the string is in the horizontal direction. I do not use the rotated string that has the MaxW idth as the string of 45 degrees orientation to calculate the real orientation of the text string because the characters in the string can have various shapes and hence the MaxW idth can be a few degrees o. from the 45 degrees. 
With the identiﬁed AvgSize and MaxW idth, for each rotated string, the SSOD applies the closing operator  a structure element of height equal to one and width equal to AvgSize to grow the string blobs, as the examples shown in the middle row in  4.18. Then, the SSOD applies the erosion operator  a structure element of height equal to one and width equal to MaxW idth multiplied by cos(45.) to shrink the area of the string blob. If the string blob is not in the horizontal direction, the erosion operator eliminates most of the blobs since their horizontal width is shorter than the approximate length of the horizontal string. 
The SSOD identiﬁes the horizontal string among the rotated strings  the number of remaining pixel after applying the erosion operator. The bottom row in  4.18 shows example results after applying the erosion operator where the horizontal string has more remaining pixel than the tilted string. 

The SSOD only applies on the strings having more than three connected components. This is because the detected orientation of a short string can be dominated by the heights of the short strings’ connected components  the RLSA. Since short strings in a raster map are usually part of a longer string, the AutoTextRecognizer searches from the centroid of a short string for nearby strings and uses the orientations of the nearby strings as the short string’s possible orientations. For example, the most common short strings in my test map are “Av” as avenue, “Dr” as drive, and “Cir” as circle, which are all part of the road names. The AutoTextRecognizer uses a dynamic distance-threshold derived from the size of the bounding box of each short string to limit the search space. 
4.2.4 Automatically Recognizing Characters  OCR Software 
Once the SSOD identiﬁes the string orientations, the AutoTextRecognizer ﬁrst rotates each string clockwise and counterclockwise to the horizontal direction according to its possible orientations (short strings might have more than one detected orientation de.pending on the number of its neighboring strings) to generate a set of rotated strings. Then, to identify the correctly oriented horizontal string ( not the upside-down one), the AutoTextRecognizer sends all rotated strings to a commercial OCR product called ABBYY FineReader 10 and automatically selects the rotated string with the highest returned recognition conﬁdence. 
The AutoTextRecognizer uses the number of connected component in the string, the number of recognized characters, and the number of suspicious characters to calculate the recognition conﬁdence, which is deﬁned as follows: NRC is the number of recognized characters, NSC is the number of suspicious recognized characters, and NCC is the number of connected components of a text string, the recognition conﬁdence is given by: 
NRC . NSC 

RecognitionConfidence = (4.4)
NCC 

If two rotated strings have the same highest recognition conﬁdence, the AutoTextRecog.nizer uses the two recognition results in the ﬁnal results. For short strings with fewer than three characters, if the recognition conﬁdence is less than 50%, the AutoTextRecognizer discards the results since the short strings are very likely to be non-text objects that are mis-identiﬁed. For longer strings, if the recognition conﬁdence is less than 50%, the Au.toTextRecognizer sets the number of suspicious characters to zero and then recalculates the recognition conﬁdence. This is because it is very likely that the quality of the original map is poor so the OCR software marks most of the recognized characters as suspicious. 
4.3 Experiments 

I have implemented the supervised text-layer-separation and automatic text-recognition approaches (the TextLayerSeparator and TextRecognizer) described in this chapter as the text recognition component in a map processing system called Strabo. In this section, I report my experiments on the recognition of text label in heterogeneous raster map  Strabo. I tested Strabo on 15 map from 10 sources. The set of test map includes three scanned map and 12 computer generated map directly generated from vector data, which are the same set of test map used in the experiments of Chapter 3 for extracting road vector data, except one scanned map that has very poor image quality and is not suitable for the character recognition task. 
For the scanned map, we purchased three paper map covering the city of Baghdad, Iraq from the International Travel map, Gecko map, and Gizi Map. I scanned the Bagdad paper map in 350 DPI to produce the raster map. For the computer generated map, we purchased a map in PDF format covering the city of St. Louis, Missouri from the Rand McNally website.5 I downloaded a map covering Afghanistan in PDF format from the United Nations Assistance Mission in Afghanistan website.6 I cropped 10 computer generated map from ﬁve web-mapping-service providers, Google map, Microsoft Live map, OpenStreetMap, MapQuest map, and Yahoo map. The 10 computer generated map cover two areas in the U.S. One area is in Los Angeles, California, and the other one is in St. Louis, Missouri. 
Table 4.1 shows the information of the test map and their abbreviations that we use in this section. For the ITM, GECKO, GIZI, RM, and UNAfg map, I crop a center area for the text recognition experiments. The total numbers of characters and words in the testing area are shown in Table 4.1. 
5http://www.randmcnally.com/ 
6http://unama.unmissions.org/ 


Map Source (abbr.)  Map Count  Map Type  No. of Chars./Words  
International Travel map (ITM)  1  Scanned  1358/242  
Gecko map (GECKO)  1  Scanned  874/153  
Gizi Map (GIZI)  1  Scanned  831/165  
Rand McNally (RM)  1  Computer Generated  1154/266  
UN Afghanistan (UNAfg)  1  Computer Generated  1607/309  
Google map (Google)  2  Computer Generated  401/106  
Live map (Live)  2  Computer Generated  233/64  
OpenStreetMap (OSM)  2  Computer Generated  162/42  
MapQuest map (MapQuest)  2  Computer Generated  238/62  
Yahoo map (Yahoo)  2  Computer Generated  214/54  
Table 4.1: Test map 


 4.19 to  4.22 shows the example areas and text of each test map, where the scanned map show poor image quality compared to the computer generated map. In addition, the text labels in the computer generated map of Google, Live, OSM, MapQuest, and Yahoo contain pixilated non-solid characters, which is especially di.cult for an OCR system to recognize even if the text labels are in the horizontal direction. 
4.3.1 Experimental Setup 

I utilized Strabo and a commercial OCR product called ABBYY FineReader 10 to recog.nize the text labels in the test map. For comparison, I tested the 15 test map  only ABBYY FineReader 10 by directly loading each of the test map into the OCR software for the automatic character recognition. 
4.3.2 Evaluation Methodology 

To evaluate my supervised text-layer-extraction approach, I report the number of user labels that were required for extracting text pixel from a test map  Strabo. For 

(a) ITM map (b) ITM text 

(c) 
GECKO map (d) GECKO text 

(e) 
GIZI map (f) GIZI text 

(a) RM map (b) RM text 

(c) 
UNAfg map (d) UNAfg text 

(e) 
Google map (f) Google text 

(a) Live map (b) Live text 

(c) 
OSM map (d) OSM text 

(e) 
MapQuest map (f) MapQuest text 










(a) Yahoo map (b) Yahoo text 
 4.22: Examples of the Yahoo map 
the 15 test map, three map are scanned map, which contain numerous colors (ITM, GECKO, and GIZI). Strabo automatically quantized the three scanned map  the Mean-shift, Median-cut, and K-means algorithms to generate four quantized images in di.erent quantization levels for each map (the four quantization levels have the number of colors as 8, 16, 32, and 64, respectively). Strabo did not apply the color segmentation algorithms on the computer generated map before user labeling. This is because the computer generated map contain a smaller number of colors. The user started  Strabo for the user-labeling task from the quantized image of the highest quantization level ( the quantized image that has the smallest number of colors). If the user could not distinguish the road pixel from other map features (e.g., background) in the quantized image, the user would need to select a quantized image containing more colors (a lower quantization level) for user labeling. 
For evaluating the recognized text labels, I report the precision and recall on the both of the character and word levels. The character precision is given by: 
NumberofCorrectlyRecognizedCharacters 
P recisioncharacter = (4.5)
NumberofRecognizedCharacters 
The character recall is given by: 

NumberofCorrectlyRecognizedCharacters 
Recallcharacter = (4.6)
NumberofGroundT ruthCharacters 
Similarly, the word precision is given by: 

NumberofCorrectlyRecognizedW ords 
P recisionword = (4.7)
NumberofRecognizedW ords 
The word recall is given by: 

NumberofCorrectlyRecognizedW ords 
Recallword = (4.8)
NumberofGroundT ruthW ords 
4.3.3 Experimental Results 

Table 4.2 shows the number of extracted text layers and the number of user labels for extracting the text layers. The ITM, GECKO, GIZI, RM, and UNAfg map contain solid characters and hence fewer user labels were needed for extracting one text layer. For the other map that have pixilated non-solid characters, the higher numbers of user labels were due to the fact that more non-text labels were used. 
Map Source  No. of Extracted Text Layers  No. of User Labels (Text/Non-Text)  Total User Labels  
ITM  3  6/3  9  
GECKO  3  3/2  5  
GIZI  2  2/2  4  
RM  2  2/2  4  
UNAfg  2  2/2  4  
Google  2  2/4  6  
Live  2  6/12  18  
OSM  2  3/10  13  
MapQuest  2  3/6  9  
Yahoo  2  1/1  2  
Table 4.2: The number of extracted text layers and the number of user labels for extract.ing the text layers 


For the map of Google, OSM, MapQuest, and Yahoo, I labeled one map from each source and used the labels to extract text layers from every map of the source. For example, for the two Yahoo map, I labeled a text label and a non-text label to extract a text layer from one map and then I used the two user labels again to extract the text layer from a second Yahoo map. 
Table 4.3 shows the numeric results from  Strabo and  ABBYY FineReader 10 itself to recognize text string in the 15 test map. Strabo extracted 6,708 characters and 1,383 words from the test map and ABBYY FineReader 10 extracted 2,956 charac.ters and 655 words. The average character precision, character recall, word precision, and word recall of Strabo are 92.77%, 87.99%, 82.07%, and 77.58% compared to ABBYY’s 71.99%, 30.09%, 46.11%, and 20.64%. Strabo produced higher numbers compared to only  ABBYY FineReader 10 in all metrics, especially the recall numbers. ABBYY FineReader 10 did not do well on identifying text regions because of the multi-oriented text strings in the test map. Without Strabo, ABBYY FineReader 10 could only recog.nize text stings that were in the horizontal direction. Moreover, my supervised text-layer extraction method separated text layers that had di.erent colors so that there were only a few text strings overlapping with non-text objects in the extracted text layer. ABBYY FineReader 10 processed the test map as a whole so that some overlapping text strings could not be recognized even when they were in the horizontal direction. 
Map Source  Char. P.  Char. R.  Char. F.  Word P.  Word R.  Word F.  
ITM (Strabo)  93.65%  93.37%  93.51  83.33%  82.64%  82.99%  
ITM (ABBYY)  86.47%  45.66%  59.76  57.55%  33.06%  41.99%  
GECKO (Strabo)  93.44%  86.38%  89.77%  83.1%  77.12%  80%  
GECKO (ABBYY)  77.87%  41.08%  53.78%  66.28%  37.25%  47.7%  
GIZI (Strabo)  95.12%  77.38%  85.34%  82.03%  63.64%  71.67%  
GIZI (ABBYY)  71.35%  16.49%  26.78%  51.43%  10.91%  18%  
RM (Strabo)  93.42%  94.8%  94.11%  87.94%  84.96%  86.42%  
RM (ABBYY)  71.86%  10.4%  18.17%  23.53%  3.01%  5.33%  
UNAfg (Strabo)  91.59%  88.05%  89.78%  82.39%  80.26%  81.31%  
UNAfg (ABBYY)  65.67%  56.07%  60.49%  34.88%  36.57%  35.7%  
Google (Strabo)  97.35%  91.77%  94.48%  89.22%  85.85%  87.5%  
Google (ABBYY)  0%  0%  0%  0%  0%  0%  
Live (Strabo)  94.78%  93.56%  94.17%  75.38%  76.56%  75.97%  
Live (ABBYY)  51.87%  47.64%  49.66%  47.89%  53.13%  50.37%  
OSM (Strabo)  95.45%  77.78%  85.71%  74.36%  69.05%  71.6%  
OSM (ABBYY)  0%  0%  0%  0%  0%  0%  
MapQuest (Strabo)  91.32%  84.03%  87.53%  81.03%  75.81%  78.33%  
MapQuest (ABBYY)  0%  0%  0%  0%  0%  0%  
Yahoo (Strabo)  69.74%  63.55%  66.50%  43.14%  40.71%  41.9%  
Yahoo (ABBYY)  0%  0%  0%  0%  0%  0%  
Avg. (Strabo)  92.77%  87.99%  90.32%  82.07%  77.58%  79.76%  
Avg. (ABBYY)  71.99%  30.09%  42.44%  46.11%  20.64%  28.52%  

Table 4.3: Numeric results of the text recognition from test map  Strabo and ABBYY (P. is precision, R. is recall, and F. is the F-Measure) 
For the test map that contain pixilated non-solid characters ( Google, Live, MapQuest, OSM, and Yahoo), ABBYY FineReader 10 only recognized some charac.ters from the Live map and reported that the other map did not contain text. This was because automatically segmenting the pixilated non-solid characters for identifying text regions is di.cult. 
Overall Strabo achieved accurate text recognition results on both the character and word levels. The errors in Strabo’s results came from several aspects: First, the poor image quality of the test map could result in poor quality of text layers, such as broken characters or the existence of non-text objects in the text layer. In my experiments, the GIZI map had the worst image quality among the scanned map and hence the result numbers of the GIZI map were the lowest among the map with solid characters. 
Second, the similarity between symbols led to false positives. There were many short strings of “Pl” for place in our test map, and most of them were mis-identiﬁed as “PI” (a capital ‘p’ and a capital ‘i’). This is because some font types, the capital ‘i’ is printed as ‘l’ and the OCR software mis-matched the two letters. Moreover, a string could be mis-identiﬁed as a totally di.erent string when the string was upside down, especially short strings, such as “Pl” and “ld”, “99” and “66”.  4.23 shows another example that the string “Zubaida” could be mis-identiﬁed as “epieqnz”. This type of false positive resulting from similar symbols is very di.cult to remove if the actual orientation of the string is unknown. One possible solution is to introduce additional knowledge of the map area to ﬁlter out unlikely results, such as “ld” and “PI” (a capital ‘p’ and a capital ‘i’). 
Third, Strabo could not detect correct orientations for signiﬁcantly curved text strings and the OCR software could not recognize all characters in curved strings.  4.24 

 4.23: The string “Zubaida” can be mis-identiﬁed as “epieqnz” 
shows two examples of curved strings and the rotated horizontal strings according to their detected orientations. If the string was slightly curved, such as the one in Fig.ure 4.24(a), Strabo could detect correct orientation so that one of the rotated horizontal strings is correctly oriented (the third string in  4.24(a)). For the curved string in  4.24(b), Strabo could not detect the correct orientation since the RLSA could only work on slightly curved labels. However, even if Strabo identiﬁed the correct orientation, since the string was curved, the OCR software could not recognize all characters of the string; for example, the recognition results of the string in  4.24(a) was “Darya” and the sub-string “Amu” was not identiﬁed. To overcome this problem, Strabo could use a lower threshold on comparing the connecting angle to the baseline for breaking any curved strings. Then, post-processing to merge the recognition results of the pieces of the curved strings should be used to recover the broken strings. 

(a) A slightly curved string with correct detected (b) A curved string with incorrect detected orien.orientation tation 
 4.24: Examples of the curved strings and their rotated images  the detected orientations 
Fourth, the OCR software could not recognize many of the pixilated non-solid char.
acters. For the pixilated non-solid characters, a character is not necessarily an individual 

connected component, and the CDA might generate incorrect string blobs. The Yahoo map had the most pixilated characters and hence the result numbers were the worst. In addition to the incorrect string blobs, the pixilated characters are di.cult for a machine to process, although humans can recognize the pixilated characters from a distance, which is a similar problem to the CAPTCHA [von Ahn et al., 2003]. 
Fifth, the CDA might not group strings with wide character-spacing. The characters in a string that had wide character spacing were not correctly grouped since the CDA used a distance threshold depending on the sizes of the characters. For example,  4.25 the string “Hindu Kush” in the UNAfg map were not identiﬁed correctly. 

 4.25: The string “Hindu Kush” has a wide character spacing 
Sixth, the CDA might mis-group characters with non-text objects. The extracted text layers had only a few non-text objects and very often the non-text objects had very di.erent sizes than the characters and were farther from a character than the character’s neighbor. Therefore, the CDA did not group the non-text objects with characters. How.ever, in some cases, the non-text objects were close to the ends of a string and hence when the characters in the strings expanded, the characters connected to a non-text ob.ject. For example, the ﬁrst image from the left in  4.26 shows a string “Qeysar” grouped with a dashed line and the second and third images are the rotated images us.ing the detected orientation. The non-text object unbalanced the string and hence the RLSA did not detect the correct orientation. A stricter connected-component ﬁlter can be used to post-process the extracted text layer for removing this type of errors. However, the connected-component ﬁlter might also remove characters and lower the recall of the results. 

 4.26: An example string with a non-text object and the rotated image  the detected orientation 
Last, there was insu.cient information from the OCR software for calculating the recognition conﬁdence. Since ABBYY FineReader 10 is consumer grade software, its results did not provide much information for Strabo to select the correctly oriented rotated string. This could be solved by adopting a OCR SDK into Strabo, such as OCRopus. 
4.3.4 Computation Time 

I built Strabo  Microsoft Visual Studio 2008 running on a Microsoft Windows 2003 Server powered by a 3.2 GHz Intel Pentium 4 CPU with 4GB RAM. The average process.ing time for the CDA on a 1688x1804-pixel text layer of 626 characters was 37 seconds, for a 2905x2384-pixel text layer of 1092 characters was 39 seconds, and for a 850x550.pixel text layer of 78 characters was 2.6 seconds. Dominant factors of the computation time are the image size, the number of characters, and the shortest distance between two characters in a string (a longer distance requires more iterations for the CDA to con.verge). The average processing time for detecting the orientation on a string longer than three characters was 2.2 seconds (a total of 922 strings), and the dominant factor on the computation time was the length of a string. Although I used multi-threading in several places in the system for parallel processing, the implementation was not fully optimized and improvements could still be made to speed the processing. 
4.4 Conclusion 

This chapter presents the TextLayerSeparator and AutoTextReconizers techniques. The TextLayerSeparator separates text layers from raster map with poor image quality or complex map, and the AutoTextReconizers automatically recognizes text labels from the separated text layer. My experiments show that the techniques separated text layers from raster map of 10 sources with varying color usages and image quality  minimal user input and achieved accurate results recognizing text labels at both the character and word levels. 
Chapter 5 
Related Work 

Separating and recognizing geographic features from raster map is an active research area that spans many research ﬁelds, such as geography, pattern recognition, computer vision, image processing, and document analysis. Therefore, it is necessary to deﬁne the terminology used to review the related work in this chapter: 
• 	
Digitizing a map is the process of converting a printed map (e.g., a paper map) into electronic format, which is a raster image of the printed map ( a raster map). 

• 	
Computerizing a map is the process of converting a printed map into a machine-editable format. For example, the results from map computerizing can be a map in Portable Document Format (PDF), where linked nodes ( road vector data) represent the road lines. 

• 	
A geographic feature can be a set of linear objects, such as the road lines and contour lines, a set of character objects, a set of homogeneous-region objects, or a set of symbolic objects. 

• 	
A geographic-feature layer (feature layer) is a raster image that contains a set of pixel that represent an individual geographic feature in a map. 

• 	
Separating a geographic-feature layer or a geographic feature is the process of ex.tracting the set of pixel that represent an individual geographic feature in a map. 

• 	
Recognizing a geographic feature is the process of converting the geographic feature into a machine-editable format, such as the road vectorization process. Therefore, computerizing a map is a process that recognizes every feature layer in a map. Recognizing a geographic feature does not necessarily require ﬁrst separating the feature layer from the raster map. 


In the following sections, I ﬁrst review the related work on separating and recognizing geographic features from raster map in general and then present the related work that focuses on separating and recognizing the geographic features of roads and text labels from raster map 
5.1 Separation of Feature Layers 
Podlasov and Ageenko [2005] utilize user-speciﬁed colors to separate individual feature layers from the raster map and then process the map  the morphological operators for removing the artifacts and restoring the feature layers, such as reconnecting broken contour lines resulting from the layer separation. To expand the foreground area of a target feature layer for restoring the layer, they ﬁrst union all the feature layers excluding the target layer as a mask image. Then, for every iteration, to expand the foreground area of the target feature layer, they shrink the foreground area of the mask image. The expansion is guaranteed to converge since the foreground area of the mask image even.tually disappears. In comparison, I determine the number of iteration for the expansion to reconnect road lines in a road layer  the automatic detected road width and road format, which requires comparatively fewer iterations to restore the road layer. 
For a more general approach, Leyk and Boesch [2010], Henderson et al. [2009], and Lacroix [2009] present automatic techniques to generate a set of color clusters for sep.arating individual feature layers from raster map. Their techniques are based on the assumptions that the color variation within a feature layer is smaller than the variation between feature layers and the pixel of the same layer are mutually close in the image. 
Leyk and Boesch [2010] develop a color image segmentation (CIS) technique that considers the local image plane, frequency domain, and color space for clustering the pixel in raster map into feature layers. The CIS technique handles raster map with poor image quality, such as scanned historical map with high degrees of mixed and false coloring and blurring. Henderson et al. [2009] utilize several classiﬁcation techniques, the K-means, graphic theoretic, and expectation maximization, with a manually speciﬁed number of feature layers to classify the color space of the raster map for separating individual feature layers. Lacroix [2009] develops the median-shift classiﬁcation technique that works in the image and color spaces for identifying a color palette that can be further used to separate individual feature layers from raster map. 
In comparison to the research in this thesis, the techniques presented in this sec.tion [Henderson et al., 2009; Lacroix, 2009; Leyk and Boesch, 2010; Podlasov and Ageenko, 2005] do not further investigate each separated feature layer and hence have no knowledge about the types of the features in the separated layers. The general techniques to separate feature layers from raster map [Henderson et al., 2009; Lacroix, 2009; Leyk and Boesch, 2010] can serve as a pre-processing step to generate the input image for my supervised techniques of separating the road and text layers, which may further reduce the number of required user labels. 
5.2 	Separation and Recognition of Feature Layers 
In this section, I ﬁrst present the related work on recognizing geographic features from raster map that relies heavily on human operators. Then, I describe the related work that utilizes prior knowledge of the map, such as map legends, to reduce the user interaction for recognizing geographic features from raster map. In comparison, my research uses general heuristics of the road lines and text labels in a raster map for the layer separation, road vectorization, and text recognition instead of  any prior knowledge of the map, which can be di.cult to access. Moreover, my supervised techniques on separating road and text layers require only minimal user e.ort. 
5.2.1 	Separation and Recognition of Feature Layers With Intensive User Interaction 
Leberl and Olson [1982] and Suzuki and Yamada [1990] present highly labor-intensive map processing systems that focus on hardware design and techniques to help human operators speed the map computerizing processes. For example, Leberl and Olson [1982] described techniques to aid the operator to move the cursor quickly, such as the line-following technique to automate the cursor movement. 
The United Nations Statistics Division present a map computerizing system called mapcan [mapcan, 1998] for manually converting the linear features in raster map into vector format and recognizing the text strings in raster map. mapcan includes an extensive set of image processing tools (e.g., the morphological ﬁters) and labeling functions for the user to manually computerize the raster map, which requires intensive user input. For example, to recognize text string  mapcan, the user needs to label the areas of each text string and the string has to be in the horizontal direction. 
5.2.2 	Separation and Recognition of Feature Layers With Prior Knowledge 
Cofer and Tou [1972] present a Machine Automated Perception of Pictorial Symbology system (MAPPS) that recognizes feature layers from raster map  a given set of geographic symbols. MAPPS includes hardware devices to digitize 35-mm transparencies to raster map and a software program to identify lines and nodes from the raster map. The feature recognition is achieved by matching the identiﬁed lines and nodes to a given set of symbols. 
Samet and So.er [1994, 1996] present a system named MARCO (denoting MAp Re.trieval by COntent) that is used for the acquisition, storage, indexing, and retrieval of map images. MARCO exploits the map legend to recognize geographic symbols in raster map, such as a ﬁsh-like symbol that represents recreational ﬁshing spots in a map. 
Myers et al. [1996] utilize a veriﬁcation-based computer-vision approach to recognize geographic symbols in the raster map  training samples of the symbols. To separate linear structures and text areas from raster map, they use prior knowledge of the semantic criteria (e.g., the lines that represent rivers are blue) and gazetteer information. 
Dhar and Chanda [2006] separate a topographic map into layers  user-speciﬁed colors. For each layer, they identify the skeleton of each connected component and then recognize geographic symbols (e.g., a tree or a grass symbols) and characters  a set of user-speciﬁed rules applied on the geometric properties of the component skeletons, such as a tree feature as a connected component with one hole (e.g., a torus). 
Kerle and de Leeuw [2009] separate and recognize the layer of population dots ( a set of ﬁlled circles with varying sizes representing the numbers of population) from a historical scanned map of Kenya. Instead of working on the pixel level, they utilize object-oriented image processing techniques that ﬁrst break the raster map into homogeneous square areas and then work on each square area for identifying the dots. The object-oriented image processing techniques work well on the dot feature; however, the features of irregular shapes, such as the text strings or elongated linear features, can cause a problem since the features cannot be broken down into homogeneous square regions. 
5.3 Road Layer Separation and Road Vectorization 
Much research has been performed in the ﬁeld of extracting linear features from raster map, such as separating lines from text [Cao and Tan, 2002; Li et al., 2000], detecting road intersections [Habib et al., 1999; Henderson et al., 2009], and vectorization of road lines [Bin and Cheong, 1998; Itonaga et al., 2003], generic linear objects [Ablameyko et al., 1993a, 1994, 1993b, 1998, 2002a,b, 2001; Ablameyko and Frantskevich, 1992; Bucha et al., 
2007], and contour lines [Arrighi and Soille, 1999; Chen et al., 2006b; Khotanzad and Zink, 2003]. 
One line of techniques uses simple methods to separate the foreground pixel or road layers from raster map and hence can only handle speciﬁc types of map. Cao and Tan [2002], Li et al. [2000], and Bin and Cheong [1998] utilize a preset grayscale threshold to remove the background pixel from raster map and work on the foreground pixel to separate their desired features. The grayscale thresholding technique does not work on raster map with poor image quality. In addition, the work of Cao and Tan [2002] and Li et al. [2000] focuses on recognizing text labels and they do not process the separated road layers further to reconnect the broken lines. Bin and Cheong [1998] extract the road vector data from raster map by identifying the medial lines of parallel road lines and then linking the medial lines. The linking of the medial lines requires various manually-speciﬁed parameters for generating accurate results, such as the thresholds to group medial-line segments to produce accurate geometry of road intersections. 
Habib et al. [1999] work on raster map that contain only road lines for extracting road intersections automatically. They detect the corner points on the identiﬁed road edges and then group the corner points and identify the centroid of each group as the road in.tersection. False-positive corner-points or intersections of T-shape roads can signiﬁcantly shift the centroid points away from the correct locations. Henderson et al. [2009] exploit the pre-deﬁned color structure of the USGS topographic map to ﬁrst separate the road layer and then detect the road intersections  the tensor-voting framework [Mordohai and Medioni, 2006]. A manual step for separating the road layer is required if the raster map does not have a pre-deﬁned color structure. 
Itonaga et al. [2003] employ a stochastic-relaxation approach to ﬁrst extract the road areas and then apply the thinning operator to extract one-pixel width road networks from raster map with good image quality. The stochastic-relaxation approach does not work on scanned map since the road areas do not have consistent color usage. Moreover, Itonaga et al. [2003] correct the distorted lines around the road intersections based on user-speciﬁed constraints, such as the road width. 
Arrighi and Soille [1999] extract the pixel having a red hue as the layer of contour lines and then utilize the morphological operators to remove noise and connect small gaps. Then, they identify the extremities of lines and apply the skeletonization algorithm  the extremities as anchor points to extract the centerlines. Finally, the extremities of the lines are evaluated for connecting or disconnecting the lines. 
For the techniques that are not limited to a speciﬁc type of map, Bucha et al. [2007] utilize the pixel-force ﬁeld algorithm [Bucha et al., 2006] to extract the centerlines of linear objects from raster map. The pixel-force ﬁeld algorithm can apply directly on color map without a binarization pre-processing step; however, the user needs to manually specify the start and end points of the linear feature. 
In a series of publications [Ablameyko et al., 1993a, 1994, 1993b, 1998, 2002a,b, 2001; Ablameyko and Frantskevich, 1992], Ablameyko et al. present an interactive map in.terpretation system that ﬁrst separates the feature layers from a raster map  user selected colors.  user-selected colors to separate the feature layers can be labori.ous since the raster map with poor image quality usually contain thousands of colors. For each feature layer, the map interpretation system produces the centerlines for linear features and the contours for region features. Then, the system links the centerlines or contours to generate the ﬁnal results. The line-linking step relies on user intervention, such as to conﬁrm a merge between two line segments or to indicate the continuation of a line. 
In comparison, my approach includes user training and is capable of handling diverse types of map, especially scanned map. Moreover, I automatically generate the road geometry and correct the distorted lines around road intersections  dynamically generated parameters. 
For the techniques that include more sophisticated user training processes for han.dling raster map with poor image quality, Salvatore and Guitton [2004] use a color extraction method as their ﬁrst step to extract contour lines from topographic map. Khotanzad and Zink [2003] utilize a color segmentation method with user annotations to extract the contour lines from the USGS topographic map. Chen et al. [2006b] exploit the color segmentation method of Khotanzad and Zink [2003] to handle common topo.graphic map ( not limited to the USGS topographic map)  local segmentation techniques. These techniques with user training are generally able to handle map that are more complex and/or have poor image quality. However, their user-training process is complicated and laborious, such as manually generating a set of color thresholds for every input map [Salvatore and Guitton, 2004] and labeling all combinations of line and background colors [Khotanzad and Zink, 2003]. In comparison, my supervised approach for separating the road layers requires the user to provide a few labels for road areas, which is simpler and more straightforward. 
In addition to the research work, a commercial product called R2V from Able Software is an automated raster-to-vector conversion software package specialized for digitizing raster map. To vectorize roads in raster map  R2V, the user needs to ﬁrst provide labels of road colors or select one set of color threshold to identify the road pixel. The manual work of providing labels of only road pixel can be laborious, especially for scanned map with numerous colors, and the color thresholding function does not work if one set of thresholds cannot separate all the road pixel from the other pixel. In comparison, I automatically identify road colors from a few user labels for extracting the road pixel. After the road pixel are extracted, R2V can automatically trace the centerlines of the extracted road pixel and generate the road vector data. However, R2V’s centerline-tracing function is sensitive to the road width without manual pre-processing, which produces small branches on a straight line when the road is wide. I detect the road width automatically and use the detected road information to generate parameters for identifying accurate road centerlines. In my experiments, I tested R2V  a set of test map and show that my road vectorization approach generated better results with considerably less user input. 
5.4 Text Layer Separation and Text Recognition 
Text recognition from raster map is a di.cult task since the text labels often overlap with other features in the map, such as road lines, and the text labels do not follow a ﬁxed orientation within a map [Nagy et al., 1997]. Raveaux et al. [2008] work on a set of color cadastral map to separate the quarter (a set of parcels) and text layers from the map. They ﬁrst apply an anti-fading algorithm on the map for restoring the faded color in the historical map. Then, they use the expectation maximization algorithm to select a color space from a set of color components (e.g., the red, green, and blur or hue, saturation, and lightness color components) for processing the map. Finally, they utilize an edge detector to detect the connected components and use a genetic algorithm [Raveaux et al., 2007] to classify the connected components into the text and quarter layers. They do not further group the characters in the text layer into text strings and recognize the strings. 
For the techniques that work on a speciﬁc type of map or character, Fletcher and Kasturi [1988] and Bixler [2000] assume that the line and character pixel are not over.lapping, and the assumption does not apply on raster map in general. Chen and Wang [1997] utilize the Hough transformation to group nearby numeric letters for identifying straight numeric strings in raster map. Then, they use a set of font and size independent features to recognize the numeric strings. The font and size independent features cannot apply on characters other than the numeric characters. 
One line of research builds speciﬁc character recognition components for handling multi-oriented text strings [Adam et al., 2000; Deseilligny et al., 1995]. Deseilligny et al. [1995] use rotation-invariant features and Adam et al. [2000] use image features based on the Fourier-Mellin Transformation to compare the target characters with the trained character samples for recognizing text labels in map. Pezeshk and Tutwiler [2010] pro.duce artiﬁcial training data ( a set of character images) from a set of initial training data  their Truncated Degradation Model and then recognize the map characters by  two Hidden Markov Models with 10 states trained for each character class. The Truncated Degradation Model helps to reduce the manual e.ort of producing the training data but for each font type, the user still has to generate a set of character images as the initial training data. These methods [Adam et al., 2000; Deseilligny et al., 
1995; Pezeshk and Tutwiler, 2010] require training for each input map, such as providing sample characters for map  di.erent fonts to generate distinct feature sets for the classiﬁcation. 
For a more general text recognition techniques, Li et al. [2000] identify the graphics layer and text labels  connected-component analysis and extrapolate the graphics layer to remove the lines that overlap with characters within each identiﬁed text label. Then, a template-matching based optical character recognition (OCR) component is used to recognize characters from the text labels. However, this extrapolation method does not apply to curved labels. 
Cao and Tan [2002] analyze the geometric properties of the connected component to ﬁrst separate text labels from graphics ( linear objects). Then, they decompose the separated graphics into line segments and a size ﬁlter is used to recover the character strokes that touch the lines. Finally, a commercial OCR product from HP is used to rec.ognize the text labels. Their method does not provide a solution for automatic processing non-horizontal text labels. 
Vel´azquez and Levachkine [2004] utilize the V-Lines and the rectangle growing ap.proaches to separate the text labels from graphics and then recognize the text labels with Artiﬁcial Neural Networks (ANN). Pouderoux et al. [2007] use component analysis and string analysis with dynamic parameters generated from the geometry of the connected components to identify strings in the raster map, which does not consider overlapping labels that commonly exist in map. They then render the identiﬁed strings horizontally for character recognition  the average angle connecting the centroid points of the components in a string, which can be inaccurate when the characters have very di.erent heights or widths. In my approach, I use a robust skew detection method derived from a morphological-operator-based skew detection technique [Najman, 2004] to identify the orientation of each string automatically. 
Although these text recognition techniques [Cao and Tan, 2002; Li et al., 2000; Poud.eroux et al., 2007; Vel´azquez and Levachkine, 2004] assume a more general type of raster map, they all require a manually prepared foreground image of the map as the input of their text recognition algorithms. However, raster map usually su.er from compression and scanning noise and hence contain numerous colors, which means the preprocessing step requires tedious manual work. In comparison, my supervised technique, which uti.lizes color segmentation methods to separate the text layers from raster map, requires only minimal user input. 
Other techniques include additional information for identifying areas of text labels in the raster map. Gelbukh et al. [2004] extend the algorithm from Vel´azquez and Lev.achkine [2004] by exploiting additional information from a toponym database, linguistic dictionaries, and the spatial distributions of various font sizes in a map to detect and cor.rect the errors introduced in the processes of text recognition. Myers et al. [1996] generate hypotheses of the characters in the text labels and the locations of the text labels  a gazetteer and identify text zones in the raster map by detecting and grouping short length patterns in the horizontal and vertical directions. The identiﬁed text zones are then veriﬁed with the hypotheses generated from the gazetteer  a keyword recogni.tion technique that detects the presence of possible text labels based on whole text shape in the text zone. The auxiliary information (e.g., a gazetteer) used in these techniques is 
usually not easily accessible compared to the raster map. 

Chapter 6 

Conclusion and Future Extensions 
In this chapter, I ﬁrst recapitulate the contributions of this thesis and then summarize the heuristics that this thesis exploits for processing the raster map. The heuristics sum up the capability of handling diverse types of map with the techniques described in this thesis. Finally, I identify potential future research directions and conclude. 
6.1 Contributions 

This thesis o.ers a general approach that requires minimal human e.ort for harvesting geographic features from heterogeneous raster map. The approach extracts feature layers of road lines and text labels and detects road-intersection templates for map alignment, generates and vectorizes road geometry for road vector data, and recognizes text labels for map context. 
In Chapter 2, 3, and 4, I presented a general approach for separating the road and text layers from heterogeneous raster map with minimal user input. This approach automatically separates the feature layers from raster map with good image quality (Chapter 2) and handles raster map with poor image quality or complex map  supervised techniques that require minimal user input (Chapter 3 and 4). 
In Chapter 3, I presented an automatic approach for extracting accurate road vector data from the separated road layers automatically. This approach exploits the geometric properties of road lines to automatically generate the road geometry and correct possible distortions in the road geometry. I showed that my approach extracts accurate road vector data from 16 raster map of 11 sources with varying color usages and image quality. 
In Chapter 4, I presented an automatic approach for recognizing text labels in the separated text layers automatically. My text recognition approach focuses on locating individual strings in the text layer and detecting the string orientations to then leverage the horizontal text recognition capability of commercial OCR software. By doing so, my approach requires no training on character recognition and beneﬁts from future improve.ments on commercial OCR software. My experiments on 15 map from 10 sources showed accurate text recognition results from heterogeneous raster map. 
To enable the processing of large raster map, the automatic road vectorization and text recognition techniques both employ a divide-and-conquer approach that divides the input map into tiles, processes each tile, and merges the results from individual tiles. Moreover, since the processing of each divided tile is a independent task, the divide-and.conquer approach beneﬁts from the multi-threading implementation and multi-processor platform for improving the overall e.ciency. 
6.2 Map Processing Heuristics 
A major advantage of the work in this thesis over the related work is that my techniques are not limited to a speciﬁc type of raster map and do not rely on prior knowledge of the input map. Although map contain unstructured features compared to classic document images, such as scanned book pages, the road lines and text labels across di.erent raster map share several visual and geometric properties. 
In this thesis, I presented automatic techniques that detect the common properties of the feature layers of road lines and text labels and then exploit the properties for processing the raster map automatically. In particular, I presented general automatic techniques to separate the road and text layers from the map and then to automatically extract road-intersection templates, vectorize road geometry, and recognize text labels. I showed accurate feature recognition results testing the general techniques on a variety of map from various sources. The tested map include an array of scanned and computer generated map with varying map complexity and image quality. The common properties of the feature layers, road lines, and text labels across raster map are as follows: 
• 	
The background luminosity levels have a dominant number of pixel. 

• 	
The number of foreground pixel is signiﬁcantly smaller than the number of back.ground pixel. 

• 	
The foreground luminosity levels have high contrast against the background lumi.nosity levels. 

• 	
The majority of the roads share the same road format: single-line or double-line roads. 

• 	
The majority of the roads have the same road width. 

• 	
In a double-line raster map, the linear structures ( elongated lines) in single-line format are not roads, but can be other geographic feathers, such as contour lines. 

• 	
Road lines are straight within a small distance ( several meters). 

• 	
Road lines are connected as the road network and hence a road layer usually has a small number of connected components or even only one large connected component when the entire road layer is connected. 

• 	
Characters are isolated, small connected components that are close to each other. 

• 	
Character strokes are generally shorter than lines that represent roads. 

• 	
Characters in a text string should have a similar size and are spatially closer than the characters in two separated strings. 


The above map processing heuristics imply both the strength and limitation of the automatic techniques in this thesis. For example, if a raster map violates the assumption that the background luminosity levels have a dominant number of pixel, the automatic map decomposition technique cannot work and hence we would need to use the supervised layer separation techniques. 
6.3 Future Extensions 

The georeferenced and machine-editable geographic features extracted from raster map o.er many possibilities for future research. In this section, I point out possible future directions based on the research in this thesis. 
With the extracted road vector data, we can utilize map conﬂation techniques [Chen et al., 2008; Wu et al., 2007] to identify the geospatial extent of the raster map. We can then include automatic post-processing based on the geospatial extent of the map to discover the real world lengths of the extracted road lines for improving the accuracy of the extracted road vector data ( the completeness and correctness). For example, given the map extent, we can infer the resolution of the road vector data and then automatically remove unlikely road branches, such as a road segment that is smaller than one meter or road lines constituting a sharp turning angle. 
For the recognized text labels, we can include additional knowledge of the map region to build a dictionary for improving the OCR accuracy. For example, we can include generic map terms for a speciﬁc region, such as “Pl” for place and “Av” for avenue in the U.S. to prevent the OCR from mis-recognizing the small strings. For a set of map covering the same region, we can ﬁrst recognize text labels in each of the map and use the mutual information to help improving the overall recognition accuracy. Moreover, a possible extension is to apply the technique to identify individual text strings in Chapter 4 to raster map  languages other than English for recognizing the text labels in map of foreign countries. 
Given the location of the road vector data and the text labels, one interesting ex.tension is to infer the relationship between the extracted geographic features, such as producing the road names for the roads in the extracted road vector data. This infor.mation helps to further utilize the extracted geographic features, such as planning the direction between two addresses  the road vector data with associated road names or creating a computer-generated map from the extracted geographic features. 
6.4 Conclusion 

This thesis presented a general approach for harvesting geographic features from raster map, which makes raster map a geospatial source that is understandable by machines. The general approach separates individual layers of geographic features from raster map and recognizes the geographic features in the separated layers. In particular, the feature recognition techniques include detection of road-intersection templates, generation and vectorization of road geometry, and recognition of text labels. The general approach automatically handles raster map with good image quality by exploiting common prop.erties of road lines and text labels in raster map. For complex map or map with poor image quality, the general approach employs a supervised technique that utilizes a few labels of road and text areas to separate the feature layers to then leverage the automatic approach for feature recognition. 
The road intersections and road vector data can be used in a map conﬂation system to identify the geospatial extent of the raster map, georeference the map, separated feature layers, and recognized features, and align them to other geospatial data that contain roads. 
We can then exploit the georefereced map, feature layers, and machine-editable map context to provide additional knowledge for viewing and understanding other geospatial data and the area covered by the map. For instance, we can create a keyword-search function for imagery by exploiting the recognized text labels from raster map that are aligned to the imagery. Moreover, we can build gazetteers from the recognized text or generate road vector data for the areas where such data are unavailable. 
Bibliography 

Ablameyko, S., Beregov, B., and Kryuchkov, A. (1993a). Computer-aided cartographical system for map digitizing. In Proceedings of the Second International Conference on Document Analysis and Recognition, pages 115–118. 
Ablameyko, S., Beregov, B., and Kryuchkov, A. (1994). Automatic map digitising: prob.lems and solution. Computing Control Engineering Journal, 5(1):33 –39. 
Ablameyko, S., Bereishik, V., Frantskevich, O., Homenko, M., Melnik, E., Okun, O., and Paramonova, N. (1993b). A system for automatic vectorization and interpretation of map-drawings. In Proceedings of the Fourth International Conference on Computer Vision, pages 456–460. 
Ablameyko, S., Bereishik, V., Frantskevich, O., Homenko, M., and Paramonova, N. (1998). A system for automatic recognition of engineering drawing entities. In Pro.ceedings of the Fourteenth International Conference on Pattern Recognition, volume 2, pages 1157–1159. 
Ablameyko, S., Bereishik, V., Homenko, M., Lagunovsky, D., Paramonova, N., and Patsko, O. (2002a). A complete system for interpretation of color map. International Journal of Image and Graphics, 2(3):453–479. 
Ablameyko, S., Bereishik, V., Homenko, M., Paramonova, N., and Patsko, O. (2002b). Automatic/interactive interpretation of color map images. In Proceedings of the 16th International Conference on Pattern Recognition, volume 3, pages 69–72. 
Ablameyko, S., Beveisbik, V., Homenko, M., Paramonova, N., and Patsko, O. (2001). Interpretation of colour map. a combination of automatic and interactive techniques. Computing Control Engineering Journal, 12(4):188–196. 
Ablameyko, S. and Frantskevich, O. (1992). Knowledge based technique for map-drawing interpretation. In Proceedings of the International Conference on Image Processing and its Applications, pages 550–554. 
Adam, S., Ogier, J., Cariou, C., Mullot, R., Labiche, J., and Gardes, J. (2000). Symbol and character recognition: application to engineering drawings. International Journal on Document Analysis and Recognition, 3(2):89–101. 
Agam, G. and Dinstein, I. (1996). Generalized morphological operators applied to map-analysis. In Proceedings of the 6th International Workshop on Advances in Structural and Syntactical Pattern Recognition, pages 60–69. 
Arrighi, P. and Soille, P. (1999). From scanned topographic map to digital elevation models. In Proceedings of the International Symposium on Imaging Applications in Geology. 
Bin, D. and Cheong, W. K. (1998). A system for automatic extraction of road network from map. In Proceedings of the IEEE International Joint Symposia on Intelligence and Systems, pages 359–366. 
Bixler, J. P. (2000). Tracking text in mixed-mode documents. In Proceedings of the ACM Conference on Document Processing Systems, pages 177–185. 
Bucha, V., Uchida, S., and Ablameyko, S. (2006). Interactive road extraction with pixel force ﬁelds. In Proceeeding of the 18th International Conference on Pattern Recognition, volume 4, pages 829 –832. 
Bucha, V., Uchida, S., and Ablameyko, S. (2007). Image pixel force ﬁelds and their appli.cation for color map vectorisation. In Proceedings of the Ninth International Conference on Document Analysis and Recognition, volume 2, pages 1228 –1242. 
Canny, J. (1986). A computational approach to edge detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 8(6):679–698. 
Cao, R. and Tan, C. L. (2002). Text/graphics separation in map. In Proceedings of the Fourth IAPR International Workshop on Graphics Recognition Algorithms and Applications, pages 167–177. 
Chen, C.-C., Knoblock, C. A., and Shahabi, C. (2006a). Automatically conﬂating road vector data with orthoimagery. GeoInformatica, 10(4):495–530. 
Chen, C.-C., Knoblock, C. A., and Shahabi, C. (2008). Automatically and accurately conﬂating raster map with orthoimagery. GeoInformatica, 12(3):377–410. 
Chen, C.-C., Knoblock, C. A., Shahabi, C., Chiang, Y.-Y., and Thakkar, S. (November, 2004a). Automatically and accurately conﬂating orthoimagery and street map. In The 12th ACM International Symposium on Advances in Geographic Information Systems (ACM-GIS’04), Washington, D.C., USA. 
Chen, C.-C., Shahabi, C., and Knoblock, C. A. (2004b). Utilizing road network data for automatic identiﬁcation of road intersections from high resolution color orthoimagery. In The 2nd Workshop on Spatio-Temporal Database Management -STDBM’04. 
Chen, L.-H. and Wang, J.-Y. (1997). A system for extracting and recognizing numeral strings on map. In Document Analysis and Recognition, 1997., Proceedings of the Fourth International Conference on, volume 1, pages 337 –341. 
Chen, Y., Wang, R., and Qian, J. (2006b). Extracting contour lines from common-conditioned topographic map. IEEE Transactions on Geoscience and Remote Sensing, 44(4):1048–1057. 
Chiang, Y.-Y. and Knoblock, C. A. (2008). Automatic extraction of road intersection position, connectivity, and orientations from raster map. In Proceedings of the 16th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, pages 1–10. 
Chiang, Y.-Y. and Knoblock, C. A. (2009a). Automatic road vectorization of raster map. In Proceedings of the Eighth IAPR International Workshop on Graphics Recognition, pages 27–28. 
Chiang, Y.-Y. and Knoblock, C. A. (2009b). Extracting road vector data from raster map. In Graphics Recognition: Achievements, Challenges, and Evolution, Selected Papers of the 8th International Workshop on Graphics Recognition (GREC), Lecture Notes in Computer Science, 6020, pages 93–105, New York. Springer. 
Chiang, Y.-Y. and Knoblock, C. A. (2009c). A method for automatically extracting road layers from raster map. In Proceedings of the Tenth International Conference on Document Analysis and Recognition. 
Chiang, Y.-Y. and Knoblock, C. A. (2010). An approach for recognizing text labels in raster map. In Proceedings of the 20th International Conference on Pattern Recogni.tion. 
Chiang, Y.-Y., Knoblock, C. A., and Chen, C.-C. (2005). Automatic extraction of road intersections from raster map. In Proceedings of the 13th ACM International Sympo.sium on Advances in Geographic Information Systems, pages 267–276. 
Chiang, Y.-Y., Knoblock, C. A., Shahabi, C., and Chen, C.-C. (2008). Automatic and accurate extraction of road intersections from raster map. GeoInformatica, 13(2):121– 
157. 

Cofer, R. H. and Tou, J. T. (1972). Automated map reading and analysis by computer. In Proceedings of the AFIPS Joint Computer Conferences, Part I, pages 135–145. 
Comaniciu, D. and Meer, P. (2002). Mean shift: a robust approach toward feature space analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 24(5):603– 
619. 

Desai, S., Knoblock, C. A., Chiang, Y.-Y., Desai, K., and Chen, C.-C. (2005). Automat.ically identifying and georeferencing street map on the web. In Proceedings of the 2nd International Workshop on Geographic Information Retrieval, pages 35–38. 
Deseilligny, M. P., Mena, H. L., and Stamonb, G. (1995). Character string recognition on map, a rotation-invariant recognition method character string recognition on map, a rotation-invariant recognition method. Pattern Recognition Letters, 16(12):1297–1310. 
Dhar, D. B. and Chanda, B. (2006). Extraction and recognition of geographical fea.tures from paper map. International Journal of Document Analysis and Recognition, 8(4):232–245. 
Fletcher, L. A. and Kasturi, R. (1988). A robust algorithm for text string separation from mixed text/graphics images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 10(6):910–918. 
Forsyth, D. A. and Ponce, J. (2002). Computer Vision: A Modern Approach. Prentice Hall Professional Technical Reference. 
Gelbukh, A., Levachkine, S., and Han, S.-Y. (2004). Resolving ambiguities in toponym recognition in cartographic map. In Proceedings of the 5th IAPR International Work.shop on Graphics Recognition, pages 104–112. 
Goldberg, D. W., Wilson, J. P., and Knoblock, C. A. (2009). Extracting geographic features from the internet to automatically build detailed regional gazetteers. Interna.tional Journal of Geographic Information Science, 23(1):92–128. 
Habib, A., Uebbing, R., and Asmamaw, A. (1999). Automatic extraction of road in.tersections from raster map. Technical report, Center for Mapping, The Ohio State University. 
Heckbert, P. S. (1982). Color image quantization for frame bu.er display. Computer Graphics, 16:297–307. 
Heipke, C., Mayer, H., Wiedemann, C., and Jamet, O. (1997). Evaluation of automatic road extraction. In International Archives of Photogrammetry and Remote Sensing, pages 47–56. 
Henderson, T. C., Linton, T., Potupchik, S., and Ostanin, A. (2009). Automatic segmen.tation of semantic classes in raster map images. In Proceedings of the Eighth IAPR International Workshop on Graphics Recognition, pages 253–262. 
Itonaga, W., Matsuda, I., Yoneyama, N., and Ito, S. (2003). Automatic extraction of road networks from map images. Electronics and Communications in Japan (Part II: Electronics), 86(4):62–72. 
Kanai, J., Rice, S. V., Nartker, T. A., and Nagy, G. (1995). Automated evaluation of ocr zoning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 17(1):86–90. 
Kerle, N. and de Leeuw, J. (2009). Reviving legacy population map with object-oriented image processing techniques. IEEE Transactions on Geoscience and Remote Sensing, 47(7):2392–2402. 
Khotanzad, A. and Zink, E. (2003). Contour line and geographic feature extraction from USGS color topographical paper map. IEEE Transactions on Pattern Analysis and Machine Intelligence, 25(1):18–31. 
Koutaki, G. and Uchimura, K. (2004). Automatic road extraction based on cross detection in suburb. In Proceedings of the SPIE Image processing: algorithms and systems III, volume 5299, pages 337–344. 
Lacroix, V. (2009). Automatic palette identiﬁcation of colored graphics. In Graphics Recognition: Achievements, Challenges, and Evolution, Selected Papers of the 8th In.ternational Workshop on Graphics Recognition (GREC), Lecture Notes in Computer Science, 6020, pages 95–100. Springer, New York. 
Leberl, F. W. and Olson, D. (1982). Raster scanning for operational digitizing of graphical data. Photogrammetric Engineering and Remote Sensing, 48(4):615–672. 
Levenshtein, V. (1966). Binary codes capable of correcting deletions, insertions and reversals. Soviet Physics Doklady, 10:707. 
Leyk, S. and Boesch, R. (2010). Colors of the past: color image segmentation in historical topographic map based on homogeneity. GeoInformatica, 14(1):1–21. 
Li, L., Nagy, G., Samal, A., Seth, S., and Xu, Y. (1999). Cooperative text and line-art extraction from a topographic map. Proceedings of the Fifth International Conference on Document Analysis and Recognition, pages 467–470. 
Li, L., Nagy, G., Samal, A., Seth, S. C., and Xu, Y. (2000). Integrated text and line-art extraction from a topographic map. International Journal of Document Analysis and Recognition, 2(4):177–185. 
Mao, S., Rosenfeld, A., and Kanungo, T. (2003). Document structure analysis algorithms: A literature survey. Document Recognition and Retrieval X, 5010:197–207. 
mapcan (1998). mapcan for Windows Software Package for Automatic Map Data Entry, User’s Guide and Reference Manual. Computer Software and Support for Pop.ulation Activities, INT/96/P74, United Nations Statistics Division, New York, NY 10017, USA. 
Michelson, M., Goel, A., and Knoblock, C. A. (2008). Identifyinig map on the world wide web. In Proceedings of the 5th International Conference on GIScience, LNCS 5266, pages 249–260. 
Mordohai, P. and Medioni, G. (2006). Tensor Voting: A Perceptual Organization Ap.proach to Computer Vision and Machine Learning. Morgan & Claypool. 
Mori, S., Suen, C. Y., and Yamamoto, K. (1995). Historical review of ocr research and development. Document image analysis, pages 244–273. 
Myers, G. K., Mulgaonkar, P. G., Chen, C.-H., DeCurtins, J. L., and Chen, E. (1996). Veriﬁcation-based approach for automated text and feature extraction from raster-scanned map. In Lecture Notes in Computer Science, volume 1072, pages 190–203. Springer. 
Nagy, G., Samal, A., Seth, S., Fisher, T., Guthmann, E., Kalafala, K., Li, L., Sivasubra.maniam, S., and Xu, Y. (1997). Reading street names from map -technical challenges. In GIS/LIS conference, pages 89–97. 
Nagy, G. L., Nartker, T. A., and Rice, S. V. (2000). Optical character recognition: An illustrated guide to the frontier. In Proceedings of the SPIE International Symposium on Electronic Imaging Science and Technology, volume 3967, pages 58–69. 
Najman, L. (2004).  mathematical morphology for document skew estimation. In Proceedings of the SPIE Document Recognition and Retrieval IX, pages 182–191. 
Pezeshk, A. and Tutwiler, R. (2010). Extended character defect model for recognition of text from map. In Proceedings of the IEEE Southwest Symposium on Image Analysis Interpretation, pages 85–88. 
Podlasov, A. and Ageenko, E. (2005). Extraction and removal of layers from map imagery data. In Kalviainen, H., Parkkinen, J., and Kaarna, A., editors, Image Analysis, volume 3540 of Lecture Notes in Computer Science, pages 1107–1116. Springer. 
Pouderoux, J., Gonzato, J. C., Pereira, A., and Guitton, P. (2007). Toponym recog.nition in scanned color topographic map. In Proceedings of the Ninth International Conference on Document Analysis and Recognition, volume 1, pages 531–535. 
Pratt, W. K. (2001). Digital Image Processing: PIKS Scientiﬁc Inside. Wiley-Interscience, 3rd edition. 
Raveaux, R., Barbu, E., Locteau, H., Adam, S., H´eroux, P., and Trupin, E. (2007). A graph classiﬁcation approach  a multi-objective genetic algorithm application to symbol recognition. In Escolano, F. and Vento, M., editors, Graph-Based Representa.tions in Pattern Recognition, volume 4538 of Lecture Notes in Computer Science, pages 361–370. Springer. 
Raveaux, R., Burie, J.-C., and Ogier, J.-M. (2008). Object extraction from colour cadas.tral map. In Proceedings of the IAPR International Workshop on Document Analysis Systems, volume 0, pages 506–514. 
Rice, S. V., Jenkins, F. R., and Nartker, T. A. (1996). The ﬁfth annual test of ocr accuracy. Technical report, University of Nevada, Las Vegas. 
Saalfeld, A. J. (1993). Conﬂation: automated map compilation. PhD thesis, University of Maryland at College Park, College Park, MD, USA. 
Salvatore, S. and Guitton, P. (2004). Contour line recognition from scanned topographic map. In Proceedings of the Winter School of Computer Graphics, pages 1–3. 
Samet, H. and So.er, A. (1994). A legend-driven geographic symbol recognition system. In Proceedings of the 12th International Conference on Pattern Recognition, volume 2, pages 350–355. 
Samet, H. and So.er, A. (1996). Maco: Map retrieval by content. IEEE Transactions on Pattern Analysis and Machine Intelligence, 18(8):783–798. 
Sezgin, M. (2004). Survey over image thresholding techniques and quantitative perfor.mance evaluation. Journal of Electronic Imaging, 13(1):146–168. 
Shi, J. and Tomasi, C. (1994). Good features to track. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 593–600. 
Suzuki, S. and Yamada, T. (1990). Maris: map recognition input system. Pattern Recognition, 23(8):919–933. 
Tang, Y. Y., Lee, S.-W., and Suen, C. Y. (1996). Automatic document processing: A survey. Pattern Recognition, 29(12):1931–1952. 
Vel´azquez, A. and Levachkine, S. (2004). Text/graphics separation and recognition in raster-scanned color cartographic map. In Llad´os, J. and Kwon, Y.-B., editors, Graphics Recognition, volume 3088 of Lecture Notes in Computer Science, pages 63– 
74. Springer. 

von Ahn, L., Blum, M., Hopper, N. J., and Langford, J. (2003). Captcha:  hard ai problems for security. In Advances in Cryptology, Eurocrypt, pages 294–311. 
Wong, K. Y. and Wahl, F. M. (1982). Document analysis system. IBM Journal of Research and Development, 26:647–656. 
Wu, X., Carceroni, R., Fang, H., Zelinka, S., and Kirmse, A. (2007). Automatic alignment of large-scale aerial rasters to road-map. In Proceedings of the 15th ACM International Symposium on Advances in geographic information systems, pages 1–8. 
Zack, G., Rogers, W., and Latt, S. (1977). Automatic measurement of sister chromatid exchange frequency. Journal of Histochemistry and Cytochemistry, 25(7):741–753. 
Ziou, D. and Tabbone, S. (1998). Edge detection techniques -an overview. International Journal of Pattern Recognition and Image Analysis, 8:537–559. 
Integrating Color Image Segmentation and User Labeling for 
Efficient and Robust Graphics Recognition from Historical map 

Yao-Yi Chiang,1 Stefan Leyk,2 and Craig A. Knoblock3 1 Information Sciences Institute and Spatial Sciences Institute,University of Southern California, 4676 Admiralty Way, Marina del Rey, CA 90292, USA.
yaoyichi@isi.edu 
2 Department of Geography, University of Colorado, UCB260, Boulder, CO 80309, USA.
stefan.leyk@colorado.edu 
3 Department of Computer Science and Information Sciences Institute,
University of Southern California, 4676 Admiralty Way, Marina del Rey, CA 90292, USA.
knoblock@isi.edu 

Keywords-color image segmentation, road vectorization, historical raster map, image cleaning 
I. INTRODUCTION 

map contain valuable cartographic information, such as locations of historical places, contour lines, building footprints, and hydrography. Extracting such cartographic information from map ( creating spatial layers that can be processed in a GIS) would support multiple applications and research fields. For example, there are numerous cases in which historical map have been used to carry out research in land-cover change and biogeography [10, 14], or urban-area development [6]. 
Today, thousands of such map and map series are available in raster format ( digital map images) in a variety of digital archives. Previous work on extracting cartographic information from raster map typically requires intensive user intervention for training and parameter tuning, in particular, when processing historical map of poor graphical quality [7, 13]. Consequently, most studies on analyzing and  cartographic information from historical map are based on time-consuming manual map digitization, which introduces subjectivity because of the limited numbers of map that can be digitized. More advanced semi-or fully automated procedures for cartographic information extraction from historical map would allow the advancement of such studies by including historical spatial data that cover large areas, are derived from a variety of map, and underlie reproducible procedures. 
In this paper, we describe a first demonstration of an interactive approach for cartographic information extraction from raster map that have limited graphical quality and contain thematic color layers. In particular, this approach integrates an efficient and effective image-cleaning procedure that is based on simple user training processes with recently described techniques of Color Image Segmentation (CIS) [11] and road vectorization [3, 4] for processing historical map. We demonstrate this approach on the extraction of road features from a historical topographic map, which suffers from poor graphical quality and thus represents a particularly challenging research object. 
II. RECENT WORK 

Here we provide a brief review of recent map processing research; a more detailed review can be found in [2]. Extracting cartographic information from raster map is challenging due to poor image quality that can be caused by scanning or image compression processes, as well as the aging of the archived paper material, which often causes effects of false coloring, blurring or bleaching [1, 5, 7]. The complexity of raster map contents increases if there are overlapping map layers of geographic features, such as roads, contour lines, and labels in different or similar colors. Color image segmentation has been investigated as a preprocessing step to separate different map color layers [1, 2, 11, 12], but there are still limitations when processing poor quality images of historical map [11]. 
Much research has been devoted to extracting geographic features from particular map series. For example, Itonaga et al. [8] describe a road-line vectorization approach from computer-generated map that cannot be applied to scanned map. Dhar and Chanda [5] extract geographic features from Indian survey map based on user-specified filters that exploit the geometric properties of these features. As with many other approaches their extraction procedure has limited applicability to other map series. Another exemplary approach that is highly customized to a specific map series is the work by Raveaux et al. [15] on extracting quartiers from historical French cadastral map. 
III. METHOD OVERVIEW 

Our interactive approach for cartographic information extraction, which is described here  the example of road vectorization, contains two major steps: (i) the separation of homogeneous thematic map layers  Color Image Segmentation (CIS) [11]; (ii) the cleaning of these separated map layers ( identifying and removing noise pixel from the CIS results) and the subsequent raster-to-vector conversion of the cleaned map layers [3, 4]. These steps allow the generation of spatial data in vector format, which then can be registered in time and space according to the information on the map and used in a GIS for spatiotemporal analysis. 
A. Color Image Segmentation 
As an important preprocessing step, Color Image Segmentation (CIS) separates thematic homogeneous color map layers. CIS is of critical importance since the outcome directly determines the image processing methods to be applied in all subsequent stages of data extraction. In our previous work, a hierarchical CIS approach based on homogeneity computation, color space clustering and iterative global/local color prototype matching, has been implemented and tested on historical USGS topographic map ( 1) [11]. The only input parameters for this CIS approach are the number and type of color layers. The approach has been improved for this case study to overcome some limitations in the final region-growing step, which led to problems in the CIS output e.g., merging of nearby features such as elevation contours or roads. Constraining the final segments to a maximum dimensionality and connectivity tests prevented a large proportion of such merging effects. 
B. Road Layer Extraction, Cleaning, and Vectorization 
To extract the road layer from the segmented map, first a sample area (approximately) centered on a road line has to be labeled ( 2(a)). The thematic map layer that has the same color as the sampled road lines ( the road-like features) are automatically identified in the entire map image and the road type (e.g., parallel lines or single lines) and road width are determined ( 2(b)) [4]. This step is carried out in a rotation-invariant way. 
The identified road layer might contain non-road pixel depending on the quality of the input map. Therefore, prior to generating the road vector data, the road layer has to be cleaned. Commercial products for raster-to-vector conversion, such as R2V1 and Vextractor,2 include image-processing tools, such as morphological or structural operators, that can be selected and adjusted by the user to manually clean the raster image. However, this manual process requires expert knowledge and is highly time demanding. In order to overcome such limitations, in this case study, we use an interactive technique, which incorporates simple user training processes for removing the undesired (non-road) pixel. This technique exploits user provided “noise samples” in order to identify appropriate image-processing tools and generate parameter sets. These parameterized tools are then applied to achieve acceptable cleaning results. The user training process is demonstrated for an example of road layer extraction in the next section. 
Once the road layer has been cleaned, we exploit our previously described technique for automatic generation of the road geometry [3] and subsequently convert the road geometry to road vector data [4]. 
IV. 	CASE STUDY: ROAD VECTORIZATION FROM A HISTORICAL USGS TOPOGRAPHIC MAP 
We demonstrate our approach  a road vectorization example. s 1-3 illustrate the different steps and the user interface of the described technique. 
Color Image Segmentation:  1(b) shows the result of CIS of the original USGS topographic map ( 1(a)). As can be seen, the segmentation successfully separates the color layers, but there are still some remaining merging effects of dense elevation contours and road lines as 
1 http://www.ablesw.com/r2v/ 2 http://www.vextrasoft.com/vextractor.htm 

(a) 
A sample USGS historical topo map 

(b) 
The color image segmentation result  1. Color Image Segmentation 

(a) 
A user label of an exmple road segment 





(b)The extracted road layer with noise pixel  2. Supervised road layer extraction 

well as missing road pixel when road pixel intersect with elevation contours – these are caused by issues of graphical quality as described earlier. Although this procedure could be tuned to provide better results, the raw and unrepaired segmentation outcome was used to test the robustness of the subsequent cleaning and the road vectorization steps when  a general non-post-processed (and thus sub-optimal) CIS outcome. Road Layer Extraction:  2(a) shows the user interface for the road-vectorization approach.  this interface a user labels a rectangular area of 20 pixel to provide a “road sample”.  2(b) shows the identified road layer. In this example, the majority of the detected linear features have a width of one pixel. 

(c) User provides examples of remaining road pixel (d) Large noise objects  thicker than road lines 

(e) 
Large noise objects are removed (f) User provides examples of small noise objects 

(g) 
Cleaning result: noise objects are removed (h) Raw road vectorization results  3. Road layer cleaning with minimal user labeling 



Road Layer Cleaning: The first step of our cleaning process removes large non-road objects; the second step removes small non-road objects. To remove large non-road objects from the road-layer-extraction result ( 3(a)), first, an image ( 3(b)) was created in which the majority of road lines were eliminated and thus non-road objects remained. The majority of road lines were eliminated by applying the erosion operator. The number of iterations for erosion was determined by the detected road width (one pixel according to the one collected sample shown). Next, the user provides an example containing road pixel that are not removed by the erosion operator ( 3(c)). The connected non-road components in  3(b) that have a similar length, width, and number of pixel as the ones in the new user label ( 3(c)) are also eliminated automatically; they represent parts of road features and will be preserved.  3(d) shows the remaining large noise objects that exceed the detected (sampled) typical road width. Removing the objects in  3(d) from  3(a) results in the image shown in  3(e). 
To remove the remaining small noisy objects in  3(e), the user provides local areas containing samples of the objects in question ( 3(f)). Connected components that show similar sizes as the ones in the samples were then removed from  3(e) and the final cleaning result was processed ( 3(g)). 
Road Layer Vectorization: The final road vector data layer ( 3(h)) was generated  the vectorization technique described in [4] with the cleaning result ( 3(g)) as input. Note that since the horizontal map grid line was not removed in the cleaning process, the vectorization result contains a portion of these grid lines. Additional operators would be needed or manual post-processing (e.g., manually edit the road vector data) would have to be carried out to remove such elements. Also some broken road lines can be observed, which indicates a need to refine the procedure or to manually edit the final data layer. Such post-processing steps are commonly required even for conventional approaches, which are applied for common-conditioned map and could not handle map of such limited image quality. 
V. DISCUSSION AND CONCLUSION 
We present the integration of a Color Image Segmentation (CIS) step with an interactive road-layer extraction process that consists of an image cleaning and a vectorization step. We describe a case study to demonstrate the performance of this integrated approach, which minimizes user effort for generating road vector data from historical raster map. Our final vectorization is based on an efficient user intervention strategy: the CIS requires only the input of the numbers of thematic layers in the historical map; the interactive extraction technique takes only 4 user labels as shown in s 2 and 3. 
The described shortcomings in the CIS will be improved by further refining and constraining the final region-growing step; the road extraction process will incorporate connectivity constraints of the road lines and additional image processing operators to increase the robustness of the final result. The presented technique shows high potential for robust extraction of cartographic information from historical map of low graphical quality and opens unique opportunities for “spatio-historical” research in various fields. 
REFERENCES 

[1] 	Chen, Y., Wang, R., Qian, J. (2006) Extracting contour lines from common-conditioned topographic map. IEEE Trans. Geosci. Rem. Sens. 44(4), 1048–1057. 
[2] 	Chiang, Y.-Y. (2010). Harvesting Geographic Features from Heterogeneous Raster map. Ph.D. thesis, University of Southern California. 
[3] 	Chiang, Y.-Y., Knoblock, C. A., Shahabi, C., and Chen, C.-C. (2008). Automatic and accurate extraction of road intersections from raster map. GeoInformatica, 13(2):121-157. 
[4] 	Chiang, Y.-Y and Knoblock, C. A. (2010). Extracting Road Vector Data from Raster map. In GREC. LNCS 6020, pp. 93–105. 
[5] 	Dhar, D. B. and Chanda, B. (2006). Extraction and recognition of geographical features from paper map. IJDAR, 8(4): 232-245. 
[6] 	Dietzel, C., Herold, M., Hemphill, J.J. and Clarke, K.C. (2005): Spatio-temporal dynamics in California's Central Valley: Empirical links to urban theory. International Journal of Geographical Information Science. 19(2):175-195. 
[7] 	Gamba P. and Mecocci A., Perceptual Grouping for Symbol Chain Tracking in Digitized Topographic map, Pattern Recognition Lett. 20 (1999) 355-365. 
[8] 	Itonaga, W., Matsuda, I., Yoneyama, N., and Ito, S. (2003). Automatic extraction of road networks from map images. Electronics and Communications in Japan, 86(4):62-72. 
[9] 	Knoblock, C. A., Chen, C., Chiang, Y.-Y., Goel, A., Michelson, M., and Shahabi, C. (2010). A General Approach to Discovering, Registering, and Extracting Features from Raster map. In Proceedings of the Document Recognition and Retrieval XVII of SPIE-IS&T Electronic Imaging, vol 7534. 
[10] Kozak, J., Estreguil, C. and Troll, M. (2007). Forest cover changes in the northern Carpathians in the 20th century: a slow transition. Journal of Land Use Science. 2(2):127-146. 
[11] Leyk S. (2010). Segmentation of Colour Layers in Historical map based on Hierarchical Colour Sampling. In GREC. LNCS 6020, pp. 231–241. 
[12] Leyk S. 	and Boesch R. (2010). Colors of the Past: Color Image Segmentation in Historical Topographic map Based on Homogeneity. GeoInformatica 14(1): 1-21. 
[13] Leyk S. and Boesch R. (2009). Extracting Composite Cartographic Area Features in Low-Quality map. Cartography and Geographical Information Science 36(1):71-79. 
[14] Petit, 	C.C. and Lambin, E.F.. (2002): Impact of data integration technique on historical land-use/land-cover change: Comparing historical map with remote sensing data in the Belgian Ardennes. Landscape Ecology 17(2), 117-132. 
[15] Raveaux, R., Burie, J.-C., and Ogier, J.-M. (2008). Object extraction from colour cadastral map. In Proceedings of the IAPR DAS, pp. 506-514. 

Recognition of Multi-Oriented, Multi-Sized, and Curved Text 
Yao-Yi Chiang Craig A. Knoblock 
University of Southern California, University of Southern California, Information Sciences Institute Department of Computer Science and Spatial Sciences Institute, and Information Sciences Institute, 
4676 Admiralty Way, Marina del Rey, CA 90292, USA 4676 Admiralty Way, Marina del Rey, CA 90292, USA Email: yaoyichi@isi.edu Email: knoblock@isi.edu 
Abstract—Text recognition is difﬁcult from documents that contain multi-oriented, curved text lines of various character sizes. This is because layout analysis techniques, which most optical character recognition (OCR) approaches rely on, do not work well on unstructured documents with non-homogeneous text. Previous work on recognizing non-homogeneous text typ.ically handles speciﬁc cases, such as horizontal and/or straight text lines and single-sized characters. In this paper, we present a general text recognition technique to handle non-homogeneous text by exploiting dynamic character grouping criteria based on the character sizes and maximum desired string curvature. This technique can be easily integrated with classic OCR approaches to recognize non-homogeneous text. In our experiments, we compared our approach to a commercial OCR product  a variety of raster map that contain multi-oriented, curved and straight text labels of multi-sized characters. Our evaluation showed that our approach produced accurate text recognition results and outperformed the commercial product at both the word and character level accuracy. 
I. INTRODUCTION 

Text recognition, or optical character recognition (OCR), is an active area in both academic research and commercial software development. Effective text recognition techniques are widely used, such as for indexing and retrieval of document images and understanding of text in pictorial images or videos. 
In classic text recognition systems, including most com.mercial OCR products, the ﬁrst step is “zoning,” which ana.lyzes the layout of an input image for locating and ordering the text blocks ( zones). Next, each of the identiﬁed text blocks containing homogeneous text lines of the same orientation is processed for text recognition. However, this zoning approach cannot handle documents that do not have homogeneous text lines, such as artistic documents, pictorial images with text, raster map, and engineering drawings. For example,  1 shows an example map that contains multi-oriented text lines of multi-sized characters and no zones of homogeneous text lines exist. 
To process documents with non-homogeneous text, one approach is to recognize individual characters separately [1, 4, 9], such as utilizing rotation invariant features of speciﬁc character sets for character recognition [4]. However, this approach requires speciﬁc training work and hence cannot 


 1. Multi-oriented and multi-sized characters in a raster map from Rand McNally map 
be easily integrated with the classic, well-developed OCR techniques that process homogeneous text. Moreover, rec.ognizing individual characters separately fails to take the advantage of word context, such as utilizing a dictionary to help recognize grouped characters that represent meaningful words. 
Instead of recognizing individual characters sepa.rately, previous work on extracting text lines from non-homogeneous text for text recognition typically handles speciﬁc cases, such as speciﬁc language scripts [8], straight text lines [5, 10], multi-oriented but similar-sized charac.ters [5, 6]. In our previous work [3], we presented a text recognition approach that locates individual multi-oriented text labels in raster map and detects the label orientations to then leverage the horizontal text recognition capability of commercial OCR software. Our previous work requires manually speciﬁed character spacing for identifying individ.ual text labels and does not consider multi-sized characters. 
In this paper, we build on our previous work [3] and present a text recognition technique to dynamically group characters from non-homogeneous text into text strings based on the character sizes and maximum desired string curvature. The hypothesis is that characters in a text string are similar in size and are spatially closer than the characters in two separated strings. Our text recognition technique does not require training for speciﬁc fonts and can be easily integrated with a commercial OCR product for processing documents that contain non-homogeneous text. 
II. RELATED WORK 

Text recognition from documents that contain non-homogeneous text, such as from raster map [7], is a difﬁcult task, and hence much of the previous research only works on speciﬁc cases. Fletcher and Kasturi [5] utilize the Hough transformation to group characters and identify text strings. Since the Hough transformation detects straight lines, their method cannot be applied on curved strings. Moreover, their work does not handle multi-sized characters. 

Goto and Aso [6] present a text recognition technique to handle multi-oriented and curved text strings, which can have touching characters. Their technique ﬁrst divides the input document into columns of equal sizes and then de.tects connected components within each column for further dividing the columns into blocks. Then the connected com.ponents in each block are expanded in various orientations to compute the local linearity for extracting text strings. This block-based approach works on touching characters but requires characters of similar sizes. 
Vel´azquez and Levachkine [13] and Pal et al. [8] present text recognition techniques to handle characters in various font sizes, font types, and orientations. Their techniques are based on detecting straight string baselines for identifying individual text strings. These techniques cannot work on curved strings. 
Pouderoux et al. [10] present a text recognition technique for raster map. They identify text strings in a map by analyzing the geometry properties of individual connected components in the map and then rotate the identiﬁed strings horizontally for OCR. Roy et al. [11] detect text lines from multi-oriented, straight or curved strings. Their algorithm handles curved strings by applying a ﬁxed threshold on the connecting angle between the centers of three nearby characters. Their orientation detection method only allows a string to be classiﬁed into 1 of the 4 directions. In both [10, 11], their methods are based on the assumption that the string curvature can be accurately estimated from the line segments connecting each character center in a string. However, this assumption does not hold when the string characters have very different heights or widths. In contrast, we present a robust technique to estimate the curvature and orientation of a text string and our technique is independent from the character size. 
III. OVERVIEW OF OUR TEXT RECOGNITION APPROACH Given a document image, there are three major steps in our approach for text recognition. First, we extract the text pixel from the input document. For an input image, the user provides example text areas where each text area is a rectangle that contains a horizontal string. The user can rotate the rectangle to select a text string that is not hori.zontally placed in the image. Since each rectangle contains a horizontal string, we exploit the fact that the text pixel are horizontally near each other to identify the colors that represent text in the image and use the identiﬁed colors to extract the text pixel [2]. Second, we dynamically group 
the extracted text pixel into text strings, which is the main focus of this paper. Third, with the identiﬁed text strings, we employ our previous work [3] to detect the orientation of each string and rotate the stings to the horizontal direction for text recognition  a commercial OCR product. 
This paper focuses on the second step of string identiﬁ.cation, which is described in the next section. The details of the other steps are described in our previous work [2, 3]. 
IV. IDENTIFYING INDIVIDUAL TEXT STRINGS 

Once we extract the text pixel, we have a binary image where each connected component (CC) in the foreground is a single character or a part of a character, such as the top dot of the ‘i’. To group the CCs into strings, we present the conditional dilation algorithm (CDA) and  2 shows the pseudo-code of the CDA. 
The CDA performs multiple iterations to expand and connect the CCs and then uses the connectivity of the expanded CCs to identify individual text strings. As shown in the ConditionalDilation function in  2, before the ﬁrst CDA iteration, the CDA sets every CC as expandable. Next, in an iteration, the CDA tests a set of conditions on every background pixel (the TestConditions sub-function) to determine if the pixel is a valid expansion pixel: a background pixel that can be converted to the foreground for expanding a CC. After an iteration, the CDA evaluates each expanded CC (the CountExpandableCC sub-function) to determine whether the CC can be further expanded in the next iteration and stops when there is no expandable CC. We describe the test conditions to determine an expansion pixel and an expandable CC in the remainder of this section. 
Character Connectivity Condition An expansion pixel needs to connect to at least one and at most two characters. This is because the maximum neighboring characters that any character in a text string can have is two. 
Character Size Condition If an expansion pixel connects to two characters, the sizes of the two characters must be similar. For a character, A, and its bounding box, Abx, the size of A is deﬁned as: 
Size = Max(Abx.Height, Abx.W idth) (1) 

For the characters connected by expansion pixel, the size ratio between the characters must be smaller than a pre.deﬁned parameter (the max size ratio parameter). For two characters, A and B, their bounding boxes are Abx and Bbx, their size ratio is deﬁned as: 
Max(Size(A), Size(B))
SizeRatio = (2)
Min(Size(A), Size(B)) 

This character size condition guarantees that every character in an identiﬁed text string has a similar size. We use the size ratio equal to two because some letters, such as the English letter ‘l’ and ‘e’, do not necessarily have the exact same size, even when the same font is used. 

1400 
Character Expandability Condition An expansion pixel needs to connect to at least one expandable CC and the expandability of a CC is determined as follows: before the ﬁrst CDA iteration, every CC is expandable. After each iteration, the CDA checks the connectivity of each expanded CC and if the expanded CC has already connected to two other CCs, the CC is not expandable. 
Next, for the remaining expanded CCs ( the ones with connectivity less than two), the CDA determines the expandability of each CC by comparing the number of iter.ations that have been done and the original size of each CC before any expansion. This is to control the longest distance between any two characters that the CDA can connect so that the characters in two separated strings will not be connected. For example, in our experiments, we empirically set the longest distance between two characters to 1/5 of the character size (the max distance ratio parameter). As a result, for a character of size equal to 20 pixel, the character will not be expandable after 4 iterations, which means this character can only ﬁnd a connecting neighbor within the distance of 4 pixel plus 1/5 of the size of a neighboring 
CC. String Curvature Condition If an expansion pixel con.
nects two CCs and at least one of the two CCs has a connected neighbor ( together as a string with at least three characters), the curvature of the set of CCs should be less than the maximum desired curvature. This condition 
allows the CDA to identify curved strings and guarantees 
that the characters of the text strings in different orien.tations will not be connected. However, determining the string curvature without knowing how the characters are aligned is unreliable. For example, considering the text string “Wellington”, if we link the mass centers or bounding-box centers of each character to represent the string curvature, the line segments linking any two neighboring characters can have very different orientations since the characters have various heights, such as the links between “We” and the one 
To accurately estimate the curvature of a string, the CDA 

curvature baseline for the string. For example, the left image in  3(a) shows an example string, and the right image shows the rearranged string as if the example string is straight and in the horizontal direction. The CDA generates the rearranged string by ﬁrst aligning each of the characters vertically and rearranging the characters’ positions in the horizontal direction so that the characters are not overlapped. The dashed line in the right image shows the curvature baseline of “dale”. This curvature baseline contains two connecting angles: the ones 
baseline, the CDA determines the string curvature by comparing the connecting angles in the original string to the ones in the curvature baseline. For example,  3(c) shows that .1 is similar to .1 ’ and .2 is similar to .2’ and hence the CDA considers the string “dale” as a straight string ( every original connecting angle is similar to its corresponding one).  3(d) shows an example where .1 is very different from .1 ’ and hence the CDA considers the string “AvRi” as a curved string. 
The CDA uses a curvature parameter to control the maximum desired curvature of a text string (the max curvature ratio parameter). If the difference between one connecting angle of a string and the corresponding angle in the string’s curvature baseline is larger than the curvature parameter, the string violates the string curvature condition. For example, with the curvature parameter set to 30% from the curvature baseline, any string with curvature within 138. (180. divided by 130%), to 234. (180. multiplied by 130%) will be preserved. 
The CDA Output After the CDA stops when there is no expansion pixel, each connected component of the expansion results is an identiﬁed text string. For example, in  4, the set of color blobs are the expansion results (each color represents a connected component) and the black pixel overlapped with a color blob belong to an identiﬁed string. In  4, the CDA does not group small CCs correctly, such as the dot on top of the character ‘i’ . This is because these small CCs violate the character size condition. The OCR system will recover these missing small parts in the character recognition step, which is more robust than adopting special rules for handling small CCs in the CDA. 

1401 

(a) The original string (left) and curvature baseline (right) of “dale” 

(b) 
The original string (left) and curvature baseline (right) of “AvRi” 

(c) 
.1/.2 is similar to .1’/.2 ’ (d) .1 is very different from .1 ’  3. Testing the string curvature condition 




 4. The CDA output 
V. EXPERIMENTS 

We have implemented the techniques described in this paper in our map processing system called Strabo. To evaluate our technique, we tested Strabo on 15 map from 10 sources, including 3 scanned map and 12 computer-generated map (directly generated from vector data).1 These map contain non-homogeneous text of numeric characters and the English alphabet. Table I shows the information of the test map and their abbreviations used in this section.  5 shows one example area in a test map. 
We utilized Strabo together with a commercial OCR product called ABBYY FineReader 10 to recognize the text labels in the test map. For comparison, ABBYY FineReader 10 was also tested alone without Strabo. For evaluating the recognized text labels, we report the precision and recall at both the character and word levels. 
Table II shows the numeric results of our experiments. Strabo produced higher numbers compared to  only 
1The information for obtaining the test map can be found on: http: //www.isi.edu/integration/data/map/prj map extract data.html 

 5. A portion of the GIZI map 
Table I 
TEST map FOR EXPERIMENT 

Map Source (abbr.)  Map Type  # Char/Word  
International Travel map (ITM)  Scanned  1358/242  
Gecko map (GECKO)  Scanned  874/153  
Gizi Map (GIZI)  Scanned  831/165  
Rand McNally (RM)  Computer Generated  1154/266  
UN Afghanistan (UNAfg)  Computer Generated  1607/309  
Google map (Google)  Computer Generated  401/106  
Live map (Live)  Computer Generated  233/64  
OpenStreetMap (OSM)  Computer Generated  162/42  
MapQuest map (MapQuest)  Computer Generated  238/62  
Yahoo map (Yahoo)  Computer Generated  214/54  


ABBYY FineReader 10 in all metrics, especially the recall. ABBYY FineReader 10 did not do well on identifying text regions from the test map because of the multi-oriented text strings in the map. ABBYY FineReader 10 alone could only recognize the stings that are in the horizontal or vertical directions. Moreover, ABBYY FineReader 10 could not detect any text region from the Google, OSM, MapQuest, and Yahoo map and hence the precision and recall are 0 at both the character and word levels. 
Overall Strabo achieved accurate text recognition results at both the character and word levels. This is because the CDA successfully grouped the multi-oriented and multi-sized characters into individual text strings for OCR. More.over, the CDA correctly identiﬁed curved strings that have their curvature within the desired curvature ratio (30%), such as the example shown in  6. 
The errors in Strabo’s results came from several aspects: 

(i) The poor image quality of the test map, especially scanned map, could result in poor quality of text pixel, such as broken characters or the existence of non-text objects in the extracted text pixel. (ii) The CDA might not correctly identify strings with signiﬁcant wide character spacing. For example,  7 the string “Hindu Kush” in the UNAfg map was not identiﬁed correctly. (iii) The CDA might group characters with non-text objects. If there exist non-text objects in the CDA input and a non-text object was close to one end of a string and has a similar size as the ending character, the CDA would connect the end character to the non-text object. A connected-component ﬁlter can be used to post-process the extracted text pixel 



Table II 
TEXT RECOGNITION RESULTS (P. IS PRECISION AND R. IS RECALL) 

Source System Ch. P. Ch. R. Wd. P. Wd. R. 
Strabo 93.6% 93.3% 83.3% 82.6%

ITM 
ABBYY 86.4% 45.6% 57.5% 33% 
Strabo 93.4% 86.3% 83.1% 77.1%

GECKO 
ABBYY 77.8% 41% 66.2% 37.2% 
Strabo 95.1% 77.3% 82% 63.6%

GIZI 
ABBYY 71.3% 16% 51.4% 10.9% 
Strabo 93.4% 94% 87.9% 84.9%

RM 
ABBYY 71.8% 10.4% 23.5% 3% 
Strabo 91.5% 88% 82.3% 80.2%

UNAfg 
ABBYY 65.6% 56% 34.8% 36.5% 
Strabo 97.3% 91.7% 89.2% 85.8%

Google 
ABBYY 0% 0% 0% 0% 
Strabo 94.7% 93.5% 75.3% 76.5%

Live 
ABBYY 51.8% 47.6% 47.8% 53.1% 
Strabo 95.4% 77.7% 74.3% 69%

OSM 
ABBYY 0% 0% 0% 0% 
Strabo 91.3% 84% 81% 75.8%

MapQuest 
ABBYY 0% 0% 0% 0% 
Strabo 69.7% 63.5% 43.1% 40.7%

Yahoo 
ABBYY 0% 0% 0% 0% 

Avg. Strabo 92.7% 87.9% 82% 77.5% 
Avg. ABBYY 71.9% 30% 46.1% 20.6% 


 6. An identiﬁed curved string with its rotated image containing the horizontal string for OCR 
for removing this type of error. However, the connected-component ﬁlter would need careful parameter settings and might also remove characters. 
VI. DISCUSSION AND FUTURE WORK 
We presented a general text recognition technique for processing documents that contain non-homogeneous text lines. This technique handles multi-oriented, curved and straight text lines of multi-sized characters and requires only three parameter settings. We show that our technique can be easily integrated with a commercial OCR product to support text recognition from documents for which classic layout analysis techniques do not work. In the future, we plan to test this text recognition technique on non-English scripts. We also plan to broaden the coverage of our technique to handle documents with mostly touching characters, such as by incorporating a character segmentation method [12]. 
ACKNOWLEDGMENT 

This research is based upon work supported in part by the University of Southern California under the Viterbi School of Engineering Doctoral Fellowship. 
REFERENCES 

[1] Adam, S., Ogier, J., Cariou, C., Mullot, R., Labiche, J., and Gardes, J. (2000). Symbol and character recognition: application to engineering drawings. IJDAR, 3(2):89–101. 
 7. Wide character spacing 

[2] Chiang, Y.-Y. (2010). 	Harvesting Geographic Features from Heterogeneous Raster map Ph.D. Dissertation, University of Southern California. 
[3] Chiang, Y.-Y. and Knoblock, C. A. (2010). An approach for recognizing text labels in raster map. In Proceedings of the 20th ICPR, pages 3199–3202. 
[4] Deseilligny, 	M. P., Mena, H. L., and Stamonb, G. (1995). Character string recognition on map, a rotation-invariant recognition method. Pattern Recognition Let.ters, 16(12):1297–1310. 
[5] Fletcher, L. A. and Kasturi, R. (1988). A robust algo.rithm for text string separation from mixed text/graphics images. IEEE TPAMI, 10(6):910–918. 
[6] Goto, H. and Aso, H. (1998). 	Extracting curved text lines  local linearity of the text line. IJDAR, 2(2– 3):111–119. 
[7] 
Nagy, G., Samal, A., Seth, S., Fisher, T., Guthmann, E., Kalafala, K., Li, L., Sivasubramaniam, S., and Xu, 

Y. 
(1997). Reading street names from map -technical challenges. In GIS/LIS conference, pages 89–97. 



[8] Pal, U., Sinha, S., and Chaudhuri, B. B. (2003). Multi-oriented english text line identiﬁcation. In Proceedings of the 13th Scandinavian conference on Image analysis, pages 1146–1153. 
[9] Pezeshk, A. and Tutwiler, R. (2010). Extended character defect model for recognition of text from map. In Proceedings of the IEEE Southwest Symposium on Image Analysis Interpretation, pages 85–88. 
[10] Pouderoux, J., Gonzato, J. C., Pereira, A., and Gui.tton, P. (2007). Toponym recognition in scanned color topographic map. In Proceedings of the 9th ICDAR, volume 1, pages 531–535. 
[11] Roy, P. P., Pal, U., Llad´os, J., and Kimura, F. (2008). Multi-oriented english text line extraction  back.ground and foreground information. IAPR International Workshop on DAS, 0:315–322. 
[12] Roy, P. P, Pal, U., Llad´os, J., and Delalandre, M. (2009). Multi-oriented and multi-sized touching character segmentation  dynamic programming. In the Pro.ceedings of the 10th ICDAR, pages 11–15. 
[13] Vel´azquez, A. and Levachkine, S. (2004). Text/graphics separation and recognition in raster-scanned color carto.graphic map. In GREC, vol 3088 of LNCS, pages 63–74. 

1403 
Noname manuscript No. 
(will be inserted by the editor) 
General Approach for Extracting Road Vector Data from 

Raster map 
Yao-Yi Chiang · Craig A. Knoblock 
Received: date / Accepted: date 
Abstract Raster map are easily accessible and con.tain rich road information; however, converting the road information to vector format is challenging because of varying image quality, overlapping features, and typi.cal lack of metadata (e.g., map geocoordinates). Previ.ous road vectorization approaches for raster map typ.ically handle a speciﬁc map series and require signif.icant user e.ort. In this paper, we present a general road vectorization approach that exploits common ge.ometric properties of roads in map for processing het.erogeneous raster map while requiring minimal user intervention. In our experiments, we compared our ap.proach to a widely-used commercial product  40 raster map from 11 sources. We showed that overall our approach generated high quality results with low redundancy with considerably less user input compared to competing approaches. 
Keywords GIS · raster map · road vectorization · map processing 
1 Introduction 
For centuries, cartographers have been producing map, which contain valuable geospatial information, such as road lines, text labels, building locations, and contour 
Yao-Yi Chiang University of Southern California, Information Sciences Institute and Spatial Sciences Institute 4676 Admiralty Way, Marina del Rey, CA 90292, USA E-mail: yaoyichi@isi.edu 
Craig A. Knoblock University of Southern California, Department of Computer Sci.ence and Information Sciences Institute 4676 Admiralty Way, Marina del Rey, CA 90292, USA E-mail: knoblock@isi.edu 
lines. Because of the availability of low cost and high-resolution scanners and the Internet, we can now obtain a huge number of map in raster format from various sources. For instance, a digital raster graphic (DRG), which is a georeferenced scanned image of a United States Geological Survey (USGS) topographic map, can be purchased from the USGS website or accessed freely from TerraServer-USA.1 Other map sources, such as online map repositories like the University of Texas Map Library,2 also provide information-rich map and historical map for many countries. Websites such as OpenStreetMap3 and MultiMap4 provide high quality computer-generated map produced directly from vec.tor data with valuable geospatial information, such as business locations. 
Because map commonly contain road networks, raster map are an important source of road information (e.g., road geometry), which is especially valuable for areas where road vector data are not readily available. More.over, since the road networks exist across various geospa.tial data sources (e.g., satellite imagery), the road topol.ogy (e.g., road connectivity) and road geometry ex.tracted from a raster map can be used as matching features to align the map and recognized map features to other geospatial data that contain roads [Chen et al., 2008; Wu et al., 2007]. 
Converting the roads in heterogeneous raster map to vector format is challenging for a number of reasons: ﬁrst, the access to metadata about the map (e.g., map geocoordinates) is often not available, which makes it di.cult to obtain prior knowledge about the region for processing the map. Second, map typically contain 
http://terraserver-usa.com/ 
http://www.lib.utexas.edu/map/ 
http://www.openstreetmap.org/ 
4 

http://www.multimap.com/ 

Fig. 1 The overall approach for extracting road vector data from heterogeneous raster map 
overlapping layers of geographic features, such as roads, contour lines, and labels. Thus, the map content is mul.tilayered and highly complex. Third, the image quality of raster map is sometimes poor due to the scanning and/or image compression processes for creating the map in raster format. 
In this paper we present an end-to-end approach to extracting accurate road vector data from full-size raster map with minimal user input.  1 shows the three major steps of our approach: (1) Road Ge.ometry Extraction, (2) Road Intersection Detec.tion, and (3) Road Vectorization. These three steps build on the map processing work in our earlier papers, which solved speciﬁc subproblems (road intersection de.tection [Chiang et al., 2008], road-intersection-template extraction [Chiang and Knoblock, 2008], road layer ex.traction [Chiang and Knoblock, 2009a], and road vec.torization [Chiang and Knoblock, 2009b]) of the overall approach. 
Beyond the overall integrated approach, this paper makes a number of additional contributions. First, we present the complete algorithms for the road layer ex.traction [Chiang and Knoblock, 2009a], road-intersection.template extraction [Chiang and Knoblock, 2008], and road vectorization [Chiang and Knoblock, 2009b] (Sec.tions 3, 4, and 5). Second, we present fast and scalable algorithms for generating road geometry from raster map that have numerous colors (Section 3.1) and large image sizes (Section 3.3). Third, we present an approach to processing full-size map in the integrated vector.ization process (Section 5.2). Fourth, we evaluate our integrated approach to road vectorization and present experimental results on a variety of map from diverse sources with varying image quality and compare our approach with a commercial product (Section 6). 
The remainder of this paper is organized as follows. Section 2 discusses the related work. Sections 3 to 5 present the three major steps of our overall approach: road geometry extraction, road intersection detection, and road vectorization, respectively. Section 6 reports on our experimental results, and Section 7 presents the conclusion and future work. 
2 Related Work 
In this section, we ﬁrst review the related work on seg.menting color map into layers of geographic features. Then we review the related research on road vector.ization from raster map and commercial products for raster-to-vector conversion. 
2.1 Color Image Segmentation for Raster map 
Leyk and Boesch [2010] present a color image segmen.tation technique that handles a series of scanned his.torical map (Siegfried map) by considering the image plane, frequency domain, and color space. This color image segmentation technique has only been tested on map with similar conditions (e.g., with the same set of map symbols) and may not work on heterogeneous raster map. In our color segmentation process, our ap.proach handles heterogeneous map types by including an interactive step for a user to select the quantized image that best represents the segmented map. 
Lacroix [2009] presents the median-shift technique, which extracts the color palette ( a small set of rep.resentative colors) from a raster map. This technique requires a preprocessing step based on the automatic edge detection, which is prone to color noise. In con.trast, our approach does not rely on the edge detection and handles raster map with poor scan quality. 
Henderson et al. [2009] focus on USGS topographic map to separate individual thematic layers from the map. Their technique is based on the color key ( the colors of individual feature layers in the map) that comes with a series of USGS topographic map. Our color segmentation approach handles a variety of raster map and does not require the knowledge of the map color-key, which is generally unavailable for scanned map. 
2.2 Research on Road Vectorization from Raster map 
Much research work has been performed in the ﬁeld of extracting graphic features from raster map, such as separating lines from text [Cao and Tan, 2002; Li et al., 2000], detecting road intersections [Chiang et al., 2008; Habib et al., 1999], extracting road vector data [Bin and Cheong, 1998; Itonaga et al., 2003], and recognizing contour lines [Chen et al., 2006; Khotanzad and Zink, 2003] from raster map. 
One line of research on graphic extraction techniques uses simple techniques to extract the foreground pixel from raster map and hence can only handle speciﬁc types of map. Cao and Tan [2002], Li et al. [2000], and Bin and Cheong [1998] utilize a preset grayscale threshold to remove the background pixel from raster map and work on the foreground pixel to extract their desired features. The grayscale-thresholding technique does not work on raster map with poor image qual.ity. In addition, the work of Cao and Tan [2002] and Li et al. [2000] focuses on recognizing text labels. They do not process the road pixel further to generate the road geometry and extract the road vector data. Bin and Cheong [1998] extract the road vector data from raster map by identifying the medial lines of parallel road lines and then linking the medial lines. The linking of the medial lines requires various manually speciﬁed parameters for generating accurate results, such as the thresholds to group medial-line segments and to pro.duce accurate geometry of road intersections. 
Habib et al. [1999] focus on raster map that con.tain only road lines to extract road intersections auto.matically. Their road intersection extraction technique detects the corner points on the extracted road edges and then groups the corner points and identiﬁes the centroid of each group as the road intersection. False-positive corner-points or intersections of T-shape roads can signiﬁcantly shift the centroid points away from the correct locations. 
Itonaga et al. [2003] focus on computer-generated raster map that contain only road and background areas. They exploit the geometric properties of roads (e.g., elongated polygons) to ﬁrst label each map area as either a road or background area. Then they apply the thinning operator to extract a 1-pixel width road network from the identiﬁed road areas. The distortion at a road intersection caused by the thinning opera.tor is corrected by merging the intersecting lines with a similar orientation, which requires user-speciﬁed con.straints, such as the maximum deviation between two intersecting lines and the maximum intersecting angle. Their approach cannot handle scanned map and they do not report evaluation results on road vector data. 
In our earlier work on road geometry extraction [Chi.ang et al., 2008], we developed an automatic technique that utilizes a grayscale-histogram-analysis method to automatically separate the foreground pixel from raster map and then identify the road intersections. The his.togram analysis method does not handle scanned map well since the noise introduced in the scanning process is sometimes di.cult to remove automatically. In addi.tion, since we use the thinning operator in our previous work [Chiang et al., 2008], the extracted road intersec.tions are not accurate when the roads are wide. 
In comparison to the previous work that handles speciﬁc types of map [Bin and Cheong, 1998; Cao and Tan, 2002; Habib et al., 1999; Itonaga et al., 2003; Li et al., 2000] and our previous approach [Chiang et al., 2008], this paper presents a semi-automatic approach, which includes user training and is capable of handling diverse types of map, especially scanned map. More.over, we automatically generate accurate road geometry by detecting and correcting the distorted lines around road intersections caused by the thinning operator to handle roads that are wide. 
Previous work has developed techniques that in.clude more sophisticated user training processes for han.dling raster map with poor image quality. Salvatore and Guitton [2004] use a color extraction method as their ﬁrst step to extract contour lines from topographic map. Khotanzad and Zink [2003] utilize a color seg.mentation method with user annotations to extract the contour lines from USGS topographic map. Chen et al. [2006] exploit the color segmentation method of Khotan.zad and Zink [2003] to handle common topographic map ( not limited to USGS topographic map)  local segmentation techniques. These techniques with user training are generally able to handle map that are more complex and/or have poor image quality. However, their user-training processes are complicated and laborious, such as manually generating a set of color thresholds for every input map [Salvatore and Guitton, 2004] and labeling all combinations of line and back.ground colors [Khotanzad and Zink, 2003]. In compar.ison, our semi-automatic approach for extracting road geometry requires the user to provide a few labels for road areas, which is simpler and more straightforward. 
2.3 Commercial Products for Raster-to-Vector Conversion 
5 http://www.adobe.com/products/illustrator.html 
CorelDraw Graphics Suite,6 and VectorMagic.7 These commercial products are designed to generate vector.ized boundaries of homogenous color areas from input images. For road vectorization from raster map, we want to extract the centerlines of road areas and these products do not support this capability. 
There are commercial products that o.er centerline vectorization functionality. Vextractor8 and Raster-to.Vector9 use line approximation algorithms that gener.ate vector lines based on the locations of the pixel in.side road areas. The line approximation algorithms do not take into consideration the geometry and topology of the road network and hence the resulting road vec.tor data are o.-center, such as the Vextractor results shown in  2(a), or have inaccurate road topol.ogy ( two lines that are connected at an intersec.tion in the road layer are not necessarily connected in the road vector data), such as the results shown in Fig.ure 2(b). Another commercial product called R2V from Able Software10 is an automated raster-to-vector con.version software package specialized in digitizing raster map. R2V is speciﬁcally designed for raster-to-vector conversion from map and can handle a variety of map speciﬁc linear features, such as curved roads and con.tour lines.  2(c) shows more accurate results in terms of road geometry and topology compared to the results of Vectractor and Raster-to-Vector.11 
To vectorize roads in raster map  R2V, the user needs to ﬁrst provide labels of road colors or se.lect one set of color thresholds to identify the road pix.els. The manual work of providing labels of only road pixel can be laborious in R2V, especially for scanned map with numerous colors, and the color thresholding function does not work if one set of thresholds cannot separate all of the road pixel from the other pixel. In comparison, we automatically identify road colors from a few user labels for extracting the road pixel. After the road pixel are extracted, R2V can automatically trace the centerlines of the extracted road pixel and generate the road vector data. However, R2V’s centerline-tracing function is sensitive to the road width without man.ual pre-processing and produces small branches from a straight line if the line is wide. We detect the road width automatically and use the detected road information to generate parameters for identifying accurate road cen.terlines. In our experiments, we tested R2V  our 

6 http://www.corel.com/ 
7 http://vectormagic.com/home 
8 http://www.vextrasoft.com/vextractor.htm 
9 http://www.raster-vector.com/ 
10 http://www.ablesw.com/r2v/ 
11 In these examples, we gave the three commercial products the same input image and we used the automatic vectorization function of each product to generate the sample results. 



(c) 
R2V (green pixel are the vectorization results and black pixel are road areas) 


Fig. 2 Sample screenshots of the road vectorization results from commercial products 
test map and show that our approach generates better results. 
3 First Step: Road Geometry Extraction 
In this section, we present our technique for generat.ing the road geometry from raster map. This tech.nique can handle raster map with poor image qual.ity, such as scanned map. We ﬁrst present a super.vised approach for extracting road pixel from raster map. Next, we describe how we exploit our previous work [Chiang et al., 2008] to identify the road cen.terlines from the extracted road pixel automatically. Finally, since scanned map are usually large images (a typical 350 dot-per-inch (DPI) scanned map can be larger than 6000x6000 pixel), we present an fast algo.rithm for detecting the road width and road format of large map e.ciently. This algorithm is an enhanced version of the Parallel-Pattern-Tracing algorithm from our previous work [Chiang et al., 2008] 
3.1 Supervised Extraction of Road pixel 
There are three major steps for the supervised extrac.tion of road pixel from raster map. The ﬁrst step is to quantize the color space of the input image. Then, a user labels road areas of every road color in the quan.tized image. Since the quantized image has a limited number of colors, we can reduce the manual e.ort in this user-labeling step. Finally, we automatically iden.tify a set of road colors from the user labels and gen.erate a color ﬁlter to extract the road pixel from the raster map. For the non-road map features drawn  the same color as the roads, we will remove them in the ﬁnal step to generate the road geometry (Section 3.2). We describe the details of each step and the labeling criteria in the following subsections. 
3.1.1 Color Quantization 
Distinct colors commonly represent di.erent layers ( a set of pixel representing a particular geographic fea.ture) in a raster map, such as roads, contour lines, and text labels. By identifying the colors that represent roads in a raster map, we can extract the road pixel from the map. However, raster map usually contain numerous colors due to the scanning and/or compres.sion processes and the poor condition of the original documents (e.g., color variation from aging, shadows from folding lines). For example,  3(a) shows a 200x200-pixel tile cropped from a scanned map. The tile has 20,822 distinct colors, which makes it di.cult to manually select the road colors. To overcome this dif.ﬁculty, we apply color quantization algorithms to group the colors of individual feature layers into clusters. Since the color variation within a feature layer is generally smaller than the variation between feature layers in a map, after applying the color quantization algorithms, we can extract individual feature layers by selecting speciﬁc color clusters. 
Our color quantization step includes three algorithms: the Mean-shift [Comaniciu and Meer, 2002], the Median-cut [Heckbert, 1982], and the K-means [Lloyd, 1982] algorithms. The Mean-shift algorithm is ﬁrst applied to preserve the edges of map features (e.g., road lines) while reducing noise. The Median-cut algorithm, which requires the least computation time among the three color quantization algorithms, is then applied to fur.ther quantize the image. The goal of the Median-cut algorithm is to keep image details in the quantized im.age, such as the image texture but the goal of our color quantization is to have a single color representing a sin.gle feature in the map ( eliminating the image tex.ture). Therefore, we apply the K-means algorithm to the result of the Median-cut algorithm to merge simi.lar colors for removing image details. We explain each algorithm in the following paragraphs. 
The Mean-shift algorithm considers the spatial re.lationships between colors in the image space and in the color space ( the image texture) and works in a multi-dimensional space of the image coordinates, X and Y, and the HSL color space, hue, saturation, and lu.minous. We use the HSL color space because of the fact that hue, saturation, and luminous provide good repre.sentation of human perception [Cheng et al., 2001]. For a pixel in the raster map, P (x, y), the corresponding node in the ﬁve-dimensional space ( X and Y from the image coordinates plus H, S, and L from the color space) is N(x, y, h, s, l), where h, s, and l represent the color of P . 
To reduce the noise in a raster map, for a pixel, P (x, y), the Mean-shift algorithm starts from comput.ing the mean node, M(xm,ym,hm,sm,lm), from N’s neighboring nodes. The mean node’s position consists of the mean values on each of the axes X, Y, H, S, and L of N’s neighboring nodes within a local area (we use a spatial distance of 3 pixel and a color distance of 25 to deﬁne the local area). If the distance between M and N is larger than a small threshold (we use a small threshold to limit the running time for the Mean-shift algorithm to converge), the Mean-shift algorithm shifts N to M and recalculates the mean node within the new local area. After the Mean-shift algorithm con.verges (the distance between the mean node and N is no longer larger than the threshold), the H, S, and L values of N are used as P (x, y)’s color. In the example shown in  3, the Mean-shift algorithm reduces the number of colors in  3(a) by 72% as shown in  3(b). 
The results after the Mean-shift algorithm can still have many colors and we utilize the Median-cut after the Mean-shift algorithm to generate an image with at most 1,024 colors. The Median-cut algorithm ﬁrst lo.cates the minimum box which contains every color in the input image in the three dimensional HSL space. Then the algorithm sorts the colors  the color com.ponent that varies the most ( the longest axis of the box) and divides the minimum box into two boxes at the median of the sorted colors. This sort-and-divide process continues to apply on the new divided boxes until the total number of boxes is smaller than the de.sired number of colors in the resulting quantized image. The colors in the same box of the sort-and-divide result are then represented by their median color to generate the quantized image. 
To further merge similar colors in the raster map, we apply the K-means algorithm to generate a quan.tized image with at most K colors. The K-means algo.rithm can signiﬁcantly reduce the number of colors in a raster map by maximizing the inter-cluster color vari.ance; however, since the K-means algorithm considers only the color space, it is very likely that the resulting map has its map features merged with a small K. For example,  3(c) shows the quantized map with K as 8 where the text labels have the same color as the road edges. Therefore, the user would need to select a larger K to separate di.erent features, such as in the quantized map in  3(d) with K as 16. 
The Median-cut algorithm helps to reduce the run.ning time of the K-means algorithm by limiting the in.put number of colors to the K-means algorithm to 1024. The Median-cut algorithm cannot replace the K-means algorithm because the Median-cut algorithm keeps im.age details in the quantized image. For example, as shown in  4, if we apply the K-means and Median-cut algorithms directly to the original image, the Median-cut results shows signiﬁcant color variation within an image object, such as the yellow pixel on the orange roads. 
3.1.2 User Labeling 
In the user-labeling step, we ﬁrst generate a set of quan.tized map in multiple quantization levels  various K in the K-means algorithm. Then the user selects a quantized map that contains road lines in di.erent col.ors from other features and provides a user label for each road color in the quantized map. A user label is a rectangle that should be large enough to cover a road intersection or a road segment. To label the road colors, the user ﬁrst selects the size of the label. Next, the user clicks on the approximate center of a road line or a road intersection to indicate the center of the label. The user label should be (approximately) centered at a road in.tersection or at the center of a road line, which is the constraint we exploit to identify the road colors in the next step. For example,  5(a) shows an example map and  5(b) shows the quantized map and the labeling result to extract the road pixel. The two user labels cover one road intersection and one road segment and contain the two road colors in the quantized map ( yellow and white) . 
3.1.3 Automatic Identiﬁcation of Road Colors and Extraction of Road pixel 
Each user label contains a set of colors, and some of the colors represent roads in the raster map. We exploit two geometric properties of the road lines in a user label to identify the road colors of a given user label, namely the centerline property and the neighboring property. 
The Centerline Property Because the user labels are centered at a road line or a road intersection, the 

(a) 
An example tile (b) The Mean-shift result 

(c)
TheK-meansresult,K=8





Fig. 3 An example map tile and the color quantization results with the color cubes 
pixel of a road color are a portion of one or more road lines that pass through or nearby the image center. For example, we ﬁrst separate pixel of individual colors from the user label shown in the top-right of  5(b) and the result is a set of six images shown in  6 (background is shown in black) and every decomposed image contains only one color from the user label. The pixel in the decomposed images, Image 3, 4, and 5, are portions of the road lines in the user label. 
To exploit the centerline property, for each decom.posed image, we ﬁrst detect lines that are constituted from the connected objects in the image. This is achieved by applying the Hough transform [Duda and Hart, 1972] to identify a set of Hough lines from the skeletons of 

Fig. 4 Median-cut algorithm result contains more image details while K-means result contains more homogenous regions 
the connected objects. The Hough transform is a fea.ture extraction technique that can identify lines ( the Hough lines) from pixel that do not necessary rep.resent every part of the lines. Since the image center of a user label is the center of a road line or a road intersection, if a Hough line is detected near the im.age center, the Hough line is most likely to represent a portion of the road lines. Hence, we detect the Hough lines in each decomposed image and compute the aver.age distance between the detected Hough lines to the image center to determine if the foreground pixel (non.black pixel) in a decomposed image represent roads in the raster map. 
 7 shows the detected Hough lines of each de.composed image, where the Hough lines that are within a distance threshold to the image centers are drawn in red and others are drawn in blue (this distance thresh.old is only used to help explain the idea). In  7, the decomposed images that contain road pixel (Im.age 3, 4, and 5 ) have more red lines than blue lines and hence the average distances between their Hough lines to their image centers are smaller than the other decomposed images. Therefore, the decomposed image that has the smallest average distance is classiﬁed as a road-pixel image ( the color of the foreground pixel in the decomposed image represents roads in the raster map). The other decomposed images with their aver.age distances between 1 pixel to the smallest average distance are also classiﬁed as road-pixel images. This 

(a) An example scanned map 


(b) The quantized map and user labels (the red boxes and crosses show the original positions and image centers of the two user labels) 
Fig. 5 An example of the supervised extraction of road pixel 
criterion allows the user label to be a few pixel o. (de.pending on the size of the user label) from the actual center of the road line or road intersection in the map, which makes the user labeling easier. In our example, Image 5 has the smallest average distance, so we ﬁrst classify Image 5 as a road-pixel image. Then, since Im.age 4 is the only image with its average distance within a 1-pixel distance to the smallest average distance, we also classify Image 4 as a road-pixel image. 
The Neighboring Property Because the road pix.els are spatially near each other in a user label, the pixel of the road colors should be spatially near each other. For example, the majority of pixel in Image 3 can ﬁnd an immediate neighboring pixel in Image 4 and 5 and vice versa, but the majority of pixel in Im.age 0 cannot ﬁnd an immediate neighbor in Image 3, 4, and 5. 
Exploiting the centerline property for the road-pixel image classiﬁcation is based on the average distance be.


Fig. 7 The identiﬁed Hough lines   6 as the input (background shown in black) 
tween the detected Hough lines to the image center. If the Hough transform detects only a few Hough lines, the Hough lines constituted from noise pixel can signif.icantly bias the average distance and hence the image will not be classiﬁed correctly. Therefore, we present the Edge-Matching algorithm for exploiting the neigh.boring property to determine if any of the decomposed images that are not classiﬁed as road-pixel images us.ing the Hough-line method ( Image 0 to 3 ) is a road-pixel image. 
The Edge-Matching algorithm utilizes a road tem.plate generated  the already classiﬁed road-pixel images and compares the unclassiﬁed images with the road template to identify road-pixel images. In our ex.ample, the road template is the combination of Image 4 and 5 as shown in  8(a) (background is shown in black). Next, we use the road template to evaluate Image 0 to 3 in turn. For a color pixel, C(x, y), in a given decomposed image to be evaluated, we search a 3x3-pixel neighborhood centered at (x, y) in the image 

(a) 
An example road template 

(b) 
The Edge-Matching results 




Fig. 8 Classifying the decomposed images  the road tem.plate 
of the road template to detect if there exists any road pixel. If one or more road pixel exist, we mark the pixel C(x, y) as a road pixel since it is spatially near one or more road pixel. After we examine every fore.ground pixel in a given decomposed image, if more than 50% of the foreground pixel in that image are marked as road pixel, we classify the decomposed image as a road-pixel image. 
 8(b) shows an example of the Edge-Matching algorithm. The ﬁrst row shows the foreground pixel of the Image 0 to 3 (background is shown in black) and the second row is the match with the road template. The bottom row shows the results after we apply the Edge-Matching algorithm to each of the images, where the non-black pixel are the matched pixel. Only Im.age 3 has more than 50% of its foreground pixel iden.tiﬁed as matched pixel so we classify Image 3 as a road-pixel image and discard the others. 
We process every user label and identify a set of road-pixel images from each label. We then scan the quantized map and extract the pixel that have their colors as one of the identiﬁed road colors as the road pixel.  9 shows the extracted road pixel of Fig.ure 5(a). Note that the map features that share the same color as the road lines are also extracted as the rectangular area shown in  9. These features will be removed in the next step when we generate the road geometry. 

Fig. 9 The extracted road pixel of  5(a) 
3.2 Automatic Identiﬁcation of Road Centerlines 
The extracted road layer ( the set of extracted road pixel) from the previous section can contain non-road map features, such as area features or text strings, if the map features are drawn  the same color as the roads. In this section, we brieﬂy explain our previous work [Chiang et al., 2008] that we employ for removing these non-road features and automatically identify the road geometry. 
Concerning the non-road map features of text strings, text/graphics separation techniques [Cao and Tan, 2002; Tombre et al., 2002] can be used to separate the text strings from the extracted road pixel ( linear ob.jects) . Our previous work uses the text/graphics sep.aration technique from Cao and Tan [2002] since the technique was developed and tested for map process.ing. 
To remove the non-road features other than text, reconnect broken road lines, and generate the road ge.ometry, we ﬁrst detect the road width and road for.mat in the input map, and then we dynamically use three types of morphological operators: the erosion op.erator, the dilation operator, and the thinning operator (see [Pratt, 2001] for a detailed description of the mor.phological operators). The erosion operator is used to remove noise objects that are smaller than the road lines and to temper road areas for generating accurate road centerlines. The dilation operator is used to ex.pand road areas for connecting broken road lines and for ﬁlling up holes in the road areas. The thinning oper.ator is used to extract the centerlines of the roads ( the skeleton of the road areas). 
The numbers of iterations of these morphological operators ( the number of times we apply each mor.phological operator) are decided dynamically based on the detected road width and road format. For example, to remove thick non-road features, if road width is 4.pixel wide, we ﬁrst remove the road lines by applying the erosion operator twice with a 3-by-3 structuring el.ement (or a 5-by-5 structuring element applied once): one iteration erodes a road line by 2 pixel; one at each side of the road. Then we apply the dilation operator to re-grow and obtain the non-road features that are thicker than road lines. By subtracting the resulting non-road features from the road layer, we remove the non-linear features such as the rectangular area in the upper-right of  9. 
Once the non-road features are removed, we use the dilation operator to expand the road areas and recon.nect the road lines automatically. Since the expansion of the road areas should not connect two nearby roads, the number of iterations of the dilation operator with a 3-by-3 structuring element is half of the detected road width for single-line format roads. This is because we assume that two road lines should be at least a road-width apart in a map. 
Concerning double-line format roads, the dilation operator not only reconnects the broken lines, but also merges parallel road lines into thick lines in single-line format. For example, by applying the dilation opera.tor twice with a 3-by-3 structuring element, the paral.lel road lines that are 4-pixel apart ( the detected road width is 4-pixel wide) are merged. This dilation technique ﬁlls up the areas in-between the parallel road lines if the distance between the road lines and every pixel in-between the parallel road lines is less than half of the detected road width. However, for road inter.section areas, depending on how the intersections are drawn in the map, the distance between the road lines and an intersection center can be larger than the half of the detected road width. In this case, the number of iterations of the dilation operator needs to be increased for ﬁlling up the areas within road intersections of the parallel lines. 
After the dilation operator, we apply the erosion op.erator to erode the thickened road lines. Finally, we use the thinning operator to generate the 1-pixel width road centerlines ( the road geometry) from the erosion re.sults.  10 shows the extracted road geometry of  9. 
In this process of extracting the road geometry from raster map, the morphological operators used for gen.erating the road geometry cause distorted lines around road intersections. As shown in  11, if we apply the thinning operator directly on the thick lines after the dilation operator shown in  11(a), the lines that are near intersections are signiﬁcantly distorted as shown in  11(b). Our approach to reduce the ex.tent of the line distortion is to erode the lines  the 







Fig. 11 Distorted road lines near road intersections caused by the thinning operator 
erosion operator and then apply the thinning operator.  11(c) and  11(d) show that the extent of the line distortion is smaller after we apply the erosion operator; however, the distortion is still not completely eliminated and will be handled in the next step by ex.tracting accurate road geometry around intersections. 
3.3 Single-Pass Parallel-Pattern Tracing Algorithm 
In our previous work [Chiang et al., 2008], we devel.oped the Parallel-Pattern-Tracing algorithm (PPT) for identifying the road format ( single-line or double-line format) and road width of the dominant road type in the raster map. For example, if 80% of the roads in the map have the road width as 10 pixel and the other 20% have the road width as 3 pixel, the resulting road width after the PPT is 10 pixel. 
The PPT checks each foreground pixel to determine if there exists any corresponding pixel in the horizontal and vertical directions at a certain road width. If we ﬁnd a corresponding pixel in each direction, we classify the pixel as a parallel-pattern pixel of the given road width. By applying the PPT iteratively from 1-pixel wide road width to K-pixel wide, we can identify the road width and road format of the majority of the roads in the map by analyzing the number of parallel-pattern pixel at each of the tested road widths. 
In our previous work, we implemented the PPT  two convolution masks. One convolution mask works in the horizontal direction and the other one works in the vertical direction to ﬁnd the corresponding pixel. The sizes of the convolution masks are designed to cover the road areas in-between two parallel road lines: if the road width is X pixel, the size of the con.volutions mask is Y xY pixel and Y is X . 2 + 1. For example, if the road width is 2 pixel, the size of the convolution masks is 5x5 pixel, which means by ap.plying the convolution masks to a pixel, we check one pixel at 2-pixel-distance to left (towards the top) of this pixel, one pixel at 2-pixel-distance to right (towards the bottom) of this pixel. Note that these convolution masks can misidentify parallel-pattern pixel if there are non-road pixel in the image, which is ﬁne because the PPT does not need every parallel-pattern pixel to be identiﬁed for detecting the road width [Chiang et al., 2008] 
For N foreground pixel, the number of computing steps to iteratively apply the PPT on the road width from 1 pixel to K pixel is: 
K

PPT (N, K)= N . (2r + 1)2 (1) 
r=1 

The time complexity is O(NK3), which requires sig.niﬁcant computing time when we have a large map (a large N) and/or we run the PPT with more iterations (a large K). 
To improve the time complexity, we developed the Single-Pass Parallel-Pattern-Tracing algorithm (SPPT), which does not rely on the image convolution and only requires a single-pass scan on the image. Moreover, the SPPT keeps a record of previously identiﬁed parallel patterns to further reduce the time complexity, which works as follows: in the previous work, to check whether there is a parallel pattern for a foreground pixel, P , at a distance D, the PPT has to check 4 pixel, 1 pixel towards the top of P at the distance D, 1 pixel to.wards the bottom of P at the distance D, 1 pixel to the left of P at the distance D, and 1 pixel to the right of P at the distance D. In the SPPT, to check whether there is a parallel pattern for a foreground pixel, P , at a distance D, the algorithm checks the parallel-pattern records and 2 pixel, 1 pixel to the right of P at the distance D and 1 pixel towards the bottom of P at the distance D. 

 12 shows the pseudo-code of the SPPT. The SPPT starts from the upper-left pixel in the image and scans the image one row at a time from left to right. To check the parallel pattern from 1 to K pixel, for a foreground pixel, the SPPT ﬁrst records the existence of this foreground pixel for the pixel at the distance from 1 to K pixel to the right and towards the bot.tom of this foreground pixel ( set the horizontal and vertical arrays as true in the pseudo-code). Next, the SPPT checks if the foreground pixel has a previously found parallel-pattern pixel ( check the horizontal and vertical arrays  the position of this foreground pixel) and then checks the pixel at the distance from 1 to K pixel to the right and towards the bottom of this foreground pixel ( the IsForeground function in the pseudo-code). The parallel pattern record ( the hor.izontal and vertical arrays) eliminates searching in the pixel’s left/top direction. Therefore, for N foreground pixel, to iteratively apply the SPPT on the road width from 1 to K pixel wide, the number of steps is: 
SP P T (N, K)=2 . N . K (2) The time complexity is O(NK), which is signiﬁcant less than  the convolution masks and enables e.cient processing of large map. 
The PPT keeps one record of the number of parallel-pattern pixel for each road width for the entire image; 
and the overall space complexity for the PPT in addi.tion to the space for storing the image is: PPT (N, K)= K (3) The SPPT keeps tracking whether a foreground pixel is 
a parallel-pattern pixel during the process. The tracking record for each foreground pixel includes an array of size K for the horizontal direction and an array of size K for the vertical direction. The overall space complexity for the SPPT in addition to the space for storing the image is: 
SP P T (N, K)=2 . N . K (4) 
The SPPT trades the space complexity for less compu.tational steps. The PPT is an exponential time algo.rithm with linear space complexity while the SPPT is a linear time algorithm with linear space complexity. 
Once we have the SPPT result, we build a parallel-pattern histogram  the number of parallel-pattern pixel as the X-axis and the road width as the Y-axis. We then identify the road width by analyzing the his.togram to detect peaks in the histogram [Zack et al., 1977]. The detailed algorithm can be found in [Chiang et al., 2008]. 
4 Step Two: Road Intersection Detection 
In this section, we describe our techniques for extracting accurate road geometry around road intersections ( road-intersection templates) to then generate accurate road vector data in a later step. A road-intersection template represents the road geometry around a road intersection, which is the position of a road intersection, the orientations of the roads intersecting at the inter.section, and the connectivity of the road intersection.  13 shows the overall approach to automatically extract accurate road-intersection templates. This pa.per builds on our previous work [Chiang et al., 2008], which focuses on extracting positions of road intersec.tions. 
4.1 Generating Road-Intersection Blobs to Label Distorted Lines 
Since the thinning operator produces distorted road ge.ometry near the road intersections and the road width determines the extent of the distortion, we can utilize the extracted road intersections and the road width to label the locations of the potential distorted lines. We ﬁrst generate a blob image with the detected road-intersection points labeled as individual foreground pix.els. Then, we apply the dilation operator to grow a blob for each of the road-intersection points  the road width as the number of iterations. For example,  14(a) shows an example map and  14(b) shows the blob image after we apply the dilation op.erator where the size of each blob is large enough to cover the road area of each road intersection in the orig.inal map. Finally, we overlap the blob image with the thinned-line image shown in  14(c) to label the extent of the potential distorted lines as show in Fig.ure 14(d). 

Fig. 13 The overall approach to extract the road-intersection templates from raster map 
4.2 Identifying and Tracing Road-Line Candidates 
To extract accurate road vector data around the in.tersections, we use the labeled image shown in Fig.ure 14(d) to detect possible road lines intersecting at each road intersection ( road-line candidates) and trace the thinned-line pixel to compute the line ori.entations. We ﬁrst identify the contact points between each blob and the thinned-lines by detecting the thinned-line pixel that have any neighboring pixel labeled by the gray boxes. These contact points indicate the start.ing points of a road-line candidate associated with the blobs. In the example shown in  14(d), the road intersection in the second top-left blob has three road-line candidates starting from the contact points that are on the top, right, and bottom of the blob. 
Once we have the contact points, to detect the road-line candidates, we present the Limited Flood-Fill al.gorithm to trace the thinned-lines from their contact points.  15 shows the pseudo-code for the Limited Flood-Fill algorithm. The Limited Flood-Fill algorithm ﬁrst labels a contact point as visited and then checks the eight neighboring pixel of the contact point to ﬁnd unvisited thinned-line pixel. If one of the eight neigh.boring pixel is not labeled as visited nor is labeled as a potential distorted line pixel, the neighboring pixel is 


(a) An example raster map (b) Road-intersection blobs 

(c) Thinned road lines (d) Labeling distortion areas 
Fig. 14 Generating a blob image to label the distorted lines 
set as the next visit point for the Limited Flood-Fill algorithm to process. 
When the Limited Flood-Fill algorithm processes a new pixel, it records the position of the pixel to later compute the road orientation. Since it is very unlikely the road lines near an intersection are signiﬁcantly curved, to trace only straight lines, we limit the number of pixel that the Limited Flood-Fill algorithm can trace from each contact point  a parameter called Max-LinePixel. The Limited Flood-Fill algorithm counts the number of pixel that it has visited and stops when the counter is larger than the MaxLinePixel variable. 
A smaller value for the MaxLinePixel variable pre.vents the Limited Flood-Fill algorithm from tracing curved lines. However, a very small value for the Max-LinePixel variable does not provide enough pixel for the Limited Flood-Fill algorithm to compute the road orientations. For example, if we set the MaxLinePixel variable to 1 pixel, there will be only eight possible orientations of the traced lines, which is not practical. In our approach, we empirically use 5 pixel for the MaxLinePixel variable to reduce the chance of tracing curved lines while still having enough pixel to generate the road orientations. As shown in  16, instead of tracing the whole curve starting from the two contact points ( the one on the right and the one on the bottom), we utilize the MaxLinePixel to ensure that the Limited Flood-Fill algorithm traces only a small portion of the thinned-lines near the contact points. 
After the Limited Flood-Fill algorithm processes ev.ery line from each contact point and records the posi.tions of the line pixel, we utilize the Least-Squares Fit.ting algorithm to ﬁnd the linear functions of the lines. Assuming a linear function L for a set of line pixel traced by the Limited Flood-Fill algorithm, by minimiz.ing the sum of the squares of the vertical o.sets between the line pixel and the line L, the Least-Squares Fitting algorithm ﬁnds the straight line L that most represents the traced line pixel. The computed line functions are then used in the next step of updating road-intersection templates to identify actual intersecting road lines and reﬁne the positions of the road intersections. 
4.3 Updating Road-Intersection Templates 
There are three possible intersecting cases for the road-line candidates of one intersection as shown in Fig.ure 17, where: the left images of the three cases are the original map; the middle images are the thinned lines with the locations of the potential distorted lines labeled by the blob images; and the right images are the traced line functions ( the line functions computed  the Least-Squares Fitting algorithm) drawn on a Cartesian coordinate plane. 
The top row of  17 shows Case One where all the road-line candidates intersect at one point. The middle row shows Case Two where the road-line can.didates intersect at multiple points and the intersect.ing points are within a distance threshold to the ini.tially detected road-intersection position. The bottom 


Fig. 17 The three intersecting cases for updating road-intersection templates 
row shows Case Three where the road-line candidates intersect at multiple points and some of the intersecting points are not near the initially detected road-intersection position. 
For Case One, we adjust the position of the road intersection to the intersecting point of the road-line candidates. We keep all road-intersection candidates as the intersecting roads of this road-intersection tem.plate. The road orientations of this road template are 0 degrees, 90 degrees, and 270 degrees, respectively. 
For Case Two,  18 shows a detailed view where: the solid red dot is the initially detected road-intersection 


Fig. 18 Case Two: adjusting the road-intersection position with.out outliers 
position; the green, blue, red, and orange lines are the road-line candidates; the solid black dots are the can.didates’ intersecting points; and the semi-transparent red circle implies a local area with radius as the de.tected road width. Since the extent of the distortion de.pends on the road width, the positional o.set between any intersecting point of the road-line candidates and the initially detected road-intersection position should not be larger than the road width. Therefore, for case two, since every intersecting point of the road-line can.didates are in the semi-transparent red circle, we ad.just the position of the road-intersection template to the centroid of the intersecting points of all road-line candidates. We keep all road-intersection candidates as the intersecting roads of this road-intersection tem.plate. The road orientations of this road template are 80 degrees, 172 degrees, 265 degrees, and 355 degrees, respectively. 
For Case Three, when two road intersections are very close to each other, the road geometry between them is totally distorted as shown in  19. In this case, the blobs of the two road intersections merge into one big blob as shown in  19(d), and we asso.ciate both road intersections with the four thinned-lines linked to this blob.  20 shows a detailed view of Case Three. Since the two intersecting points where the dashed road-line candidate intersects with two other road-intersection candidates are more than a road width away from the initially detected road-intersection position, we discard the dashed road-line candidate. We use the centroid of the remaining two in.tersecting points as the position of the road-intersection template. Since we discard the dashed road-line can.didate, the connectivity of this road-intersection tem.plate is three and the road orientations are 60 degrees, 150 degrees, and 240 degrees, respectively. Case Three shows how the blob image helps to extract correct road 


(a) 
An example of road (b) Thickened lines segments 

(c) 
Distorted centerlines (d) Merged blobs Fig. 19 Merged nearby blobs 




Fig. 20 Case Three: adjusting the road-intersection position with outliers 
orientations even when an intersecting road line is to.tally distorted by the thinning operator. This case is not limited to three intersecting roads. Our approach holds when the distorted road line has the same orientation as the lines outside the distortion area. For example, in Case Three shown in  17, the distorted line is part of a straight line that goes throughout the intersection so it has the same orientation as the 240-degree line. In addition, the intersecting road lines need to have a similar road width because the road width is used to determine the outlier of the road-line candidates. 
 21 shows example results of the accurately extracted road-intersection templates and the results of  the thinning operator only. By utilizing the knowledge of the road width and road format, we auto.matically detect and correct the distorted lines around road intersections caused by the thinning operator and generate accurate road-intersection templates. Since this 
(a) 
 the thinning operator only 

(b) 
Accurate road-intersection templates 




Fig. 21 Example results compared to  the thinning opera.tor only 
approach is based on the heuristic that the road lines near an intersection are straight within a short distance smaller than the MaxLinePixel parameter, for signiﬁ.cantly curved road lines around the road intersections ( roads that are curved and shorter than the Max-LinePixel parameter), the traced line functions would not be accurate. 
5 Step Three: Road Vectorization 
In this section, we describe our techniques for vectoriz.ing the extracted road geometry  the road-intersection templates. For large raster map, instead of processing the entire map at once, we ﬁrst divide the map into 2000x2000-pixel tiles with overlapping areas on their connected borders and extract the road vector data for each tile. Then we combine the road vector data from each tile and generate the road vector data for the en.tire map. 
5.1 Road Vectorization  Road-Intersection Templates 
With the knowledge of potential distorted areas and the accurate positions of the road intersections as shown in s 22(a) and 22(b), we start to trace the road pix.els in the thinned-line image to generate the road vec.tor data. The thinned-line image contains three types of pixel: the non-distorted road pixel, distorted road pix.els, and background pixel.  22(a) shows the three types of pixel, which are the black pixel not covered 




(c) 
Straight-line patterns 	(d) Extracted road vector data 

Fig. 22 Extracting road vector data from an example map 
by the gray boxes, black pixel in the gray boxes, and white pixel, respectively. We create a list of connecting nodes (CNs) of the road vector data. A CN is a point where two lines meet at di.erent angles. We ﬁrst add the detected road intersections into the CN list. Then, we identify the CNs among the non-distorted road pix.els  a 3x3-pixel window to check if the pixel has any of the straight-line patterns shown in  22(c). We add the pixel to the CN list if we do not detect a straight-line pattern since the road pixel is not on a straight line. 
To determine the connectivity between the CNs, we developed an eight-connectivity ﬂood-ﬁll algorithm called the Road-Tracer to trace the road pixel. Fig.ure 23 shows the pseudo-code of the Road-Tracer. Note that the Road-Tracer algorithm and the Limited Flood-Fill algorithm in  15 are both derived from the traditional image processing ﬂood-ﬁll algorithm, which is used to determine the pixel connectivity or to ﬁll up connected areas. The di.erences between the the Road-Tracer algorithm and the Limited Flood-Fill algorithm in this paper are their stopping criteria. 
The Road-Tracer algorithm starts from a CN, trav.els through the road pixel (both non-distorted and dis.torted ones), and stops at another CN. Finally, for the CNs that are road intersections, we use the previously updated road intersection positions as the CNs’ posi.tions. The CN list and their connectivity are the re.sults of our extracted road vector data.  22(d) shows the extracted road vector data. The road vector data around the road intersections are accurate since we do not generate any CN  the distorted lines except the road intersections ( our algorithm does not record the geometry of the distorted lines) and the intersection positions are updated  the accurate road orientations. 
5.2 Divide-and-Conquer Extraction of Road Vector Data 
We divide a raster map into overlapping tiles and pro.cess each tile for extracting its road vector data. Fig.ure 24 shows an input scanned map and we divide the desired map region into four overlapping tiles. After we process all the tiles, we combine the extracted road vec.tor data from each tile as one set of road vector data for the entire raster map. 
For the extracted road vector data of each tile, we cut the vector data at the center of the overlapping areas as the dashed lines shown in  24, and we discard the vector data located in the areas between the dashed line and the tile borders. This is because the extracted road vector data near the image borders are usually inaccurate from  the image processing operators (e.g., the morphological operators). 
We merge the road vector data from two neighbor.ing tiles by matching the intersections of the dashed lines and the road lines ( the cutting points) of the two neighboring sets of road vector data. For exam.ple,  24(b) shows two sets road vector data from two neighboring tiles. We ﬁrst generate a set of cut.ting points for the left set of road vector data  the intersections of the vertical dashed line and the road lines of the left tile’s road vector data. Then, we gen.erate the set of cuttings points for the right set of road vector data. Finally, we merge the two sets of road vec.tor data by connecting two road lines in the two tiles ending at the cutting points of the same location as shown in  24(c) where each of the horizontal ar.rows point to a matched pair of cutting points shown as the cross marks.  24(d) shows the merged road vector data. 
We ﬁrst merge the road vector data of tiles on the same row from left to right and then we merge the integrated vector data of each row into the ﬁnal re.sults from top to bottom. Note that the divide-and.conquer approach does not reduce the overall computa.tional complexity, but introduces additional computa.tional overhead ( more pixel need to be processed). However, by dividing the input image into tiles that can be processed independently, our road vectorization pro.cess can scale to arbitrarily large images as long as we can divide the input image into smaller regions. More.over, since each tile is processed independently, the road vectorization process can take the advantage of multi-core or multi-processor computers to process the tiles in parallel. 

6 Experiments 
In this section, we report on our experiments on the ex.traction of road vector data from heterogeneous raster map  the techniques described in this paper. We have implemented our overall approach as two com.ponents in a system called Strabo. The ﬁrst compo.nent, called Road Layer Extraction, is the supervised road-pixel-extraction technique in Section 3 (the ﬁrst step, road geometry extraction). This component takes a raster map as input and extracts the road layer. The second component, called Road Layer Vectorization, in.cludes the remaining techniques in Section 3 and the road intersection detection and road vectorization tech.niques in Sections 4 and 5. This component takes the extracted road layer as input and generates the road vector data. 
We tested Strabo on 40 map from 11 sources. Ta.ble 1 shows the information of the test map and their abbreviations used in this section.12 The ITM, GECKO, GIZI map cover the city of Baghdad, Iraq and were scanned in 350 DPI. The UNIraq map covers the city of Samawah, Iraq, which is from the United Nations Assistance Mission for Iraq website13 and provides no information of scan resolution. The UNAf map covers Afghanistan and is from the United Nations Assistance Mission in Afghanistan website.14 The Afghanistan map 
12 The detailed information for obtaining the test map and the ground truth can be found on: http://www.isi.edu/ integration/data/map/prj_map_extract_data.html 
13 http://www.uniraq.org 
14 http://unama.unmissions.org/ 

Map Source (map count, abbr.)  Map Type  Dimension (pixel)  
International Travel map (6, ITM)  Scanned  4000x3636  
Gecko map (3, GECKO)  Scanned  5264x1923  
Gizi Map (4, GIZI)  Scanned  3344x3608  
UN Iraq (9, UNIraq)  Scanned  4000x3636  
Rand McNally (4, RM)  Computer  2084x2756  
UN Afghanistan (4, UNAfg)  Computer  3300x2550  
Google map (2, Google)  Computer  800x550  
Live map (2, Live)  Computer  800x550  
OpenStreetMap (2, OSM)  Computer  800x550  
MapQuest map (2, MapQuest)  Computer  800x550  
Yahoo map (2, Yahoo)  Computer  800x550  

Table 1 Test map 
shows the main and secondary roads, cities, political boundaries, airports, and railroads of the nation. The RM map covers the city of St. Louis, Missouri and is from Rand McNally.15 The Google, Live, OSM, Map-Quest, Yahoo map cover one area in Los Angeles, Cal.ifornia and one area in St. Louis, Missouri, which are from Google map, Microsoft Live map, OpenStreet-Map, MapQuest map, and Yahoo map, respectively.  25 shows examples of the test map, where the scanned map show poor image quality, especially the Gecko and Gizi map with the shadows caused by the fold lines. 
6.1 Experimental Setup 
This section presents the experimental setup for the road layer extraction and vectorization components in Strabo. 
15 http://www.randmcnally.com/ (a) ITM map (b) GECKO map 






(c) 
GIZI map (d) UNIraq map 

(e) 
RM map (f) UNAfg map 




Fig. 25 Examples of the test map 
Road Layer Extraction We ﬁrst used Strabo to generate a road layer for each test map automatically. This automatic technique of Strabo is implemented based on our previous grayscale-histogram analysis technique [Chi.ang et al., 2008]. Then we manually checked the road layer to determine if user intervention was required. 
Strabo successfully extracted the road layer from the last ﬁve sources shown in Table 1 and did not extract correct road layers for the other six sources. Among the six sources (ITM, GECKO, GIZI, UNIraq, RM, and UNAfg), four sources correspond to scanned map (ITM, GECKO, GIZI, and UNIraq) since the grayscale-histogram analysis technique could not sep.arate the foreground pixel from the background. The other two map sources contain non-road linear features, which are drawn  the same single-line format as the roads, and hence the automatically extracted road layers contain these linear features. To achieve the best results for the six sources, we utilized the supervised technique presented in Section 3.1 to extract the road layers. 
The supervised technique ﬁrst quantized the map from the four scanned-map sources (ITM, GECKO, GIZI, and UNIraq) to generate quantized images in various quantization levels (the quantization levels have the number of colors as 32, 64, 128, and 256, respectively). We did not apply the color segmentation algorithms on computer-generated map (test map from RM and UNAfg) before user labeling. This is because the computer-generated map contain a smaller number of colors. The UNAfg map has 90 unique colors and there is only one color representing both the major and secondary roads in the map. The RM map has 20 unique colors with 5 colors representing roads. The user starts  Strabo for the user-labeling task with the quantized image of the highest quantization level ( the quantized image that has the smallest number of colors). If the user can.not distinguish the road pixel from other map features (e.g., background) in the quantized image, the user se.lects a quantized image containing more colors (a lower quantization level) for user labeling. 
For comparison, we tested the 40 test map  R2V from Able Software. R2V allows the user to use one set of color thresholds to extract the road pixel from the map for vectorization. For raster map that require more than one set of color thresholds (all of our test map except the UNAfg map require more than one set of color thresholds), the user has to manually specify sample pixel for each of the road colors, which requires signiﬁcant e.ort. Therefore, for the raster map that require more than one set of color thresholds to extract their road pixel, we used the road layers extracted from Strabo (which are the same set of road layers used to test Strabo’s road vectorization function) without  R2V’s manual pre-processing and post-processing. The UNAfg map requires only one set of color thresholds to extract the road pixel  R2V and both R2V and Strabo generated the same road layer for the UNAfg map. 
Road Layer Vectorization Once the road layers were extracted, the second component of Strabo then processed the road layers and automatically generated the road vector data. For comparison, we utilized “Auto Vectorize” function in R2V to process the same set of road layers for generating the road vector data auto.matically. 
6.2 Evaluation Criteria 
This section describes the evaluation criteria for the road layer extraction and vectorization components in Strabo. 
Road Layer Extraction To evaluate the road.layer-extraction component, we report the number of user labels that were required for extracting road pix.els from each map source  Strabo. 
Road Layer Vectorization For evaluating the ex.tracted road vector data from the road-layer-vectorization component, we report the accuracy of the extraction re.sults  the road extraction metrics proposed by Heipke et al. [1997], which include the completeness, correct.ness, quality, redundancy, and the root-mean-square (RMS) di.erence. We had a third-party to manually draw the centerline of every road line in the map as the ground truth. 
The completeness is the length of true positives di.vided by the sum of the lengths of true positives and false negatives, and the optimum is 100%. The cor.rectness is the length of true positives divided by the sum of the lengths of true positives and false positives, and the optimum is 100%. The quality is a combina.tion metric of the completeness and correctness, which is the length of true positives divided by the sum of the lengths of true positives, false positives, and false negatives, and the optimum is 100%. The redundancy is the length of matched extraction minus the length of matched reference. The redundancy shows the per.centage of the matched ground truth that is redundant ( more than one true positive line matched to one ground-truth line), and the optimum is 0. The redun.dancy does not depend on the number of line segments in the matched extraction line or the matched reference. The RMS di.erence is the average distance between the extracted lines and the ground truth, which represents the geometrical accuracy of the extracted road vector data. 
To identify the length of the true positives, false neg.atives, and matched ground truth, Heipke et al. [1997] suggest  a bu.er width of half of the road width in the test data so that a correctly extracted road seg.ment is in between the road edges as shown in  26. In our test map, the roads range from 5 to 12 pixel wide. We used a bu.er width of 3 pixel, which means a correctly extracted line is no farther than 3 pixel from the road centerlines. For example, to calculate the length of the true positives, we ﬁrst drew the extracted road vector data  1-pixel-width lines as the blue lines shown in the top of  26 ( the length of a road segment is approximately the number of pixel of the drawn road line). Then we drew the ground truth lines (the reference roads)  the width as the bu.er width on top of the blue lines as the red box shown in in  26. The number of blue pixel outside the red box is the length of false positives and the total number of blue pixel minus the length of false positives is the length of true positives. More details on calculating the metrics can be found in [Heipke et al., 1997]. 

6.3 Experimental Results 
This section presents the experimental results of the road layer extraction and vectorization components in Strabo. 
Road Layer Extraction Table 2 shows the num.bers of colors in the images used for user labeling and the numbers of user labels used for extracting the road pixel. Strabo did not generate quantized map for the RM and UNAfg map ( no values for their quan.tized map colors in Table 2) because they contain only a small number of colors. 
For all the scanned map, only 1 to 9 labels were needed for Strabo to extract the road pixel. The num.ber of user labels varied from 1 to 9 because of the varying complexity of map content and the number of road colors of the tested map sources. Strabo’s user labeling function is an interactive process. During the road-layer-extraction experiments, a user ﬁrst selected one or more labels for a map source and then instructed Strabo to show the layer extraction results  the se.lected labels. If not all of the road lines were extracted, the user would provide more labels and then re-examine the results. This interactive labeling process continued until all of the road lines were extracted. Since the user label does not have to contain only road pixel, the user does not have to carefully avoid including non-road pix.els in the label and hence does not take a long time to decide on a label ( usually less than 30 seconds). 
In contrast to R2V, which requires manually pro.viding samples of each road color, Strabo’s interactive strategy provides a much easier-to-use approach for ex.tracting road layers from raster map. Note that if the color quantization failed to group the road colors into groups because of poor map quality, as found in some historical map, this interactive strategy would not work well. In this case, a more advanced color segmentation technique could be used to produce a quantized map for user labeling [Chiang et al., 2011]. 
Road Layer Vectorization Table 3 shows the nu.meric results from  Strabo and R2V to extract road vector data from the 16 test map. The average completeness, correctness, quality, redundancy, and re.dundancy of Strabo and R2V are shown in the bot.tom rows of Table 3. Strabo produced more accurate road vector data with a smaller RMS. We emphasize the numbers where R2V generated a better result than Strabo. R2V produced better completeness numbers for ﬁve test map, but this was because R2V gener.ated highly redundant lines, while Strabo eliminated small branches, such as the highway ramps shown in  27. 
R2V could achieve better results if we tuned R2V with manually speciﬁed pre-processing and post-process.ing functions. To demonstrate the pre-processing and post-processing functions in R2V for improving the vec.torization result on the road layer shown in  27(b), we ﬁrst manually resized the image to a quarter of the original size for reducing the thickness of the line ar.eas. Next, we applied the de-speckle function in R2V to remove noise objects. We then used the image edit.ing function in R2V to manually draw lines to ﬁll up the gaps between broken lines and holes; in this par.ticular example, we manually drew 14 areas and the black pixel in  28 shows the edited result. Fi.nally, we applied the automatic vectorization function with spline smoothing to generate the results as the road lines shown in  28. Note that the results are signiﬁcantly improved after manual processing, but the road geometry near the intersections is not accurate compared to our results shown in  27(c) due to the fact that Strabo automatically detects and corrects distorted lines near road intersections. 
One limitation of Strabo’s automatic vectorization process is that the process to generate road geometry relies on the width of the majority of roads. This can be seen in  27 where Strabo eliminated small branches because Strabo detected the road width as the width of the majority of roads (in  27, the majority roads are the white roads) and used the de.tected road width to set up the parameters for gener.ating the road geometry automatically. As a result, in the examples in  27, the number of iterations of the erosion operator was larger than the width of the small branches so that the branches were eliminated after applying the erosion operator. 
Map Source  Original Map Colors  Quantized Map Colors  User Labels  
ITM  779,338  64  9  
GECKO  441,767  128  5  
GIZI  599,470  64  9  
UNIraq  217,790  64  5  
RM  20  N/A  5  
UNAfg  90  N/A  1  

Table 2 The number of colors in the image for user labeling of each test map and the number of user labels for extracting the road pixel 
Map Source  Completeness  Correctness  Quality  Redundancy  RMS  
ITM (Strabo)  90.02%  93.95%  85.08%  0.85%  3.59  
ITM (R2V)  96.00%  66.91%  65.09%  117.33%  13.68  
GECKO (Strabo)  93.75%  94.75%  89.12%  0.61%  2.95  
GECKO (R2V)  96.52%  76.44%  74.39%  52.64%  8.27  
GIZI (Strabo)  92.90%  96.18%  89.59%  0.00%  2.46  
GIZI (R2V)  93.43%  95.03%  89.08%  39.42%  11.17  
UNIraq (Strabo)  88.31%  96.01%  85.19%  0.00%  6.94  
UNIraq (R2V)  94.92%  78.38%  75.22%  18.82%  5.19  
RM (Strabo)  96.03%  84.72%  81.85%  1.60%  2.79  
RM (R2V)  92.74%  68.89%  65.36%  33.56%  16.03  
UNAfg (Strabo)  86.02%  99.92%  85.96%  0.00%  3.68  
UNAfg (R2V)  88.26%  99.92%  88.20%  12.36%  3.98  
Google (Strabo)  99.62%  99.87%  99.49%  0.00%  0.81  
Google (R2V)  83.45%  81.93%  70.41%  18.78%  19.16  
Live (Strabo)  99.47%  98.31%  97.79%  0.00%  8.08  
Live (R2V)  83.42%  71.36%  62.69%  29.25%  23.85  
OSM (Strabo)  99.81%  100.00%  99.81%  0.00%  0.76  
OSM (R2V)  90.47%  93.71%  85.79%  6.82%  10.84  
MapQuest (Strabo)  99.85%  100.00%  100.00%  0.00%  0.73  
MapQuest (R2V)  92.01%  93.41%  87.149%  7.52%  6.72  
Yahoo (Strabo)  99.97%  99.97%  99.94%  0.00%  0.69  
Yahoo (R2V)  86.10%  77.17%  68.62%  103.29%  26.49  
Avg. (Strabo)  95.07%  96.70%  92.15%  0.28%  3.05  
Avg. (R2V)  90.66%  82.10%  75.64%  39.98%  13.22  

Table 3 Numeric results of the extracted road vector data (3-pixel-wide bu.er)  Strabo and R2V 
s 29 to 35 show some example results. Note that the geometry of the extracted road vector data is very close to the road centerlines for both straight and curved roads, especially the computer-generated map from the web-mapping service providers. 
For the lower than average completeness numbers in the ITM, GECKO, GIZI, and UNAfg map, some broken lines were not reconnected since the gaps were larger than the iterations of the dilation operator af.ter the non-road overlapping features were removed, such as the gaps in the UNAfg, GECKO, and GIZI map shown in s 29, 30, and 31. The broken lines could be reconnected with post-processing on the road vector data since the gaps are now smaller than they were in the extracted road layers resulting from the dilation operator. One post-processing example could be connecting extreme points that are within the dis.tance of a user-speciﬁed threshold in the road vector data. This post-processing step could potentially in.crease the completeness of the road vector data results. For the lower than average completeness numbers in the scanned UNIraq map, some of the road lines as shown in  32 are dashed lines and the ground truth were drawn as solid lines. 
For the text labels in the test map, except the RM map, the map contained text labels drawn in a di.er.ent color than the roads so that the text pixel were re.moved during the extraction of road layers ( Strabo identiﬁed the colors that represent roads and used the road color to extract the road layers). The overlapping text was also removed and hence the extracted road layers were broken as shown in the examples of the ITM, UNAfg, Gecko, and GIZI map in  29 to  31. 
For the RM map,  33(b) shows the extracted road pixel  the supervised function of Strabo. Many characters were extracted since they share the same color as the black lines. Although we removed 

(a) ITM map (portion) (b) Extracted road pixel 

(c) Strabo results (d) R2V results 
Fig. 27 Example results  Strabo and R2V of a cropped area from the ITM map 

Fig. 28 R2V results with manual image editing (red lines are the extracted road vector data and black pixel are the edited road area 
the majority of the characters automatically  the text/graphics separation and connected component anal.ysis techniques [Cao and Tan, 2002; Chiang et al., 2008] as described in Section 3.3, some of the characters were miss-identiﬁed as road lines since they touch the road lines and the connected-component analysis approach we used could not remove this type of false positive, which resulted in lower completeness, correctness, and quality. For the Google, Live, OSM, MapQuest, and Ya.hoo map, because of the good image quality, Strabo automatically separated the foreground pixel from the background and the foreground pixel contain both the text and road pixel as the example shown in  35. Since the text labels in these map were placed on top of the road lines, the extracted road layers show accu.rately connected road lines. 
The ITM, GECKO, GIZI, UNIraq, and RM map had lower than average correctness numbers since some of the non-road features were also extracted  the identiﬁed road colors and those parts contributed to false positive road vector data.  29(b) shows a portion of the ITM map where the runways are rep.resented  the same color as the white roads and hence were extracted as road pixel.  32 shows a portion of the UNIraq map where some of the building pixel were extracted since they share the same colors as the road shadows.  33 shows two grid lines in pink on the left and right portions of the RM map were also extracted since they have the same color as the major road shown at the center of the map. Including a user validation step after the road pixel were extracted could further reduce this type of false positive resulting in higher correctness numbers. 
Strabo’s redundancy numbers are generally low since we correctly identify the centerlines for extracting the road vector data. The average RMS di.erences are un.der 3 pixel, which shows that the thinning operator and our approach to correct the distortion result in good quality road geometry. For example, in the re.sults shown in  34 and  35, although the extracted road lines are thick, Strabo extracted accu.rate road vector data around the intersections. The high redundancy numbers of R2V resulted from no manual pre-processing before R2V’s automatic function to ex.tract the centerlines of the roads, and the automatic function is sensitive to wide road lines. 
6.3.1 Computation Time 
We built Strabo  Microsoft Visual Studio 2008 running on a Microsoft Windows 2003 Server powered by a 3.2 GHz Intel Pentium 4 CPU with 4GB RAM. The average processing time for the entire process of vectorizing the road pixel for a 800x550-pixel map was 5 seconds, for a 2084x2756-pixel map was 2 minutes, and for a 4000x3636-pixel map was 3.5 minutes. The dominant factors of the computation time are the image size, the number of road pixel in the raster map, and the number of road intersections in the road layer. The implementation was not fully optimized and improve.ments could still be made to speed up the processes, such as multi-threading on processing map tiles of an input map. 

(a) ITM map (portion) (b) Road pixel of (a) 
(a) GECKO map (portion) (b) Road pixel of (a) 

pgflastimage 
(d) UNAfg map (portion) 


(e) Road pixel of (d) (f) Road vector data of (d) 
Fig. 29 Examples of the road vectorization results on the ITM and UNAfg map 
7 Conclusion and Future Work 
We present a general approach to extract accurate road vector data from heterogeneous raster map with mini.mal user input. This approach handles raster map with poor image quality  a semi-automatic technique. We show that our approach extracts accurate road vec.tor data from 40 raster map from 11 sources with varying color usages and image quality. In the future, we plan to extend our approach to include automatic post-processing on the road vector data. For example, without knowing the real-world lengths of the extracted road lines, we cannot apply heuristics for post-processing on the extracted road vector data, such as removing road lines that are shorter than 1 meter. With the ex.tracted road vector data, we plan to utilize map conﬂa.tion techniques, such as the one from Chen et al. [2008], to identify the geocoordinates of the road vector data and then discover the actual lengths of the extracted road lines. We can then utilize real-world heuristics, such as thresholds on the road length and road turn.


(c) 
Road vector data of (a) (d) GECKO map (portion) 

(e) 
Road pixel of (d) (f) Road vector data of (d) 




Fig. 30 Examples of the road vectorization results on the GECKO map 
ing angles, to apply automatic post-processing on the extracted road vector data to improve the results. 
Acknowledgements 
We thank Phyllis O’Neil for her excellent proofreading and editing work. We also thank Weili Chiang for man.ually digitizing the test map to generate the ground truth. 
This research is based upon work supported in part by the University of Southern California under the Viterbi School of Engineering Doctoral Fellowship. 
References 
Ballard, D. H. (1981). Generalizing the hough trans.
form to detect arbitrary shapes. Pattern Recognition, 
13(2):111–122. 

(a) GIZI map (portion) (b) Road pixel of (a) 

(c) 
Road vector data of (a) (d) GIZI map (portion) 

(e) 
Road pixel of (d) (f) Road vector data of (d) 



Fig. 31 Examples of the road vectorization results on the GIZI map 
D. Bin and W. K. Cheong. A system for automatic ex.traction of road network from map. In Proceedings of the IEEE International Joint Symposia on Intelli.gence and Systems, pages 359–366, 1998. 
R. Cao and C. L. Tan. 	Text/graphics separation in map. In Proceedings of the Fourth GREC, pages 167–177, 2002. 
Cheng, H., Jiang, X., Sun, Y., and Wang, J. (2001). Color image segmentation: advances and prospects. Pattern Recognition, 34(12):2259 – 2281. 
C.-C. Chen, C. A. Knoblock, and C. Shahabi. 	Auto.matically and accurately conﬂating raster map with orthoimagery. GeoInformatica, 12(3):377–410, 2008. 
Y. Chen, R. Wang, and J. Qian. Extracting contour lines from common-conditioned topographic map. IEEE Transactions on Geoscience and Remote Sens.ing, 44(4):1048–1057, 2006. 

(a) 
UNIraq map (portion) 

(b)
Roadpixelof(a)

(c)
Roadvectordataof(a)










(a) MapQuest map 



(c) 
Road vector data of (a) 


Fig. 34 Examples of the road vectorization results on a MapQuest map 
Y.-Y. Chiang and C. A. Knoblock. Automatic extrac.tion of road intersection position, connectivity, and orientations from raster map. In Proceedings of the 16th ACM GIS, pages 1–10, 2008. 
Y.-Y. Chiang and C. A. Knoblock. A method for au.tomatically extracting road layers from raster map. In Proceedings of the Tenth ICDAR, pages 838–842, 2009a. 
Y.-Y. Chiang and C. A. Knoblock. Extracting road vector data from raster map. Selected Papers of the Eighth GREC, LNCS, 6020:93–105, 2009b. 
Y.-Y. Chiang, C. A. Knoblock, C. Shahabi, and C.-C. Chen. Automatic and accurate extraction of road intersections from raster map. GeoInformatica, 13 (2):121–157, 2008. 
Y.-Y. Chiang, S. Leyl, and C. A. Knoblock. Integrat.ing Color Image Segmentation and User Labeling for 
(a) 
OSM map 

(b)
Roadpixelof(a)

(c) 
Road vector data of (a) 





Fig. 35 Examples of the road vectorization results on a OSM map 
E.cient and Robust Graphics Recognition from His.torical map. The Ninth IAPR International Work.shop on Graphics RECognition, 2011b. 
D. Comaniciu and P. 	Meer. Mean shift: a robust ap.proach toward feature space analysis. IEEE TPAMI, 24(5):603–619, 2002. 
Duda, R. O. and Hart, P. E. (1972). Use of the hough transformation to detect lines and curves in pictures. Communications of the ACM, 15:11–15. 
A. Habib, R. Uebbing, and A. Asmamaw. 	Automatic extraction of road intersections from raster map. Project Report, Center for Mapping, The Ohio State University, 1999. 
P. Heckbert. Color image quantization for frame bu.er display. SIGGRAPH, 16(3):297–307, 1982. 
C. Heipke, H. Mayer, C. Wiedemann, and O. Jamet. Evaluation of automatic road extraction. In Interna.
tional Archives of Photogrammetry and Remote Sens.ing, pages 47–56, 1997. 
T. C. Henderson, T. Linton, S. Potupchik, and A. Os.tanin. Automatic segmentation of semantic classes in raster map images. In Proceedings of the Eighth IAPR International Workshop on Graphics Recogni.tion, pages 253–262. 
W. 	Itonaga, I. Matsuda, N. Yoneyama, and S. Ito. Automatic extraction of road networks from map images. Electronics and Communications in Japan (Part II: Electronics), 86(4):62–72, 2003. 
A. Khotanzad and E. Zink. Contour line and geographic feature extraction from USGS color topographical paper map. IEEE TPAMI, 25(1):18–31, 2003. 
V. Lacroix. 	Automatic palette identiﬁcation of col.ored graphics. In Graphics Recognition: Achieve.ments, Challenges, and Evolution, Selected Papers of the 8th International Workshop on Graphics Recog.nition (GREC), Lecture Notes in Computer Science, 6020, pages 95–100. Springer, New York. 
S. Leyk and R. Boesch. Colors of the past: color image segmentation in historical topographic map based on homogeneity. GeoInformatica, 14(1):1–21. 
L. Li, G. Nagy, A. Samal, S. C. Seth, and Y. Xu. 	In.tegrated text and line-art extraction from a topo.graphic map. IJDAR, 2(4):177–185, 2000. 
Lloyd, S. P. (1982). Least squares quantization in pcm. IEEE Transactions on Information Theory, 28:129– 
137. 
W. K. Pratt. Digital Image Processing: PIKS Scientiﬁc Inside. Wiley-Interscience, 3rd edition, 2001. 
S. Salvatore and P. Guitton. 	Contour line recognition from scanned topographic map. In Proceedings of the Winter School of Computer Graphics, 2004. 
J. Shi and C. Tomasi. Good features to track. In Pro.ceedings of the IEEE CVPR, pages 593–600, 1994. 
Tombre, K., Tabbone, S., P´elissier, L., Lamiroy, B., and Dosch, P. (2002). Text/graphics separation revisited. In Lopresti, D., Hu, J., and Kashi, R., editors, Doc.ument Analysis Systems V, volume 2423 of Lecture Notes in Computer Science, pages 615–620. Springer Berlin / Heidelberg. 
X. Wu, R. Carceroni, H. Fang, S. Zelinka, and 
A. Kirmse. Automatic alignment of large-scale aerial rasters to road-map. In Proceedings of the 15th ACM GIS, pages 1–8, 2007. 
G. Zack, W. Rogers, and S. Latt. Automatic measure.ment of sister chromatid exchange frequency. Journal of Histochemistry and Cytochemistry, 25(7):741–753, 1977. 
Generating Named Road Vector Data from 
Raster map 

Yao-Yi Chiang1 and Craig A. Knoblock2 
1 University of Southern California, 
Information Sciences Institute and Spatial Sciences Institute 
4676 Admiralty Way, Marina del Rey, CA 90292, USA 

yaoyichi@isi.edu 
2 University of Southern California, 
Department of Computer Science and Information Sciences Institute 
4676 Admiralty Way, Marina del Rey, CA 90292, USA 

knoblock@isi.edu 
Abstract. Raster map contain rich road information, such as the topol.ogy and names of roads, but this information is “locked” in images and inaccessible in a geographic information system (GIS). Previous approaches for road extraction from raster map typically handle this problem as raster-to-vector conversion and hence the extracted road vector data are line segments without the knowledge of road names and where a road starts and ends. This paper presents a technique that builds on the re.sults from our previous road vectorization and text recognition work to generate named road vector data from raster map. This technique ﬁrst segments road vectorization results  road intersections to determine the lines that represent individual roads in the map. Then the technique exploits spatial relationships between roads and recognized text labels to generate road names for individual road segments. We implemented this approach in our map processing system, called Strabo, and demonstrate that the system generates accurate named road vector data on example map with 92.83% accuracy. 
Keywords: Raster map, road vectorization, text recognition, named road vector data, map labeling 
1 Introduction 

Cartographers have been making map for centuries and road map are one of the most used map among all map types. Today we have access to a huge number of map collections in raster format from a variety of sources. For instance, the United States Geological Survey (USGS) has been mapping the United States since 1879. The USGS topographic map at various time periods cover the en.tire country and contain informative geographic features, such as contour lines, buildings, and road lines. These raster map are easily accessible compared to other geospatial data (e.g., road vector data) and present a unique opportunity for obtaining road information for the areas and time periods where and when 
Yao-Yi Chiang and Craig A. Knoblock 
road vector data do not otherwise exist. For example, we can generate named road vector data (road vector data that have a road-name attribute) from histor.ical map and build an accurate geocoder [Goldberg et al., 2009] or a gazetteer for spatiotemporal analysis of human-induced changes in the landscape. 
Generating named road vector data from raster map is challenging for a number of reasons. First, map typically contain overlapping layers of geographic features, such as roads, contour lines, and text labels. Thus, the map content is usually highly complex and presents a di.cult task for converting the road geometry in raster map to vector format. Second, map contain characters of various sizes constituting multi-oriented text labels, which cannot be recognized  classic optical character recognition (OCR) techniques. Finally, even after the road geometry is vectorized and text labels are recognized, there still exists the problem of labeling individual road lines with the recognized labels. 
This paper presents an approach to generate named road vector data from raster map while requiring minimal user e.ort.  1 shows our overall ap.proach, which integrates our previous map processing work (the interactive road vectorization [Chiang and Knoblock, 2011a] and text recognition tech.niques [Chiang, 2010; Chiang and Knoblock, 2011b]) and o.ers a new contri.bution: an automatic technique to identify individual road segments from the road vectorization results and then associate the recognized road labels with the road segments. This technique is the reverse engineering of cartographic-labeling methods [Agarwal et al., 1998; Doddi et al., 1997; Edmondson et al., 1996; Free.man, 2005]. The resulting named road vector data can be used in a geographic information system (GIS). 

Fig. 1. The overall approach for generating named road vector data from heterogeneous raster map 
The remainder of this paper is organized as follows: Section 2 describes our previous map processing work on which the techniques in this paper built, Sec.tion 3 presents this paper’s main contribution on associating road vector data with road labels, Section 4 reports on our experimental results, Section 5 dis.cusses the related work, and Section 6 presents the discussion and future work. 
Generating Named Road Vector Data from Raster map 
2 Previous Work 

This section brieﬂy reviews our previous work on text recognition [Chiang, 2010; Chiang and Knoblock, 2011b] and road vectorization [Chiang and Knoblock, 2011a] from raster map. 
2.1 Text Recognition 

In our previous work, we developed an interactive text recognition approach that requires only minimal user e.ort for processing heterogeneous raster map [Chi.ang, 2010; Chiang and Knoblock, 2011b]. This approach ﬁrst exploits a few examples of text areas for extracting text pixel and locating individual text strings.  2 shows our user interface for labeling example text areas. Fig.ure 3 shows an example map and the results where individual text strings are identiﬁed and shown in distinct colors (the color is only for explaining the idea). Once individual text strings are identiﬁed, we automatically detect the string orientations and rotate the strings to horizontal to then leverage conventional OCR software for recognizing the horizontal strings. 

Fig. 2. Our user labeling interface for text recognition from raster map 

Fig. 3. Identify individual text labels 
2.2 Road Vectorization 

In our previous work, we developed an interactive road vectorization approach that requires minimal user e.ort to handle heterogeneous raster map [Chiang 
Yao-Yi Chiang and Craig A. Knoblock 
and Knoblock, 2011a]. Similar to our text recognition approach, this road vec.torization technique exploits a few examples of road areas to extract road pixel and generate road vector data. 
To identify the road colors in a raster map for extracting the road pixel, our approach asks a user to ﬁrst select a few example areas of roads. An example area of a road is a rectangle that is centered at a road intersection or a road segment. We exploit the fact that the road pixel in an example area of roads are a portion of one or more linear objects that are near the area center to determine the colors that represent roads in a map. 
With the separated road layer ( the set of extracted road pixel), we automatically detect the road width and format ( single-line or double-line roads) and then dynamically generate parameters for applying the morphologi.cal operators ( the dilation, erosion, and thinning operators) to extract and rebuild the road geometry ( the centerline representation of the road net.work). The left image of  4 shows an example map and the middle image shows the extracted road geometry, where the road lines near the intersections are distorted as a result of applying the morphological operators on thick lines. To extract accurate road vector data around the intersections, we detect the road intersections in the road geometry and label potential distortion areas around the intersections. Finally, we trace the thinned-line pixel outside the distortion areas to reconstruct the road intersections and generate the road vector data. The right image of  4 shows the resulting road vector data where the road geometry around the intersections is accurate. 

3 Association of Road Vector Data and Road Labels 
Our text/road association algorithm includes four major components. (i) The ﬁrst component processes road vectorization results to generate individual road segments. Each road segment contains a set of line segments constituting the same road in a map. (ii) The second component assigns each road label to a road segment. (iii) For the road segments that are assigned with more than one road label, this component arranges the road labels to generate a road name  the relative positions between the assigned road labels and the road segment. (iv) Finally, the fourth component propagates the road names from road segments that have assigned road labels to the road segments that do not have assigned 
Generating Named Road Vector Data from Raster map 
labels. In addition, if a road name is broken into several parts to label a long road in the input map, the separate parts are merged into a road name. 
3.1 Determining Road Segments 

The input to our road segmentation algorithm is the road vector data gener.ated from our previous road vectorization work. The extracted road vector data contains a set of line segments, which are short, straight lines, without the knowl.edge of which line segments belong to each road segment in the map. Since road name changes commonly happen at road intersections, we use the locations of road intersections to group the input line segments into individual road segments 
– a road segment is a section of a road that is bounded by road endpoints or road intersections where more than two line segments meet.  5 shows an example input and output of our road segmentation algorithm. 

Fig. 5. Determining road segments from line segments based on road intersections 
Our road segmentation algorithm ﬁrst computes the connectivity of the end.points of every line segments in the input road vector data. If an endpoint connects to only one other endpoint, the endpoint is a road end, namely a RE (the green squares in  5). If an endpoint connects to more than two other endpoints, the endpoint is a road intersection, namely a RI (the yellow circles in  5). If an endpoint is neither a RE nor a RI, the endpoint is a connecting point, namely a CP. 
Once we have the connectivity of every endpoint of the input line segments, we iteratively process every input line segments until every line segment is as.signed to a road segment. In one iteration, our algorithm starts from an unpro.cessed line segment and we ﬁrst check the connectivity of its two endpoints. If the two endpoints are both classiﬁed as either a RE or RI, we assign the line segment as a road segment itself and then continue to process other line seg.ments. If no or only one endpoint is classiﬁed as either a RE or RI, we search for the unprocessed line segments that connect to this line segment through the endpoints that are classiﬁed as a CP. We stop when the connected line segment we found contains a RE or RI or no line segment exists that is connected to this line segment. s 6(a), 6(b), and 6(c) show an example test map, the input road vector data, and the road segmentation results. The red crosses shows the endpoints of the line segments and the endpoints of road segments in Fig.ures 6(b) and 6(c), respectively. The road segmentation results are then used 
Yao-Yi Chiang and Craig A. Knoblock 
in the next step with the recognized road labels to generate named road vector data. In an unusual case where the road name changes at non-road intersection locations, user input would be required to further separate the road segments. 




(c) Output road segments 	(d) The identiﬁed road labels in rect.angular bounding boxes 
Fig. 6. Example inputs and intermediate results for grouping road segments and de.termining the locations of road labels 
3.2 Initial Association of Road Labels and Road Segments 
Once we have the road segments, we start to assign each recognized road label to one of the road segments.  6(d) shows the test map where the rectangles show the bounding boxes of the identiﬁed road labels. The recognized road labels, together with the identiﬁed road segments, are the input to this step. 
Map labeling is a well investigated technique in both cartography [Edmond.son et al., 1996] and computer science [Agarwal et al., 1998; Doddi et al., 1997; 
Generating Named Road Vector Data from Raster map 
Freeman, 2005]. In general, to label linear features in a map, a computer pro.gram or a cartographer places the labels in parallel to the corresponding linear features. The distance between a label and the corresponding feature should be smaller than the distance between the label to any other features of the same kind in the map. Therefore, to determine the correspondence between a road label and a road segment, we ﬁrst assign every road label to the road segment that is the closest to the label and has the same orientation of the label. 
To compute the distance between a road segment and a road label, we use the mass center of the road label to represent the position of this label. We calculate the distance between the mass center to each of the line segments in a road segment and use the shortest line-segment-to-mass-center distance as the distance between the road segment and the road label. 
For a road label containing n character pixel, (xi,yi), the road label’s mass center, (Xm,Ym), is calculated as follows: 
nn i=1 xii=1 yi
Xm = ,Ym = (1) 
nn 
To determine the parallelism between a road label and a road segment, we compare the orientation of the road segment with the orientation of the road label. The orientation of each road label is determined  our text recognition algorithm [Chiang and Knoblock, 2011b], and we compute the orientation of each road segment as follows: we ﬁrst utilize the Least-Squares Fitting algorithm to ﬁnd a straight line that best ﬁts each road segment in the two dimensional space and then compute the orientation of the straight line. Assuming a linear function L for a set of points in a road segment, by minimizing the sum of the squares of the vertical o.sets between the points and the line L, the Least-Squares Fitting algorithm ﬁnds the line L that most represents the road segment. For the target line function L as: 
Y = m . X + b, (2) 
the Least-Squares Fitting algorithm works as follows: 
n xy . xy 
m = .. (3) 
nx2 . ( x)2 
and .. 
y . mx 
b = (4) 
n 
With the line function of every road segment, we then derive the orientations of all road segments by applying the inverse trigonometric function, ArcT an, to the slope (m) of the road segments’ line functions. 
Because the orientations of the road labels and road segments are not esti.mated from the same type of data format (the road labels are a group of pixel and the road segments are vectors), to determine the parallelism between a road label and a road segment, we empirically deﬁne a bu.er, B, as 10.. If the dif.ference between the orientations of a road label and a road segment is smaller than B, the road label and the road segment is determined to be in parallel. 
Yao-Yi Chiang and Craig A. Knoblock 
For curved roads, if the road label is also curved along the curvature of the roads, the estimated orientation of the road label is similar to the road orientation determined  this approach. However, in the case where straight strings are used to label curved roads, the road label would not be assigned correctly and would need manual correction. 
3.3 Arrangement of Road Labels for Generating Road Names 
We can have multiple road labels assigned to a road segment if a road name is divided into more than one label in the map as shown in  7. In this case, we need to determine the order of the assigned road labels for a road segment to then assemble the ordered road labels for generating a road name. 

Fig. 7. More than one road labels can be assigned to a road segment 
Given a road segment with multiple assigned road labels, for each of the road labels, we ﬁrst determine which side of the road segment the road label appears in the map. This is determined  the cross product between the two endpoints,(Xs,Ys) and (Xe,Ye), of the road segment and the mass center, (Xm,Ym), of the road label: 
((Xe . Xs) . (Ym . Ys)) . ((Ye . Ys) . (Xm . Xs)) (5) 
The sign of the result from the cross product indicates which side the road label appears in the map. Once we have the relative position between the assigned road labels and the road segment, we ﬁrst rotate the road labels to the horizontal direction  the label orientation and then check the relative position of the road labels and arrange the road labels as follows: 
(i) 
If two road labels appear on the same side of the road segment, we order the labels  their Xm positions. This is because in English writing, a sequence of words is read from the left (a smaller Xm) to the right (a larger Xm). 

(ii) 
If two road labels appear on di.erent sides of the road segment, we order the road labels  their Ym positions. Similarly, this is because in English writing, a sequence of words should be read from the top (a larger Ym) toward the bottom (a smaller Ym). For example, as shown in  7, in the initial assignment, the road labels “Culver” and “Wy” are both assigned to the same road segment. After we rotate both road labels to the horizontal direction, the road label, “Culver”, has a larger Ym value among the two road labels, so it should be placed in front of “Wy”. This case is also demonstrated  the road names “Jones St” and “Krum Av” in  7. 


Once we determine the order of the road labels for a road segment, we con.catenate the ordered road labels to generate a merged road name. 
Generating Named Road Vector Data from Raster map 
3.4 Propagation of Road Names 

Generally in computer map labeling and cartography map-making principles, not every road segment in a map is labeled with a road name because of the limited labeling space in the map and to avoid possible overlap of map labels. Therefore, repetitive road names are eliminated to improve the reading experience. For example, the “St. Louis Av” and the “University St” shown in  8 are spread across more than one intersection, but the road names only appear once in the map. The green arrows indicate the possible start and end points of these roads that can be interpreted by a viewer. Moreover, words belong to a road name can be spread out to indicate the extent of a road, such as the “Greer Av” and “ Dodier St” in  8. 

Fig. 8. Road labels do not repeat for each segment 
As a result, after every road label is assigned to a road segment and multiple road labels that are assigned to a road segment are merged, we still need to assign road names to the road segments that do not have an assigned road label, and we need to merge the road labels that belong to the same road name but are assigned to more than one road segment. For example, we need to merge “Greer” and “Av” into “Greer Av” and then assign the merged road name to the corresponding road segments. 
We start from a road segment, RS, that has an assigned road label, and we search for any other road segment that is connected to this road segment and has the same orientation. If a connected road segment, NextRS, has no assigned road label, we record that NextRS has the same road name as RS. If NextRS has assigned road labels, we order the assigned road labels of RS and NextRS  the described method in the previous section. Then for the ordered road names, A followed by B, if B is a short string, we determine that the combination of A and B represents a road name and hence A and B should be merged. This is because if the last word in the sequence is a short string, this last word very often represents the road-type abbreviations (e.g., Av, St, Pl, Wy, and Dr). We deﬁne a short road label as a label of less than 5 characters since the longest abbreviations of the road types are “Blvd”, which are 4 characters. This rule helps to merge two words into a complete road name. 
The name propagation algorithm runs iteratively and records the number of road segments that have their road names assigned during each iteration. After an iteration, the algorithm checks if the number of road segments that have their 
10 Yao-Yi Chiang and Craig A. Knoblock 
road names assigned has increased. If the number stays the same, the algorithm stops since there are no road names that can be propagated. The results after this name propagation algorithm is a set of road segments, each labeled with a road name or an empty label indicating there is no road label in the map associated with this road segment. 
4 Experiments 

We have implemented the approach described in this paper in a system called Strabo. This section presents our experiments on Strabo for generating named road vector data from 6 raster map of 2 map sources. The 2 sources are Rand McNally map (RM map) and Afghanistan Information Management Services (AIMS map).3  6(a) shows an example RM map. The RM map are designed for navigation purpose and contain very detailed road information. The RM map represent common street map that can be purchased in local gas stations and tourist stores. The AIMS map contain only the information of major roads and are commonly used in urban planning. 
We focus on evaluating the techniques for associating road names to the road vector data. The details of our road vectorization and text recognition results can be found in our previous work [Chiang, 2010; Chiang and Knoblock, 2011a]. To generate named road vector data from RM map, we labeled 1 road area, 1 text area, and 1 non-text area. For AIMS map, we labeled 1 road area and 1 text area. Based on these example areas, Strabo converted the road lines in the original map to vector format, recognized the road labels, and generated named road vector data. 
Strabo recognized 154 road labels and 892 road segments in the RM map, and 15 road labels and 338 road segments in the AIMS map. We manually veriﬁed each road segment  the test map. Among the 892 road segments in RM map, 866 road segments (97.09%) were correctly identiﬁed. Among the 338 road segments in AIMS map, 327 road segments (96.75%) were correctly identiﬁed. The incorrect road segments have false road topology and/or geom.etry.  9 shows an intersection where the road topology was incorrect due to the various road widths of the intersecting road lines. Sharp angles make the intersecting lines closer to each other and hence our road vectorization algorithm could not produce accurate geometry  the morphological operators. 
To evaluate the overall performance for generating named road vector data, we deﬁne the accuracy as the length of correctly labeled road segments divided by the length of all identiﬁed road segments. A correctly labeled road segment is deﬁned as follows: every line segment of a correctly labeled road segment represents a part or all of a road line that has the road name as the assigned name of the road segment. The accuracy for RM map is 92.38% and for AIMS map is 93.27%. 
 10 shows a portion of the extracted named road vector data dis.played and labeled  Esri ArcMap. The yellow lines are the extracted road 
3 The information for obtaining the test map and ground truth can be found on: 
http://www.isi.edu/integration/data/map/prj_map_extract_data.html 
Generating Named Road Vector Data from Raster map 

Fig. 9. Examples of incorrectly extracted road topology (red lines are the extracted road lines) 
4

vector data and the red text with underlines are the assigned names.From  10, we can see that Strabo successfully propagated the road names to the corresponding road segments so that the road lines that are not labeled in the original map also had correctly assigned road names. 
The majority of errors in our experimental results are due to the fact that some extracted road topology and/or geometry are incorrect. During the road vectorization process, the road topology could be incorrectly extracted due to the various road widths of the intersecting road lines. Because of this incor.rect road topology, the road names of the connecting roads could not propagate through this intersection and resulted in falsely assigned road names or incom.plete road names. Including a manual editing process for the results of the road vectorization and segmentation steps would reduce this type of error. 
In addition, OCR could produce recognition errors. For example, in the test map, the string “BLVD” was recognized as “8LVD” and “Parnell St” was recog.nized as “Pamell St”. If one or more characters of a road name was incorrectly recognized, the named road vector data results for the road segments associated with this road name were all considered to be incorrect. 
Overall, Strabo generated accurate named road vector data: the average ac.curacy for the 6 map from the 2 sources is 92.83%. To improve the results, we could have a user editor to process the extracted road vector data and recognized road labels for quality assurance so the text/road association algorithm could have more accurate input data. 
5 Related Work 

Map processing is an active area in both academic research and commercial software. However, to the best of our knowledge, the work presented in this paper is one of the ﬁrst complete approaches to handling the problem of generating named road vector data from raster map. 
The most closely related work is a map computerizing system called Map-Scan [mapcan, 1998] from the United Nations Statistics Division. mapcan has the functionality for manually converting the linear features in raster map into vector format and recognizing the text strings in raster map. mapcan includes an extensive set of image processing tools (e.g., the morphological operators) and labeling functions for the user to manually computerize the raster map, which requires intensive user input. For example, to recognize text strings  
4 The map labeling algorithm of ArcMap did not label every road segments. 
Yao-Yi Chiang and Craig A. Knoblock 

Fig. 10. The resulting named road vector data from RM and AIMS map 
mapcan, the user needs to label the areas of each text string and the string has to be in the horizontal direction. The association between the road names and extracted road vector data is achieved manually. In contrast, our approach requires only minimal user e.ort for recognizing road labels and extracting road vector data from raster map, and further, we associate road names to road vector data automatically. 
For text recognition from raster map, Pouderoux et al. [2007] present a text recognition technique for raster map. They identify text strings in a map by analyzing the geometric properties of individual connected components in the map and then rotate the identiﬁed strings horizontally for OCR. Roy et al. [2008] detect text lines from multi-oriented, straight or curved strings. Their algorithm handles curved strings by applying a ﬁxed threshold on the connecting angle be.tween the centers of three nearby characters. Their orientation detection method 
Generating Named Road Vector Data from Raster map 13 
only allows a string to be classiﬁed into 1 of the 4 directions. In both [Pouderoux et al., 2007; Roy et al., 2008], their methods do not hold when the string char.acters have very di.erent heights or widths. Moreover, these approaches handle speciﬁc types of road labels and do not work further to determine the association between the recognized road labels and the geographic features in raster map. 
For road vectorization from raster map, Bin and Cheong [1998] extract road vector data from raster map by identifying the medial lines of parallel road lines and then linking the medial lines. The linking of the medial lines requires various manually speciﬁed parameters for generating accurate results, such as the thresholds to group medial-line segments to produce accurate geometry of road intersections. 
Itonaga et al. [2003] focus on non-scanned raster map that contain only road and background areas. They exploit the geometric properties of roads (e.g., elongated polygons) to ﬁrst label each map area as either a road or background area. Then they apply the thinning operator to extract a 1-pixel width road network from the identiﬁed road areas. The geometry distortions in the thinning results are then corrected by user-speciﬁed constraints, such as the maximum deviation between two intersecting lines. 
In comparison to the approach in this paper, the techniques of Bin and Cheong [1998] and Itonaga et al. [2003] require signiﬁcant user e.ort on param.eter tuning. Moreover, their approaches do not determine where line segments compose a road segment in the vectorization result. 
In addition to road vectorization research work, many commercial prod.ucts o.er the functionality for raster-to-vector conversion, such as Vextractor,5 Raster-to-Vector,6 and R2V from Able Software.7 However, these commercial products do not have any text recognition capability, and hence do not work for generating named road vector data. 
6 Discussion and Future Work 
This paper presented a complete approach for generating named road vector data from raster map. In particular, we presented an approach that automat.ically identiﬁes individual road segments from road vectorization results and then associates recognized road labels with corresponding road segments. This approach, together with our previous road vectorization and text recognition work, allows a user to use only minimal e.ort for extracting named road vector data from raster map. The resulting named road vector data are widely useful, such as for supporting a geocoder, building a gazetteer, and enriching available road information for spatial analysis in a GIS. In the future, we plan to test this work  raster map with non-English labels. In addition, we plan to exploit the named road vector data generated from historical raster map for spatiotemporal analysis. 
5 http://www.vextrasoft.com/vextractor.htm 
6 http://www.raster-vector.com/ 
7 http://www.ablesw.com/r2v/ 

Bibliography 
Agarwal, P. K., van Kreveld, M., and Suri, S. (1998). Label placement by max.imum independent set in rectangles. Computational Geometry, 11(3-4):209 – 
218. 

Bin, D. and Cheong, W. K. (1998). A system for automatic extraction of road network from map. In Proceedings of the IEEE International Joint Symposia on Intelligence and Systems, pages 359–366. 
Chiang, Y.-Y. (2010). Harvesting Geographic Features from Heterogeneous Raster map. PhD thesis, University of Southern California. 
Chiang, Y.-Y. and Knoblock, C. A. (2011a). A general approach for extract.ing road vector data from raster map. International Journal on Document Analysis and Recognition, DOI: 10.1007/s10032-011-0177-1. 
Chiang, Y.-Y. and Knoblock, C. A. (2011b). Recognition of multi-oriented, multi-sized, and curved text. In Proceedings of the Eleventh International Conference on Document Analysis and Recognition. 
Doddi, S., Marathe, M. V., Mirzaian, A., Moret, B. M. E., and Zhu, B. (1997). Map labeling and its generalizations. In Proceedings of the eighth annual ACM-SIAM symposium on Discrete algorithms, pages 148–157. 
Edmondson, S., Christensen, J., Marks, J., and Shieber, S. M. (1996). A general cartographic labelling algorithm. Cartographica: The International Journal for Geographic Information and Geovisualization, 33(4):13–24. 
Freeman, H. (2005). Automated cartographic text placement. Pattern Recogni.tion Letters, 26:287–297. 
Goldberg, D. W., Wilson, J. P., and Knoblock, C. A. (2009). Extracting geographic features from the internet to automatically build detailed re.gional gazetteers. International Journal of Geographic Information Science, 23(1):92–128. 
Itonaga, W., Matsuda, I., Yoneyama, N., and Ito, S. (2003). Automatic extrac.tion of road networks from map images. Electronics and Communications in Japan (Part II: Electronics), 86(4):62–72. 
mapcan (1998). mapcan for Windows Software Package for Automatic Map Data Entry, User’s Guide and Reference Manual. Computer Software and Support for Population Activities, INT/96/P74, United Nations Statistics Di.vision, New York, NY 10017, USA. 
Pouderoux, J., Gonzato, J. C., Pereira, A., and Guitton, P. (2007). Toponym recognition in scanned color topographic map. In Proceedings of the Ninth International Conference on Document Analysis and Recognition, volume 1, pages 531–535. 
Roy, P. P., Pal, U., Llados, J., and Kimura, F. (2008). 	Multi-oriented English text line extraction  background and foreground information. IAPR International Workshop on Document Analysis Systems, pages 315–322. 
Efficient and Robust Graphics Recognition from 
Historical map 

Yao-Yi Chiang,1 Stefan Leyk,2 and Craig A. Knoblock3 
1 Information Sciences Institute and Spatial Sciences Institute, 
University of Southern California, 4676 Admiralty Way, Marina del Rey, CA 90292, USA. 
yaoyichi@isi.edu 
2 Department of Geography, University of Colorado, UCB260, Boulder, CO 80309, USA. 
stefan.leyk@colorado.edu 
3 Department of Computer Science and Information Sciences Institute, 
University of Southern California, 4676 Admiralty Way, Marina del Rey, CA 90292, USA. 
knoblock@isi.edu 
Abstract. Historical map contain rich cartographic information, such as road networks, but this information is “locked” in images and inaccessible to a geographic information system (GIS). Manual map digitization requires intensive user effort and cannot handle a large number of map. Previous approaches for automatic map processing generally require expert knowledge in order to fine-tune parameters of the applied graphics recognition techniques and thus are not readily usable for non-expert users. This paper presents an efficient and effective graphics recognition technique that employs interactive user intervention procedures for processing historical raster map with limited graphical quality. The interactive procedures are performed on color-segmented preprocessing results and are based on straightforward user training processes, which minimize the required user effort for map digitization. This graphics recognition technique eliminates the need for expert users in digitizing map images and provides opportunities to derive unique data for spatiotemporal research by facilitating time-consuming map digitization efforts. The described technique generated accurate road vector data from a historical map image and reduced the time for manual map digitization by 38%. 
Keywords: Color image segmentation, road vectorization, historical raster 
map, image cleaning 
Introduction 

map contain valuable cartographic information, such as locations of historical places, contour lines, building footprints, and hydrography. Extracting such cartographic information from map ( creating spatial layers that can be processed in a GIS) would support multiple applications and research fields. For example, there are numerous cases in which historical map have been used to carry out research in land-cover change and biogeography [Kozak et al., 2007; Petit and Lambin, 2002], and urban-area development [Dietzel et al., 2005]. 
Today, thousands of such map and map series are available in scanned raster format ( digital map images) in a variety of digital archives. Previous work on extracting cartographic information from raster map typically requires intensive user intervention for training and parameter tuning, in particular, when processing historical map of poor graphical quality [Gamba and Mecocci, 1999; Leyk and Boesch, 2010]. 
Consequently, most studies that utilize cartographic information from historical map for their analysis are based on time-consuming manual map digitization, which introduces subjectivity because of the missing reproducibility and the limited number of map that can be digitized. More advanced semi-or fully-automated procedures for cartographic information extraction from historical map would allow the advancement of such studies by including historical spatial data that are derived from a variety of map, underlie repeatable procedures with reproducible results, and cover large areas. 
In this paper, we present an interactive graphics recognition technique based on straightforward user training processes to minimize the required user effort for processing raster map, especially for the raster map that have limited graphical quality. We demonstrate this approach  a historical U.S. Geological Survey (USGS) topographic map in a road vectorization example, which builds on our previous work on color image segmentation (CIS) [Leyk, 2010] and road vectorization [Chiang et al., 2008; Chiang and Knoblock, 2011]. The USGS topographic map suffers from poor graphical quality and thus represents a particularly challenging research object. 
The remainder of this paper is organized as follows. Section 2 discusses related work. Section 3 presents our graphics recognition technique  an example of road vectorization from a historical USGS topographic map. Section 4 reports on our experimental results. Section 5 presents the conclusions and future work. 
Recent Work 

Here we provide a brief review of recent map processing research; a more detailed review can be found in [Chiang, 2010; Chiang and Knoblock, 2011]. Extracting cartographic information from raster map is challenging due to poor image quality caused by scanning or image compression processes, as well as the aging of the archived paper material, which often causes effects of false coloring, blurring or bleaching [Chen et al., 2006; Dhar and Chanda, 2006; Gamba and Mecocci, 1999]. The complexity of raster map contents increases if there are overlapping map layers of geographic features, such as roads, contour lines, and labels in different or similar colors. Color image segmentation has been investigated as a preprocessing step to separate individual map color layers [Chen et al., 2006; Chiang, 2010; Leyk, 2010; Leyk and Boesch, 2010]. However, there are still limitations when processing poor quality images of historical map [Leyk, 2010]. 
Much research has been devoted to extracting geographic features from particular map series. For example, Itonaga et al. [2003] describe a road-line vectorization approach from computer-generated map that cannot be applied to scanned map. Dhar and Chanda [2006] extract geographic features from Indian survey map based on user-specified image processing filters that exploit the geometric properties of these features. Another exemplary approach that is highly customized to a specific map series is the work by Raveaux et al. [2008] on extracting quartiers from historical French cadastral map. As with many other recent approaches, the described extraction procedures from Itonaga et al. [2003], Chanda [2006], and Raveaux et al. [2008] have limited applicability to other map series or types and require expert knowledge for parameter tuning. 

3 Methods: Road Vectorization from a Historical USGS Topographic Map 
We demonstrate our approach  a road vectorization example. s 1 shows a sample USGS historical topographic map. Typical problems existed in old map (and can be seen here too) that limit the graphical image quality are bleaching and blurring effects as well as mixed or false coloring as consequences of archiving paper material and scanning procedures.  2 illustrates the different steps and the user interface of the described technique. Our graphic recognition approach consists of three major steps: (i) separation of homogeneous thematic map layers  color image segmentation (CIS) [Leyk, 2010], (ii) interactive extraction and cleaning of these separated map layers, and (iii) subsequent raster-to-vector conversion of the cleaned map layers [Chiang et al., 2008; Chiang and Knoblock, 2011]. 
3.1 Color Image Segmentation 

As an important preprocessing step, color image segmentation (CIS) separates thematic homogeneous colored map layers. CIS is of critical importance since the outcome directly determines the image processing methods to be applied in all subsequent stages of data extraction. In our previous work, a hierarchical CIS approach based on homogeneity computation, color space clustering, and iterative global/local color prototype matching has been implemented and tested on historical USGS topographic map (s 1 shows a sample map) [Leyk, 2010]. The only input parameters for this CIS approach are the number and types of color layers. The approach has been improved for this paper to overcome some reported limitations in the final region-growing step, such as merging of nearby elevation or road line features. Constraining the final segments to maximum widths and enforcing connectivity between homogeneous portions was incorporated in order to avoid such merging effects and thus improve the final CIS output. 



(a) The color image segmentation result (b) A user label of an exmple road segment 
(c) 
The extracted road layer with noise pixel (d) Erosion operator to remove most road pixel 

(e) 
User provides examples of road pixel (f) Remaining large noise objects 

(g) 
Large noise objects are removed (h) User provides examples of small noise objects 

(i) 
Noise objects are removed (j) Raw road vectorization results Fig. 2. Road layer cleaning with minimal user labeling 














 2(a) shows the result of CIS of the original USGS topographic map portion ( 1). While the performance of the segmentation is visually satisfying ( individual map layers are shown in unique colors), there are some remaining merging and mixing effects at places with dense elevation contours and road lines as well as gaps in road lines where they intersect with elevation contours; these problems are caused by issues of graphical quality as described earlier. Although this CIS procedure could be further tuned to provide better results, the raw and unrepaired segmentation outcome is used to test the robustness of the subsequent cleaning and road vectorization steps when  a general non-post-processed (and thus sub-optimal) CIS outcome. 
3.2 Road Layer Extraction and Cleaning by Example 
Road Layer Extraction by Example. To extract the road layer (the black layer in the CIS result) from the segmented map, first one sample area (approximately) centered on a road line has to be identified by a user.  2(b) shows the user interface for the road-vectorization approach.  this interface a user labels a rectangular area of 20.by-20 pixel to provide a “road sample.” The thematic map layer that has the same color as the identified road lines ( the road-like features) are automatically identified in the entire map image and the road type (e.g., parallel lines or single lines) and road width are determined [Chiang and Knoblock, 2011]. This step is carried out in a rotation-invariant way.  2(c) shows the identified road layer. In this example, the majority of the detected linear features have a width of one pixel. 
Road Layer Cleaning By Example. The identified road layer might contain non-road pixel depending on the quality of the input map and the CIS result. Therefore, prior to generating the road vector data, the road layer has to be cleaned. Commercial products for raster-to-vector conversion, such as R2V1 and Vextractor,2 generally include image-processing tools, such as morphological or structural operators that can be selected and adjusted by the user to manually clean the raster image. However, this manual process requires expert knowledge and is very time consuming. In order to overcome such limitations, in this paper, we present an interactive technique, which incorporates simple user training processes for removing the undesired (non-road) pixel. This technique exploits user provided “noise samples” in order to identify appropriate image-processing filters and to generate parameter sets. These parameterized image-processing filters are then applied to efficiently clean the map image and remove existing noise. 
In this road vectorization example, the first step of our cleaning process removes large non-road objects (noise objects that are thicker than road lines); the second step removes small non-road objects. To identify large non-road objects in the road-layer.extraction result ( 2(c)), first, an image ( 2(d)) is created in which the majority of road lines are eliminated by applying the erosion operator and thus non-road objects remained. The number of iterations for erosion is determined by the detected road width (one pixel according to the one collected sample) [Chiang and Knoblock, 2011]. 
1 http://www.ablesw.com/r2v/ 2 http://www.vextrasoft.com/vextractor.htm 

The identified large noise objects ( 2(d)) contain some road pixel that are not eliminated by the erosion operator. This is because some of the road lines are thicker than the identified road width. To remove these road pixel from the identified large noise objects, the user provides an example containing road pixel that are not removed by the erosion operator ( 2(e)). The connected components in  2(d) that have a similar length, width, and number of pixel as the ones in the new user label ( 2(e)) are removed automatically; they represent parts of road features and will be preserved.  2(f) shows the remaining large noise objects that exceed the detected (sampled) typical road width. Removing the objects in  2(f) from  2(c) results in the image shown in  2(g) which contains road pixel and small noise objects. 
To remove the remaining small noise objects in  2(g), the user again provides local areas containing samples of the objects in question ( 2(h)). Connected components that show similar sizes as the ones in the samples are then removed from  2(g) in order to generate the final cleaning result ( 2(i)). 
3.3 Road Layer Vectorization 

Once the road layer has been cleaned, we employ our previously described technique for automatic generation of road geometry [Chiang et al., 2008] and subsequently convert the road geometry to road vector data automatically [Chiang and Knoblock, 2011].  2(j) shows the road vectorization results (without manual post-processing) consisting of 1-pixel wide road centerlines. 
In this road vectorization example, the horizontal map grid line is not removed during the cleaning process and the vectorization result thus contains a portion of these grid lines. Additional operators would be needed or manual post-processing (manually edit the road vector data) would have to be done to remove such elements. Also some broken road lines can be observed, which indicates the need to refine the procedure or to manually edit the final data layer. Such post-processing steps are generally needed for both semi-and fully-automatic approaches to process map of limited image quality. 
4 Experiments 

In this section our experiments are described in which the proposed approach is tested for extracting road vector data from a historical USGS topographic map that covers the city of St. Jose, California.3,4  3(a) shows the original map tile (2283 . 2608 pixel). We compared the interactive recognition technique with a completely manual map digitization process by (i) the required manual processing time and (ii) the accuracy of extracted road vector data. The required manual processing time for our approach includes the time for user labeling and manual post-processing. We use Esri 
3 	USGS Topographic map: Map page, San Jose, California (San Jose Quadrangle); edition of 1899 (reprint 1926), engraved in July 1897 by USGS; scale: 1/62,500 
4 	The detailed information for obtaining the test map and the ground truth can be found on: http://www.isi.edu/integration/data/map/prj_map_extract_data.html 
ArcMap 5 for both our manual post-processing step and the compared manual digitization work. 
Given the test map tile, a user first specified the number of desired map layers. Then the segmented map with separated color layer was generated based on CIS ( 3(b)). The user-specified number of desired map layers was 4 in this case: 




(a) 
The extracted road layer (b) The cleaned road layer Fig. 4. The test map tile and intermediate results 

5 http://www.esri.com/ 

Table 1. Comparison of the required user time for road vectorization from the test map Manual Work Time 
Methods Task (Hours: Minutes: Seconds) 

User labeling 0:0:33 
Our Interactive Approach Manual post-processing 0:49:52 Completely Manual
Manual Digitization 01:21:24 
Approach 

Table 2. Accuracy of the extracted road vector data from our interactive approach  themanual digitization results as the ground truth 
Road 

Buffer  Completeness  Correctness  Redundancy  RMS  
1 pixel  99.9%  100%  0.054%  0.152 pixel  
2 pixel  100%  100%  0.14%  0.152 pixel  

black, red, blue and white (background). Within the segmented map, the user selected 1 sample area of roads and 2 sample areas of noise objects to generate a cleaned road layer.  4(a) shows the extracted road layer with some of the remaining non-road pixel, and  4(b) shows the cleaned result after removing these noise pixel. Some of the broken road lines were also eliminated, accidentally. The cleaned road layer was then automatically converted to a set of road vector line features. Finally, the user manually edited the road vector data (e.g., recover the broken lines) to achieve a complete road vectorization result. 
Table 1 shows the required user time for our approach and the completely manual digitization. Our approach required a total of 50 minutes and 33 seconds to vectorize the road network from the test map tile. The computing time for road-layer-extraction was 3 seconds, for road-layer-cleaning 52 seconds, and for road vectorization 56 seconds. In contrast, complete manual digitization required 1 hour, 21 minutes, and 24 seconds. Our approach reduced the digitization time by 38%. The manual post-processing time in our approach was mainly spent with creating missing lines in the top-right corner of the map tile and connecting broken lines. The completely manual digitization required more than 1 hour because manual tracing of the exact road centerlines requires precise mouse movement on the pixel level. 
We also compared the extracted road vector data from the two approaches based on the completeness, correctness, quality, redundancy, and the root-mean-square (RMS) difference (the average distance between the extracted lines and the ground truth) [Heipke et al., 1997; Chiang and Knoblock, 2011]. We use the manual digitization results as ground truth to compute the described metrics from the road vector data generated by our interactive approach. 

The completeness is the length of true positives divided by the sum of the lengths of true positives and false negatives, and the optimum is 100%. The correctness is the length of true positives divided by the sum of the lengths of true positives and false positives, and the optimum is 100%. The redundancy is the length of matched extraction minus the length of matched reference. The redundancy shows the percentage of the matched ground truth that is redundant ( more than one true positive line matched to one ground-truth line), and the optimum is 0. The RMS difference is the average distance between the extracted lines and the ground truth, which represents the geometrical accuracy of the extracted road vector data. To identify the length of the true positives, false negatives, and matched ground truth, we used buffer widths of 1 and 2 pixel. This means that a correctly extracted line is no farther than 1 or 2 pixel from the ground truth. 
Table 2 shows the accuracy of our extracted road vector data  the manual digitization results as the ground truth. Our approach generated accurate results (high completeness and correctness) at both 1-pixel and 2-pixel buffer sizes. In addition, our results contain very few redundant road lines and are very close to the ground truth regarding their geometry ( low RMS).  5 shows details of portions of the final road vector data from our approach on top of the extracted raster black layer. Note that the resultant road vector data are very close to the original road centerlines. 
6 Conclusions and Future Work 

We presented an efficient and robust approach for graphics recognition from historical raster map. We showed that this approach minimizes the required user intervention and reduces the manual digitization time for road vectorization from a historical USGS topographic map by 38%. The presented technique shows high potential for robust 
extraction of cartographic information from historical map of low graphical quality 
and opens unique opportunities for “spatio-historical” research in various fields. We plan to improve our graphics recognition technique by further refining and 
constraining the final region-growing step in the CIS to reduce the number of noise 
objects in the CIS result. Moreover, we plan to incorporate connectivity and additional 
geometry constraints in the image-cleaning-by-example step to further reduce the 
required manual processing time. 
References 

Chen, Y., Wang, R., Qian, J. (2006) Extracting contour lines from common-conditioned topographic map. IEEE Transaction Geoscience and Remote Sensing, 44(4), 1048–1057. 
Chiang, Y.-Y. (2010). Harvesting Geographic Features from Heterogeneous Raster map. Ph.D. thesis, University of Southern California. 
Chiang, Y.-Y., Knoblock, C. A., Shahabi, C., and Chen, C.-C. (2008). Automatic and accurateextraction of road intersections from raster map. GeoInformatica, 13(2):121-157. 
Chiang, Y.-Y and Knoblock, C. A. (2011). General Approach for Extracting Road Vector Datafrom Raster map. International Journal on Document Analysis and Recognition, 2011. 
Dhar, D. B. and Chanda, B. (2006). Extraction and recognition of geographical features from paper map. International Journal on Document Analysis and Recognition, 8(4): 232-245. 
Dietzel, C., Herold, M., Hemphill, J.J. and Clarke, K.C. (2005). Spatio-temporal dynamics inCalifornia's Central Valley: Empirical links to urban theory. International Journal of Geographical Information Science. 19(2):175-195. 
Heipke, C., Mayer, H., Wiedemann, C., and Jamet, O. (1997). Evaluation of automatic roadextraction. International Archives of Photogrammetry and Remote Sensing, 32: 47–56. 
Gamba P. and Mecocci A., (1999). Perceptual Grouping for Symbol Chain Tracking in Digitized Topographic map, Pattern Recognition Letters, 20(4): 355-365. 
Itonaga, W., Matsuda, I., Yoneyama, N., and Ito, S. (2003). Automatic extraction of road networks from map images. Electronics and Communications in Japan, 86(4):62-72. 
Knoblock, C. A., Chen, C., Chiang, Y.-Y., Goel, A., Michelson, M., and Shahabi, C. (2010). A General Approach to Discovering, Registering, and Extracting Features from Raster map.In Proceedings of the Document Recognition and Retrieval XVII of SPIE-IS&T Electronic Imaging, vol 7534. 
Kozak, J., Estreguil, C. and Troll, M. (2007). Forest cover changes in the northern Carpathiansin the 20th century: a slow transition. Journal of Land Use Science. 2(2):127-146. 
Leyk S. (2010). Segmentation of Colour Layers in Historical map based on HierarchicalColour Sampling. In GREC. LNCS 6020, pages 231–241. 
Leyk S. and Boesch R. (2010). Colors of the Past: Color Image Segmentation in HistoricalTopographic map Based on Homogeneity. GeoInformatica 14(1): 1-21. 
Leyk S. and Boesch R. (2009). Extracting Composite Cartographic Area Features in Low-Quality map. Cartography and Geographical Information Science 36(1):71-79. 
Petit, C.C. and Lambin, E.F.. (2002): Impact of data integration technique on historical land.use/land-cover change: Comparing historical map with remote sensing data in the BelgianArdennes. Landscape Ecology 17(2), 117-132. 
Raveaux, R., Burie, J.-C., and Ogier, J.-M. (2008). Object extraction from colour cadastral map. In Proceedings of the IAPR Document Analysis Systems, pages. 506-514. 
Automatically Identifying and Georeferencing Street map on the Web 
Sneha Desai, Craig A. Knoblock, Ching-Chien Chen Yao-Yi Chiang, Kandarp Desai Geosemble TechnologiesUniversity of Southern California 2041 Rosecrans Ave. Suite 245 Information Sciences Institute El Segundo, CA 90245 4676 Admiralty Way jchen@geosemble.comMarina del Rey, CA 90292 [desai, knoblock, yaoyichi]@isi.edu, kandarpd@usc.edu 
Categories and Subject Descriptors 
H.2.8 [Database Management]: Database Applications— Spatial Databases and GIS 
General Terms 
Algorithms, Design 
Keywords 
Street map, map search engine, map georeferencing 
1. INTRODUCTION 
There are a wide variety of street map available on the Web, but many of these map cannot be easily located. In addition, many of these map lack detailed metadata that describes important information, such as the geocoordinates and scale of the map. Image search engines, such as Google Images can be used to find map, but accurately and efficiently identifying street map among different images remains a challenging task. This is because image sources might include images that are not street map, such as photographs, icons, scanned documents, political map and climate map. Those images are not relevant for applications which seek only street map. These sources provide data from their own spatial databases and stored information services. They are searched on the basis of context, for example, on the basis of filenames included in a query. It is important to find a method that does content-based searches for street map and identifies them by image content, such as their unique patterns of roads and not just by their context. 
In this paper, we describe an approach to efficiently retrieve all available street map by integrating various sources and eliminating images that are not street map. We explain the method to identify street map among all the images retrieved by applying certain image processing techniques to identify unique patterns, such as street lines, which differentiate them among all other images. To identify street map, we apply Law’s texture classification algorithm [6, 7] to recognize the unique image 
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. To copy 
otherwise, or republish, to post on servers or to redistribute to lists, 
requires prior specific permission and/or a fee. 
GIR’05, November 4, 2005, Bremen, Germany. 
Copyright 2005 ACM 1-59593-165-1/05/0011…$5.00. 

patterns such as street lines and street labels. In addition, we exploit the utilization of our previous work of the automatic extraction of the road intersections to find the intersections on the street map identified [9] as well as utilizing GEOPPM [1], an algorithm for automatically determining the geocoordinates and scale of the map. 
The rest of the paper is organized as follows. Section 2 explains the previous work. Section 3 describes the method of classifying street map and finding intersections and geocoordinates of the map. Section 4 reports our experimental results. Section 5 discusses the related work and Section 6 concludes and presents future work. 
2. PREVIOUS WORK ON GEOREFERENCING STREET map 
In our previous work, we presented a technique to automatically extract road intersections on the map and then determine the geocoordinates and scale of the map. Considering the fact that road layers are usually unique patterns to identify the coverage of the map, we identify the road intersections on the street map for extracting that unique pattern. We utilize the automatic image processing and pattern recognition algorithms [9] to identify the intersection points on the street map. This section is utilized in our present approach and shown as module 2 in  1.  
After finding intersections,  the intersection patterns, we automatically conflate the street map (whose geocoordinates are unknown in advance) and the imagery with known vector data utilizing the GEOPPM algorithm [1].  this technique, we can identify the geocoordinates and scale of the street map. We describe a specialized point pattern matching algorithm to align the two point sets and conflation techniques to align imagery with map. The work is utilized in our present work and shown as module 3 in  1. 
3. MAP FINDER – OVERALL APPROACH 
The overall approach of the map finder is shown in  1. The input is the query string of a city name. The final output would be all street map of the city queried along with the geocoordinates and scales. We describe the whole process of map finder in three modules. The first module includes the automatic classification of street map among images, which are retrieved from different image sources. In the second module, we automatically extract the road intersection points on the street map identified by the first module.  the set of road intersection points we determine the 

City name (Query String) 


Street map of the city queried 
Module 2 : Automatic extraction of intersections Intersections on the street map 




Module 3 : Automatic georeferencing street map 
Geocoordinates and scales  of the street map 


 1. The overall approach of the map finder 
geocoordinates and the scale of the map in the third module. This section describes the details of the first module in  1.  
3.1 Retrieving All Available Street map on the Web 
We retrieve all available street map on the web by integrating image sources such as Google Images and Yahoo Images. Image sources have different images including photographs, icons, political map and climate map, which are not street map. Therefore we focus on the images retrieved from the image sources. We used wrapper technology discussed in [4] to extract the image URLs from the sources. We check the availability of an image from the URL provided by the sources. Moreover, there might be some duplicate image URLs. We eliminate both nonworking and duplicate URLs. After finding available images from the total retrieved images, we identify the street map among all the available images. 
3.2 Identifying Street map 
The primary task is to identify street map that can be passed to the automatic extraction of road intersections ( module 2 in  1) and automatic georeferencing ( module 3 in  1). This process is shown in  2. 
First, we filter all the street map from all different types of images. From those filtered street map, we identify those street map which are good for extracting road intersection patterns and conflating with known vector data. Each image has its own unique texture features, which are different and can be distinguishable from other images. We use Law’s texture classification algorithm [6, 7] to find unique patterns of the street map. 
Images from Image Search 

Street map 
Non-Street map 
Filter 2 
Detailed Street map 

 2. Identifying street map among all different types of images
There are 25 attributes of street map that are distinguishable for each image. We calculate these attributes for three color values (red, green and blue) thereby providing 75 different values to identify the street map. The algorithm identifies the textures such as lines and spots on the images. The measures are computed by first computing small convolution kernels of the digital image and then performing a nonlinear windowing operation as discussed in [6, 7]. For the automatic training and classification, we use SVMlight V2.0 support vector machine on a large scale training and classification [8]. We manually provide 1150 image examples including positive and negative examples to train the SVM and then do the automatic classification of the images. 
3.2.1 Identifying Street map among All Images, Retrieved from Image Search 
In the first phase, we automatically identify street map having street lines and street labels, as shown in the  3b, 3c, 3d. The image sources contain many different types of images. We distinguish all these images into two main categories. The first category includes the street map, having the street information such as street lines and street labels. The second category includes images such as photographs, icons, logos, scanned documents, charts, graphs, commercial advertisements as well as economic map, political map, weather map, etc., which do not contain any street information, as shown in  3a. This filtering step is represented as Filter 1 in  2. 
3.2.2 Identifying Street map that can be Passed to GEOPPM to Find Geocoordinates 
To get the geocoordinates of the map, the map image can be passed to the GEOPPM algorithm. GEOPPM does the matching of intersection points, thereby requiring an appropriate pattern of intersection points on street map. For recognizing these types of map, we define the measure of intersection point density as follows: 
 Total number of intersections found on the map 
Intersection Point Density =       Size of the map, pixel x pixel 


 a. Images, which are not street map, eliminated by Filter-1 



For dense map (e.g., low resolution map with resolution >= 7 m/pixel, having an intersection point density in the range of 0.01 to 0.02), we cannot find all available intersection points by the intersection detector [9]. This is because most of the street labels touch the street lines, so that when we remove the characters for extracting the road network from the map we might loose the street line information. Without being able to detect a sufficient number of intersections, GEOPPM cannot determine the accurate geocoordinates. One of the dense map found is shown in  3b. 
For the sparse map (e.g., high resolution map with resolution <= 1 m/pixel, having an intersection point density in the range of 0.00001 to 0.00008), though the intersection detector could find the intersections on these types of map, GEOPPM would not be able to identify the matching pattern to the available vector data. This is because these map would not have a sufficient number of intersection points on it.  3c shows this type of sparse map. 
The GEOPPM finds the geocoordinates of the street map for which it can find an appropriate point pattern to match with the available vector data.  3d illustrates this type of map. This step is represented as Filter 2 in  2. 
4. EXPERIMENTS 
We experimented on two image sources: Google Images and Yahoo Images. We retrieve the images from these sources and apply map filtering techniques on the images. We used the city of El Segundo, CA as the query string input to retrieve all available street map. We report our experiments in the form of a tree in  4. In the tree, the result we achieve is shown without parenthesis and the recall and precision in parenthesis.  
Total retrieved image URLs from image sources 
198 


Removing nonworking and duplicate URLs 
Nonworking URLs + Working URLs 
Duplicate URLs 152 (R=100%, P=100%) 
46 (R=100%, P=100%) 


Filter-1 
Non-Street map Street map, found by 113 (R=100%, P=97.35%) Filter1 39 (R=92.86%, P=100%) 

Filter-2 
Street map, found by 
Dense and sparse street map 
Filter2 
17 (R=94.44%, P=100%) 
22 (R=100%, P=95.45%) 

Street map not of the city Automatically queried georeferenced street map  15 (R=88.24%, P=100%) of the city queried 7 (R=100%, P=71.43%) 
 4. Experimental results 

We define our recall as correctly identified street map / actual street map returned by the sources and precision as correctly identified street map / total street map returned. In the final stage we get 100 % recall and 71.43 % precision on automatically georeferencing the map that cover the city queried. 
The computation time primarily depends on the time consumed by the filtering technique, and it varies with the size of the image because the filter measures the unique texture features of the image pixel by pixel. The time it took for classifying the image ranges from 1.1 seconds to 72.93 seconds with the average of 
29.65 seconds. 
5. RELATED WORK 
Although there is much research in the area of image retrieval and classification, we did not find any work related to identifying street map and georeferencing them.  
The approach described in [5] finds the image categories of news web sites and classifies the story, previews, commercial advertisements and logos. They focus on the combination of frequency domain and image and text features for the categorization, but they are not doing any kind of identification or classification of map. In our work, we demonstrate a method of identifying unique street map properties like lines and labels with the help of distinctive image texture measures.  
For compound images with text and lines, a new approach has been built in [3], which exhibits Transform Coefficient Likelihood (TCL) for identifying the text portion on the image. The approach could be used for the classification of map by text features. We are also identifying the street labels, which are text, but we are  pixel neighborhood and patterns for identifying street map texture properties. 
The Web Seer [2], which is the image search engine for the World Wide Web, provides a way to locate the images on the web by  file name, file type, file size and color depth. They demonstrate the method to search images such as photographs and drawings by the image content tests such as band difference test, farthest neighbor test, color test and narrowness test. In contrast, we apply our method to identifying the street map from different types of images retrieved from the image search. In addition, our work georeferences the street map identified. 
6. CONCLUSION AND FUTURE WORK 
The main contribution of this paper is the design and implementation of the fusion of useful searching and integration technique as well as the identification of the street map among all images retrieved from the image sources. In addition, we find intersection points on the resulting street map and eventually determine the geocoordinates and the scale of those street map.  
We achieve a precision of 95.45 % in identifying the street map. This demonstrates that our approach leads to remarkably accurate identification of street map. The precision of 71.43 % for automatically georeferecing the street map illustrates that we are successful in finding geocoordinates and scale of the resultant street map by utilizing the intersection detector and GEOPPM. The street map classified by our filters could be used in a variety of applications and map search engines. 
We intend to extend our approach in several ways. First we plan to further classify the images into different categories of map such as political map, weather map, etc. We plan to minimize the use of additional image content to identify the image. We plan to develop OCR-related techniques to extract textual information from the identified street map in order to label the detected intersections found and to support textual searching on map. 
7. ACKNOWLEDGEMENT 
This research is based upon work supported in part by the National Science Foundation under Award No. IIS-0324955, and in part by the Air Force Office of Scientific Research under grant number FA9550-04-1-0105. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of any of the above organizations or any person connected with them. 
8. REFERENCES 
[1] 	
C. C. Chen, C. A. Knoblock, C. Shahabi, Y. Y. Chiang, and 

S.
 Thakkar. “Automatically and Accurately Conflating Orthoimagery and Street map”. The 12th ACM International Symposium on Advances in Geographic Information Systems (ACM-GIS'04), Washington, D.C., USA, November 2004. 


[2] 	C. Frankel, M. Swain, and V. Athitsos. “Webseer: an image search engine for the world wide web.” Technical Report TR96-14, University of Chicago, Chicago, IL, 1996. 
[3] 	I. Keslassy, M. Kalman, D. Wang, and B. Girod. “Classification of compound images based on transform coefficient likelihood.” In Proceedings of the 2001 International Conference on Image Processing (ICIP’2001), Thessaloniki, Greece, October 2001. 
[4] 	I. Muslea, S. Minton, and C. A. Knoblock. “Hierarchical wrapper induction for semi structured information sources”. Autonomous Agents and Multi-Agent Systems, 4(1/2), 2001. 
[5] 	J. Hu and A. Bagga. “Functionality Based Web Image Categorization.” In Proceedings of the Twelfth International World Wide Web Conference, Budapest, Hungary, May 2003. 
[6] 	K. Laws. Textured Image Segmentation, Ph.D. Dissertation, University of Southern California, Los Angeles, CA, January 1980. 
[7] 	K. Laws. Rapid texture identification. In SPIE (The International Society for Optical Engineering) Vol. 238 Image Processing for Missile Guidance, pages 376-380, 1980. 
[8] 	T. Joachims, “Making large-Scale SVM Learning Practical”. Advances in Kernel Methods - Support Vector Learning, B. Schölkopf, C. Burges , and A. Smola (ed.), MIT-Press, 1999. 
[9] 	Y. Y. Chiang, C. A. Knoblock, and C. C. Chen.  “Automatic Extraction of Road Intersections from Raster map”. In Proceedings of the 13th ACM International Symposium on Advances in Geographic Information Systems (ACM.GIS'05), Bremen, Germany, November 2005. 

A General Approach to Discovering, Registering, and Extracting Features from Raster map 
Craig A. Knoblockab, Ching-Chien Chenb, Yao-Yi Chianga , 
Aman Goela, Matthew Michelsonc, and Cyrus Shahabiab 

aUniversity of Southern California, Information Sciences Institute and 
Department of Computer Science, Los Angeles, CA, USA; 
bGeosemble Technologies, 841 Apollo Street, El Segundo, CA, USA; 
cFetch Technologies, 841 Apollo Street, El Segundo, CA, USA 

ABSTRACT 
map can be a great source of information for a given geographic region, but they can be di.cult to ﬁnd and even harder to process. A signiﬁcant problem is that many interesting and useful map are only available in raster format, and even worse many map have been poorly scanned and they are often compressed with lossy compression algorithms. Furthermore, for many of these map there is no meta data providing the geographic coordinates, scale, or projection. Previous research on map processing has developed techniques that typically work on map from a single map source. In contrast, we have developed a general approach to ﬁnding and processing street map. This includes techniques for discovering map online, extracting geographic and textual features from map,  the extracted features to determine the geographic coordinates of the map, and aligning the map with imagery. The resulting system can ﬁnd, register, and extract a variety of features from raster map, which can then be used for various applications, such as annotating satellite imagery, creating and updating map, or constructing detailed gazetteers. 
Keywords: raster map, map discovery, map extraction, road extraction, OCR, registration, conﬂation 
1. INTRODUCTION 
map provide an incredibly rich source of information and are available for most of the world.  1 pro.vides a 1946 Ordnance Survey map that shows the road network, railway lines, waterways, building struc.tures, property lines, and industries, along with text labels for many of these features. But many map, such as this one, are only available in either paper or as scanned raster ﬁles, which makes it di.cult to au.tomatically extract the features from them. Modern day map are typically made from a set of vector layers and rendered as a raster layer for display; but even for many recently constructed map, the raster or printed map are around long after the vector layers have been lost or discarded. The challenges are how to ﬁnd map of a given area, how to determine the coverage of the discovered map, how to register the map with other geospatial sources, such as aerial imagery, and how to extract features, such as the road and text layers, from them. 
Further author information: Craig A. Knoblock: knoblock@isi.edu, Ching-Chien Chen: jchen@geosemble.com, Yao-Yi Chiang: yaoyichi@isi.edu, Aman Goel: amangoel@isi.edu, Matthew Michelson: mmichelson@fetch.com, Cyrus Shahabi: shahabi@usc.edu 

There has been a signiﬁcant amount of previous work on automatic map processing. This includes work on extracting road intersections,1 extracting road networks,2, 3 extracting topographic lines,4–6 performing optical character recognition on the text,7, 8 registering map with imagery,9 extracting other types of symbols from map (e.g., the symbol for a swamp),10 and so on. While there has been a great deal of work, the one thing that all of this work has in common is that each technique focuses on a particular type of map (for example, the USGS topographic map. have been a focus in many research papers). Many of the proposed techniques could be applied to other types of map, but they would ﬁrst need to be modiﬁed or tuned to work well on these map. The problem here is that there are almost as many types of map as there are map since each new map often uses di.erent colors, symbology, fonts, and layers for the di.erent features. In contrast, we developed a general approach to ﬁnding and processing raster map. Unlike the previous work, we do not assume that we have previously seen a given type of map. However, we do assume that the map is drawn to scale, the orientation of the map is known (most map are oriented with North being up and if not they indicate on the map the North direction), the map has a road network on it, and the map provides a detailed road network (e.g., it does not simply show the freeways or highways). Given these assumptions, our approach can process a large and diverse set of raster map. 
In the remainder of this paper we present our end-to-end approach to discovering raster map, registering their precise location, and extracting road and text features from the map. Our approach starts with a name of a city and ﬁrst searches for map that cover that location. This is done by ﬁnding images  a search engine and then classifying the images into the ones that are likely to be map (Section 2). Subsequently, for each of these map the next step is to extract the road and text layers from these map (Section 3). For the road layer, the system then identiﬁes the intersections in the road layer, including the intersection location, road connectivity, and road orientation for each intersection (Section 4). The extracted intersections are then compared to the knowledge of existing road networks to register the map and determine the mapping between the intersections on the map and the ones on the road network (Section 5). Given this mapping, the system can then align the map with the corresponding imagery  a technique called conﬂation (Section 6). We then brieﬂy describe the planned next steps in our research on feature recognition from the extracted road and text layers (Section 7). We compare our work to the related work on these topics (Section 8). Finally, we conclude with a summary of the contributions and promising directions for future research (Section 9). 
2. RETRIEVING map ON THE INTERNET 
There are many interesting map freely available on the World Wide Web, ranging from street map to public transportation route map. Yet, there is no single data source that collects all of them for a given region. Online map are spread across sites and exist in many di.erent formats. Sometimes they are embedded within documents. Even commercial image search engines are inadequate for retrieving map. We found that the number of map returned among the top results for queries like “Tehran map” is small. For example, at the time of writing this paper, the ﬁrst result page on Yahoo Image Search† for this query, had only two map among 21 images on the page. 
This motivates a need for automatically and accurately ﬁnding map on the Web. This task breaks into two distinct sub-tasks. First, our method must ﬁnd candidate map images. This process is based solely on context, rather than deep image analysis, since analyzing the content of each image on the Web would be too expensive both in terms of time and computational e.ort. The second sub-task then classiﬁes these candidate images as map or non-map, based on their image content. We use a K-Nearest Neighbors classiﬁer based on Content Based Image Retrieval (CBIR),11 where each image is represented by a feature vector. We have developed a set of features based on the Water-ﬁlling algorithm12 that capture the prominent characteristics of map. In our previous work,13 we describe our work on this classiﬁer in detail.  2(a) shows the architecture of our system. The rectangles in the diagram represent collections of data (e.g., documents, images) and the parallelograms depict processes. 
. 
http://topomap.usgs.gov/ 
†http://images.search.yahoo.com/ 
We have observed that most high quality map on the Web are found embedded within PDF reports and other documents because governments, compa.nies, and other organizations include map in many of their location-speciﬁc reports.  2(b) shows an example of a detailed street map embedded within a larger PDF document. Further, these images are large (from 2MB to 50MB), resulting in high-quality, high-utility map for our collection. This is in contrast to many of the images that exist on the Web indepen.dently of documents, since these images are smaller (to lessen the burden on casual browsers), and hence of lower quality. Therefore, in order to collect candi.date map, we query commercial search engines, such as Google, for PDF documents related to the area. We then download the PDFs, parse them, and extract their images. We have developed a PDF parser, based on the o.cial PDF speciﬁcation‡ by Adobe, to extract the embedded images out of the downloaded ﬁles. 
2.1 FEATURES FOR CLASSIFYING IMAGES 
Once we collect the initial set of candidate images from documents, such as PDFs, we extract a set of features from these candidate images that we believe capture the deﬁning characteristics of map and distinguish them from other images. These features are based on the Water-ﬁlling algorithm.12 This algorithm sim.ulates pouring water through the lines of an image. As such, it operates on the edge-map of an image, which makes it color invariant.§ Then, statistics are generated based on the traversal of the water through the edge map. These statistics include the time re.quired to ﬁll up the sections of the edge map (“Filling Time”), the amount of water required to ﬁll it (“Wa.ter Amount”), and the number of points at which the ﬂow of water was divided into two or more streams (“Fork Count”). We note that an edge map can result in a number of disjoint edge sections, and therefore our method calculates these Water-ﬁlling statistics for all of the sections.¶ 
Once our approach calculates the statistics for each edge section, it creates a histogram of values for each feature, putting the values into one of eight di.erent bins representing di.erent intervals of values. Thus we get a 24 element feature vector that represents the image. Since the number of edge sections is roughly proportional to the size of the image, we normalize the 



‡http://www.adobe.com/devnet/pdf/pdf reference.html 
§
We use the Canny edge detector14 to create our edge map. 
¶
A mid-sized image (800 pixel by 800 pixel) has about 1000 such disjoint edge sections.  3. A sample image, its Canny edge map, and a part of it zoomed in to show the disjoint edge section 

vector to make it size-invariant by scaling the histograms so that the total number of edge sections is equal to 1000.  3 shows an example image of a map, its Canny edge map, and a section of it enlarged to show the disjoint edge sections. 
Map images, in general, have relatively longer lines, which represent streets, freeways and borders with other regions. Accordingly, the Filling Time for most edge sections tends to be high. Also, map have higher Fork Counts because of the repeated branching of roads or borders. On the other hand, a typical image that is not a map does not have such long lines. For example, a picture of vegetation will have many short segments for edges of leaves on trees and other growth. Therefore non-map images will have Fork Counts of mostly zero or one. The generated histograms reﬂect this property. A map generally skews toward larger Filling Times, Fork Counts and Water Amounts, whereas a non-map image skews toward shorter segments and lower Fork Counts. 
2.2 CLASSIFYING IMAGES  A CBIR K-NEAREST-NEIGHBOR METHOD 
map share many common characteristics in general, such as those described above. Yet, they come in a wide variety (street map, weather map, physical map, etc.), where each type has its own characteristics and distribution of features, which are slightly di.erent for each of them. For example, street map have sparse, straight lines, whereas contour map have a large number of curved lines. This implies that in addition to boundaries that exist between non-map and all types of map, there also exist various overlapping clusters in the feature space corresponding to these subtypes. Yet, deﬁning these sub-classes a priori for classiﬁcation presents a number of di.culties. For instance, should an urban-hydrographical map be its own class or should it be a member of urban/hydrography map? 
To identify map we use a K-Nearest Neighbor (K-NN) classiﬁer because it can take advantage of clusters in the feature space and implicitly model the subclasses of map. Therefore, even as new subclasses arise, the set of neighbors can be extended (say by adding new training map) rather than having to train a new classiﬁer for a newly deﬁned subclass. To quickly ﬁnd the closest neighbors to a given image, we use Content-Based Image Retrieval,11 where an image is represented by a set of features, and similar images (e.g., close neighbors) are those that are closest in the space of these features. We use L1-distance in Euclidian space as a measure of the similarity between images, where the L1-distance between two points is the sum of the absolute di.erences of their coordinates. Our resulting CBIR-based K-NN classiﬁer takes as input an image represented in our Water-ﬁlling feature space, and returns the nine most similar images based on our CBIR approach. If the majority of neighbors are map, then the input image is classiﬁed as a map.  4 shows the schematic of our classiﬁer. 
To evaluate this approach, we built a repository of 4000 images (2000 map and 2000 non-map) by manually labeling images taken from the Web. We included map of US and non-US cities and of large and small geographical regions, such as towns as well as continents. This repository covers a large range of subtypes of 

 4. An example of how classiﬁcation is done  a K-NN classiﬁer based on Content Based Image Retrieval 
Table 1. Performance of the two approaches 

Type of set-up  Precision  Recall  F1-measure  
CBIR approach  77.39  71.20  74.17  
Desai et al.  69.23  47.62  56.43  

map. To test our classiﬁer, we collected 4000 new images by querying Yahoo Image Search for 14 di.erent cities (e.g., Los Angeles, Tehran, Shanghai, etc.) and 2 continents (Asia and Africa). We retained only those images that did not exist in the repository. We then classiﬁed these images  our CBIR-based approach as well as  the method proposed in Desai et al.15 as a baseline comparison. The results of classiﬁcation  both approaches are given in Table 1. As shown in the table, our approach achieves an F1-measure of 74%, which is about an 18% improvement over an earlier approach to the same problem. 
3. EXTRACTING ROAD AND TEXT LAYERS FROM map 
Once a set of map has been collected from the Web, the next step is to unlock the geospatial information hidden in those map. The ﬁrst step in this process is to separate the pixel of each geographic feature from a map. A set of extracted pixel representing a particular geographic feature in a raster map is called a feature layer, where the road pixel constitute the road layer and the character pixel constitute the text layer.  5 shows a raster map from the Via Michelin website. and the extracted road and text layers from the map. The extracted feature layers can be used to annotate other geospatial data; for example, we can align the extracted text layer 
.http://www.viamichelin.com 

(a) A raster map from Via Michelin (b) The extracted road layer (c) The extracted text layer  5. An example of a raster map and its extracted road and text layers 
with imagery to label the roads in the image, as shown in  6. Further, we can convert the raster layers to machine-readable geographic features (e.g., road vectors) and create map meta data for indexing the map (as described in Section 7). 
The extraction of feature layers is a challenging task because of the varying image quality of raster map (e.g., poor quality scanned map), the complex.ity of map ( overlapping features in map), and the typical lack of meta data (e.g., map geocoordi.nates, map source, vector data source). To overcome these di.culties, we developed map decomposition techniques, each requiring a di.erent amount of user e.ort to extract the road and text layers from hetero.geneous raster map.16–20  7 shows our overall approach to handling map of varying quality. Fig.ure 8(a) shows an example of a map with good image quality from the Census TIGER/Line source... Since we retrieved the raster map directly from the TIGER/Line website without compressing the image and the map is generated digitially from vector data without scanning, the map has consistent background colors and contains fewer colors overall compared to the scanned map shown in  8(b) and  8(c). For the raster map with good image quality such as the TIGER/Line map, we developed an automatic approach that exploits common map properties to extract the feature layers from the 
map.16, 18 
We utilize the fact that the map background generally has a dominant numbers of pixel compared with the foreground and the foreground has a high contrast against the background to ﬁrst remove the map background and then extract the foreground pixel. We then exploit the distinctive geometry between the road lines and character strokes (e.g., road lines are longer compared to character strokes, character strokes are near each other) to separate the road and character pixel from the foreground pixel. The automatic approach has the beneﬁt of  no prior knowledge of the map and no user input, but it cannot handle raster map with poor 
.. http://www.census.gov/geo/www/tiger/ 


 7. The overall approach to extract road and text layers from raster map 

(a) A TIGER/Line map (b) A USGS topographic map (c) A scanned Thomas-Guide map  8. Example raster map 
image quality. This is because the raster map with poor image quality, such as scanned map, usually contain numerous colors and the background does not have a consistent color usage, so the map properties assumed in our automatic approach do not hold for these map. 
To achieve broad coverage of various map sources and varying image quality, we developed a supervised approach, which requires user labeling to extract the road and text layers from raster map.17, 20 The su.pervised techniques can be used on scanned and com.pressed map that are otherwise di.cult to process automatically and tedious to process manually ( manually classify each pixel/color in the raster map based on the geographic feature the pixel/color repre.sents). One of the supervised techniques that we de.veloped is a pixel-based classiﬁcation technique that exploits the texture di.erences between areas around text pixel, line pixel, and background pixel to auto.matically classify each map pixel for extracting the feature layers.17 Within a local area, the textures of the foreground and the background are di.erent since the colors of the background are more consistent while the colors of the foreground change frequently. Among the foreground objects, lines and characters also have di.erent texture representations as shown in  9(a). This pixel-based supervised approach en.ables the extraction of road and text layers from poor quality map, but the training process requires the user to provide individual locations of roads, charac.ters, and background pixel in the raster map, which can be a laborious task if the image quality is very poor and requires many training samples. 
We also developed a color-based supervised map decomposition approach that analyzes user labels of road areas ( rectangular areas speciﬁed by the user) to identify the road colors used in a raster map for extracting the road layer.20 The approach that the color-based technique uses to minimize user input is to exploit the fact that a user label is required to be centered at a road line or a road intersection, as shown by the two user labels in  9(b). Since a user label is centered at a road line, if the pixel of a particular color in the user label constitute straight lines ( we detect Hough lines21 for identifying straight lines) and the lines are near the center of the user label, the pixel are road pixel and we classify the pixel color as a road color.  this approach, the user only has to provide enough user labels to cover each road color in the raster map. For example,  9(b) shows an example scanned map and the two user labels required ( one for the white roads and one for the yellow roads) to extract the road layer from the map. In the experiments on the color-based approach,20 we show that the average number of user labels to extract the road layer from 100 test map is less than four per map, which is signiﬁcantly less user input compared to providing pixel samples in the pixel-based approach. In order to avoid having a user label every map, we also developed a method of classifying map based on the spatial relationship between colors in the map, which allows the system to determine whether it can use the learned road colors from map on which the system had been previously trained.22 
4. RECOGNIZING ROAD INTERSECTIONS 
From the extracted road layer, we developed an automatic approach to extract a set of road-intersection tem.plates,16, 18, 19 which represents an abstraction of the road network in a raster map.  10 shows an example of a road-intersection template, which consists of the road intersection position, the road connectivity, and the road orientation. Since road networks commonly exist in various geospatial data, the set of road-intersection templates of a raster map can serve as a reference feature in a conﬂation system23, 24 to compute a transformation matrix for aligning the map with other geospatial layers, such as imagery. Further, the aligned road-intersection templates can be used as seed templates for extracting roads from imagery.3 



 11(a) and  11(b) show an example map and the map’s foreground layers ( the binary map).  11(c) shows the extracted road layer  our map decomposition approach described in the previous section. Some of the road lines in the extracted road layer are broken since a portion of the road pixel also belong to the text layer ( overlapping features), such as where characters touch road lines. These shared pixel are removed during the layer extraction processes when we separate the road and character pixel from the foreground pixel. To reconnect the broken road lines and produce the road topology ( the central-line representation of the road network) for extracting the road inter.sections, we ﬁrst identify the road width and road format ( double-line and single-line roads) of the road layer. We developed the Parallel-Pattern Tracing algorithm (PPT),16, 18 which employs two convolution masks working on the hor.izontal and vertical directions to search for corresponding pixel of parallel lines to determine the road width and road format automatically. In a road layer where road lines are drawn in two parallel lines ( double-line format) as shown in the example in  11(c), the road width is the pixel distance between corresponding road pixel on the parallel lines, as shown by the gray dashed lines in  11(d). If a road line is drawn  one line ( single-line format), the detected road width is the thickness of the majority of the road lines. The road width and road format help to determine the parameter settings in the next step, which uses morphological operations25 to produce the road topology. 
To produce the road topology, we ﬁrst thicken the road lines to reconnect broken lines by utilizing the binary dilation operator as shown in  11(e). During the thickening process  the dilation operator, we also merge parallel lines into thick single lines if the road layer is double-line format, and we determine the number of iterations of the dilation operator ( how far the foreground region should be expanded)  the road width. Then, we apply the binary erosion operator and the thinning operator to shrink the thickened road lines and generate one-pixel width lines as shown in  11(f) and  11(g). We use the erosion operator to shrink the lines before we apply the thinning operator so that the thinning operator does not have to be applied directly on the thick lines. This is because the thinning operator distorts lines near the intersections and the extent of the distortion depends on the thickness of the lines before the thinning operator is applied. 
With the one-pixel-width road lines, we utilize the corner detector26 to detect salient points as candidates for road intersections as shown with the cross marks in  11(h). A salient point is a point where two lines meet at di.erent angles and a road intersection is a point where more than two lines meet at di.erent angles, so we can compute the connectivity of each salient point to identify actual road intersections. We draw a box around each salient point and then use the number of foreground pixel that intersect with this rectangle as the connectivity of the salient point. If the connectivity is less than three, we discard the point; otherwise, it is identiﬁed as a road intersection. 
From an identiﬁed road intersection, we trace the road lines connected at the intersection to generate a road-intersection template.19 Although the binary erosion operator helps to minimize the extent of the distortion caused by the thinning operator, the road topology near the intersections is still not accurate, especially near T-shape intersections.  11(i) shows a distorted example of the road topology around a T-shape intersection and  11(j) shows an inaccurate road-intersection template if the distorted lines are traced to generate the result. We use the road width detected  PPT to mark potential distortion areas as shown by the gray boxes in  11(k). This is because the extent of the distortion is determined by the thickness of the thickened lines, which is determined by the number of iterations of the binary dilation operator based on the road width. We then trace the lines outside the gray boxes to generate accurate road orientations and update the positions of the road intersections based on the intersecting roads and their orientations. In the experiments on this work,19 we show that the geometry and positions of our extracted road-intersection templates are very close to the ground truth ( manually drawn road-intersection templates).  11(l) shows a portion of the extraction results for  11(a). With the accurate results, conﬂation applications that use the road-intersection templates as a 



(a) An example raster map (b) The binary map (c) The extracted road layer (d) The road width 

(e) 
Thickened road lines (f) Eroded road lines (g) Thinned road lines (h) Salient points 

(k)Markedintersectionsandtracedroadlines
(l)Accurateextractionre-sults(redlines)


(i) 
Distorted road lines (j) A distorted extraction 






result (blue lines)  11. Automatic extraction of road-intersection templates 
reference feature can reduce their search space during the matching processes by  the road orientation and connectivity as features to select possible matches. Moreover, applications that work on extracting roads from imagery also beneﬁt from the accurate road-intersection templates since the application can start with accurate seed templates for identifying road areas. 
5. DETERMINING THE GEOCOORDINATES OF A MAP 
Once we have identiﬁed a set of road intersections from a map, we can in turn compare those intersections with existing georeferenced road vector data to determine the coordinates of the map. We can obtain road vector data with known coordinates for various regions of the world. For example, the US Census TIGER/Line ﬁles†† contain road networks for the United States and NAVTEQ‡‡ NAVSTREETS covers the entire US as well as many other regions worldwide. A prerequisite step is processing those vector datasets to produce a road intersection database in order to compare the intersections from the map. We have built such a database covering the entire US, where we found 11.4 million road intersections. 
After detecting a set of road intersection points from each dataset ( the map and the existing road vector data) separately, the remaining problem is how to match these intersection points e.ectively and e.ciently to locate a common distribution (or pattern) in these intersections. Intuitively, our system uses the layout of the 
††http://www.census.gov/geo/www/tiger/ 
‡‡http://www.navteq.com/ 

 12. The overall approach to determine the coordinates of a map 
detected intersections from each dataset to compute their geospatial relationship.  12 shows our overall approach.  detected road intersections as input, our system locates the common point pattern across these two point sets by computing a proper transformation T between them. The system can then utilize this transformation to determine the coordinates of the map. 
The transformation T is a 2D rigid motion (rotation and translation) with scaling. Because the majority of map are oriented such that north is up, we only compute the translation transformation with scaling. A brute-force method to resolve the point pattern matching problem would be to generate all possible matching point pairs from both point sets to obtain the translation and scaling factors. Each transformation thus generated could then be applied to the entire set of points in a map to determine whether the majority of the points can be aligned to another point set. Since this algorithm would be very time consuming, we developed a number of techniques to improve its performance by exploiting auxiliary information, such as the map scale, the connectivity of the intersections ( the number of intersected road segments), and the density of these intersections. The basic idea is to exclude all unlikely matching point pairs. For example, given a point pair (x1, y1) and (x2, y2) from the map, we need to only consider pairs (lon1, lat1) and (lon2, lat2) from the road vector, such that the real world distance between (x1, y1) and (x2, y2) is close to the real world distance between (lon1, lat1) and (lon2, lat2). In addition, (x1, y1) and (lon1, lat1) would be considered as a possible matching point if and only if they have similar road connectivity and orientation. We named this approach GeoPPM and  13 illustrates how GeoPPM works  an example. This enhanced algorithm improves the execution time by at least two orders of magnitude. In our previous papers24, 27 we provide additional details on these techniques. 
6. ALIGNING A MAP WITH IMAGERY 
After GeoPPM generates a set of matched point pairs (called “control point pairs”) for the map and road vector data, we can deform the map to align it to the road vector data utilizing these identiﬁed control point pairs. Furthermore, in our previous work,23 we developed a technique to e.ciently and automatically align road vector data with orthorectiﬁed imagery. This implies that  the road vector as glue, we can align a map with the corresponding imagery. We ﬁrst align a road vector data with an image, and then we match map intersections with the road vector data. Finally, we apply a technique, called rubber-sheeting, to align the map and imagery based on the matched control-point pairs. Intuitively, imagine stretching a map as if it was made of rubber. The rubber-sheeting algorithm deforms a map algorithmically, forcing registration of control points over the map with their corresponding points on the imagery to accomplish the overall alignment. Rubber-sheeting is a commonly-used method for aligning various geospatial datasets.9  14 illustrates how the rubber-sheeting technique partitions both datasets into triangular regions to deﬁne the localized transformations based on the 

(a) 
A georeferenced road vector 	(b) An selected intersection from (c) A selected point pair from the 

dataset with a selected intersection 	the vector data with connectivity vector data including the angle be-and orientation information tween the selected points 

(d)
Thecandidatepointsonthemapforthepointselectedin(a)ifthereisnoﬁlteringtoprunethesearchspace






 13. Comparing a road vector dataset (a-c) with a map (d-f)  GeoPPM to prune the search space 

 14. Partition each dataset into corresponding triangular regions based on the control point pairs (black points on the image and corresponding white points on the map) 
control point pairs.  15 illustrates the map-imagery align.ment result after replacing the image pixel semi-transparently with the corresponding pixel on the map by  the computed transformation coe.cients. 
7. NEXT STEPS: RECOGNIZING THE ROAD AND TEXT LAYERS 
Now that we can extract the map layers and determine the precise geocoordinates of a map, our next step is to recognize the features of the extracted layers. In particular, we want to build accurate road layers by turning the extracted road layer into vector data. We also plan to work on the problem of performing optical charac.ter recognition on the text layers. This problem can be especially challenging on map with a lot of text close to the road lines and text that follows the directions of the roads. Subsequently, given the resulting features, we plan to work on the problem of how to associate the text labels with the road vectors. In this section, we describe our initial work on road and text recognition. In Section 3, we already described how we can extract the road and text layers from a map. However, the output of that step is the corresponding raster layers for each feature. The next step is to turn the road layer into vector data and the text layer into text. This builds on the previous steps for extracting the raster layer for the roads and text, extracting accurate intersection points, and aligning the road and text layers with the imagery. 


There are two general challenges in turing a road layer from a map into an accurate vector layer. First, the location of the vectors may not be accurate because of inaccuracies or lack of meta data about the original map. This problem is addressed by our approach to automatically registering a map with an image as described in Section 5. The output of the registration process is a set of control-point pairs that were used in Section 6 to align a map with an image. The same set of control point pairs can be applied to the extracted road-vector data to conﬂate the road vector data with the orthorectiﬁed aerial imagery, which will ensure that the vector data will be properly aligned for a given region. The second challenge is that the road layer extraction process may introduce additional inaccuracies through the use of the morphological operations needed to ﬁll in missing pixel and construct the single-line road network. To address this problem, we use the method for accurately recognizing the road intersections as the starting point for the extraction of the road vector data. As described in Section 4, we are able to accurately extract both the location and angles of the roads that comprise an intersection. We use these extracted intersections as the seed points to trace the road lines and extract the vector data from the road layer.28 
Recognizing the text layers is even more challenging. Some of the issues are that there may be a variety of fonts and font sizes, the characters may be broken where they intersect with the roads, the road names may have a variety of orientations, and on some map the names may even follow the curvature of the roads. We plan to start with commercial optical character recognition (OCR) software and apply it to the text of the map since these systems have a lot of knowledge and training to handle various fonts and character sets. However, in order to use these systems we have to address the problem of orienting the text so that it is displayed horizontally from left to right. One way to do that is to use the fact that the road names typically follow the orientations of the roads to determine the correct orientation for the OCR system. We can also use the fact that characters are grouped into words and we can group the characters to ﬁnd the most likely orientation. Another problem is that given a noisy text layer, the OCR system may make recognition errors on the individual characters. To address this problem we plan to use background knowledge for a given region that provides the names of roads and other features for the area. Thus, instead of just matching individual characters, OCR can be applied in the context of entire words such that feature names in the knowledge base are preferred over unknown feature names. 
8. RELATED WORK 
In our previous work on harvesting map from the internet,15 we used Laws’ Texture29 as the representative feature set to di.erentiate between map and non-map, with Support Vector Machines30 as the classiﬁer. In this work we use Water-ﬁlling features with a CBIR-based K-NN classiﬁer. As demonstrated by our results, the CBIR approach with Water-ﬁlling features surpasses the previous performance by about 18% in F1-measure and yields a substantial processing improvement in both time and memory requirements. A lot of research has been done in the area of ﬁnding the right features for comparing images. While we use Water-ﬁlling features as they accurately capture the key features of a map, other studies have proposed “salient point” features based on wavelets31 and shape similarity.32 
The CBIR-based, K-Nearest Neighbor approach has been previously used to classify medical images.33 That work also includes Water-ﬁlling features for the CBIR component. However, this combination performs the worst among the ﬁve di.erent classiﬁcation systems they tested. This is because Water-ﬁlling features work best on images with sharp boundaries and no color gradient and medical images like X-rays have amorphous boundaries and gradual color gradients. On the other hand, map satisfy the requirement of the Water-ﬁlling algorithm precisely and we have exploited it to our beneﬁt. In addition, the context in which they apply these algorithms is very di.erent from ours. Our system is geared towards automatically harvesting map from the Web, while their system is used to classify images so that they can be queried categorically. 
In the previous work on text/graphics separation from raster map, Cao and Tan7 and Li et al.8 utilize a preset grayscale threshold to remove the background pixel from raster map and then detect text labels from the remaining foreground layers of the map. The road pixel are the by-product after the text pixel are identiﬁed. Since in their work7, 8 the main goal is to recognize the text labels, they do not perform further processing to extract the road topology of the raster map. For the previous work that focuses on recognizing road features (e.g., road lines and intersections) from the raster map, that work assumes a simpler type of raster map for their automatic processing. Habib et al.1 extract road intersections from raster map that contain road lines only. Itonaga et al.2 employ a stochastic relaxation approach to ﬁrst extract the road areas and then apply the thinning operator to extract a one pixel-width road network from digitally generated map ( not scanned map). These approaches each handle a speciﬁc type of map. In comparison, the approach presented in this paper is not limited to a speciﬁc map type and can be used on heterogeneous raster map, including scanned map and map that contain overlapping feature layers. 
Other map processing techniques require user training to process raster map with low image quality. Salva.tore and Guitton4 use a color thresholding method as their ﬁrst step to extract contour lines from topographic map. Khotanzad and Zink5 utilize a color segmentation method with user annotations to extract the contour lines from the USGS topographic map. Chen et al.6 extended the color segmentation method in Khotanzad and Zink’s work5 to handle common topographic map ( not limited to the USGS topographic map)  local segmentation techniques. The techniques with user training are generally able to handle map that are more complex. However, the user training processes are complicated and labor intensive, such as manually gen.erating color thresholds for every input map4 and labeling all combinations of line and background colors.5 In comparison, our supervised color-based map decomposition approach requires the user to label only a few road areas, which is simpler and more straightforward. 
There have been a number of e.orts to automatically or semi-automatically detect matched features across di.erent GIS datasets.9, 34–36 Given a feature point from one dataset, these approaches utilize di.erent matching strategies to discover the corresponding point on another dataset within a predetermined distance. Typically, these existing algorithms only handle the matching of geospatial datasets in a known geometry systems (e.g., the same coordinate system). Moreover, various commercial GIS systems, such as ESEA MapMerger, have been implemented to achieve the matching of vector datasets with di.erent accuracies. However, most of the existing commercial systems require manual work to transform two GIS datasets into the same geocoordinates beforehand. There are no other automatic methods to resolve the matching and alignment of a map of unknown coordinates with other datasets ( road vector data or imagery). Furthermore, our technique infers the coordinates and scale of the map. 
9. CONCLUSION 
In this paper we described a general approach to discovering, registering, and extracting features from raster map. The contributions of this work include the ability to identify map from other types of images, the ability to extract road and text layers from a raster map, the automatic recognition of road intersection, including the accurate extraction of the cardinality and angles of the roads, the algorithms to automatically determine the geocoordinates of a map  background data on the road network, and the techniques for aligning a map with aerial or satellite imagery. A key part of this contribution are techniques that are not speciﬁc to a given map type and can be applied to a wide range of raster map. The result of the combination of all of these techniques means that we can build systems that can automatically discover map for a region, determine their precise coverage and scale, accurately overlay the map on top of imagery, and build a database of the road and text layers that can be used for linking other types of information or even building new map. Overall, these techniques provide the ability to exploit the tremendous amount of information contained in raster map that was previously unavailable. 
Beyond the current work on recognizing the road and text features in a map and associating the two, there are a number of interesting directions for future research. We would like to remove or reduce some of the assumptions identiﬁed in the beginning of the paper in terms of what types of map can be processed. First, we would like to be able to handle abstract road map, which are map that do not contain fully detailed road networks and map where the orientation is not known. Second, we would like to eliminate the need to know the general area of a map in order to register a map. Once we can perform the OCR on the text layer, we can then use the recognized text to focus on the most likely area or areas of a map, which will make it possible to process a map without any knowledge of its general location. Third, we would like to remove the requirement that we have a vector dataset of an area in order to align a map with that location. Once we are able to extract a road vector layer from a map, we can then use our previous work on aligning road vector data with imagery23 to create a vector layer for areas where no such vector layer is available. Then other map of that region can be registered and aligned  this new vector dataset. 
ACKNOWLEDGMENTS 
This research is based upon work supported in part by the United States Air Force, Air Force Research Labora.tory, O.ce of Scientiﬁc Research under contract number FA9550-08-C-0010 and grant number FA9550-07-1-0416, and in part by a gift from Microsoft. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the o.cial policies or endorsements, either expressed or implied, of any of the above organizations or any person connected with them. 
REFERENCES 
1. 
A. Habib, R. Uebbing, and A. Asmamaw, “Automatic extraction of road intersections from raster map,” tech. rep., Center for Mapping, 1999. 

2. 
W. Itonaga, I. Matsuda, N. Yoneyama, and S. Ito, “Automatic extraction of road networks from map images,” Electronics and Communications in Japan (Part II: Electronics) 86(4), pp. 62–72, 2003. 

3. 
G. Koutaki and K. Uchimura, “Automatic road extraction based on cross detection in suburb,” in Proceed.ings of the SPIE, Computational Imaging II 5299, pp. 337–344, 2004. 

4. 
S. Salvatore and P. Guitton, “Contour line recognition from scanned topographic map,” in Proceedings of the Winter School of Computer Graphics, 2004. 

5. 
A. Khotanzad and E. Zink, “Contour line and geographic feature extraction from USGS color topographical paper map,” IEEE Transactions on Pattern Analysis and Machine Intelligence 25(1), pp. 18–31, 2003. 

6. 
Y. Chen, R. Wang, and J. Qian, “Extracting contour lines from common-conditioned topographic map,” IEEE Transactions on Geoscience and Remote Sensing 44(4), pp. 1048–1057, 2006. 

7. 
R. Cao and C. L. Tan, “Text/graphics separation in map,” in 	Proceedings of the Fourth International Workshop on Graphics Recognition Algorithms and Applications, pp. 167–177, 2002. 

8. 
L. Li, G. Nagy, A. Samal, S. C. Seth, and Y. Xu, “Integrated text and line-art extraction from a topographic map,” International Journal of Document Analysis and Recognition 2(4), pp. 177–185, 2000. 

9. 
A. J. Saalfeld, Conﬂation: automated map compilation. PhD thesis, University of Maryland, College Park, MD, USA, 1993. 

10. 
G. K. Myers, P. G. Mulgaonkar, C.-H. Chen, J. L. DeCurtins, and E. Chen, “Veriﬁcation-based approach for automated text and feature extraction from raster-scanned map,” in Lecture Notes in Computer Science, 1072, pp. 190–203, Springer, 1996. 

11. 
A. W. M. Smeulders, M. Worring, S. Santini, A. Gupta, and R. Jain, “Content-based image retrieval at the end of the early years,” IEEE Transactions on Pattern Analysis and Machine Intelligence 22(12), pp. 1349–1380, 2000. 

12. 
X. Zhou, Y. Rui, and T. Huang, “Water-ﬁlling: A novel way for image structural feature extraction,” in Proceedings of the International Conference on Image Processing, pp. 570–574, 1999. 

13. 
M. Michelson, A. Goel, and C. A. Knoblock, “Identifying map on the World Wide Web,” in Proceedings of the 5th International Conference on Geographic Information Science, LNCS 5266, pp. 249–260, Springer, (New York), 2008. 

14. 
J. Canny, 	“A computational approach to edge detection,” IEEE Trans. Pattern Analysis and Machine Intelligence 8, pp. 679–714, 1986. 

15. 
S. Desai, C. A. Knoblock, Y.-Y. Chiang, K. Desai, and C.-C. Chen, “Automatically identifying and geo.referencing street map on the Web,” in Proceedings of the 2nd International Workshop on Geographic Information Retrieval, pp. 35–38, 2005. 

16. 
Y.-Y. Chiang, C. A. Knoblock, and C.-C. Chen, “Automatic extraction of road intersections from raster map,” in Proceedings of the 13th ACM International Symposium on Advances in Geographic Information Systems, pp. 267–276, 2005. 

17. 
Y.-Y. Chiang and C. A. Knoblock, “Classiﬁcation of line and character pixel on raster map  discrete cosine transformation coe.cients and support vector machines,” in Proceedings of the 18th International Conference on Pattern Recognition, pp. 1034–1037, 2006. 

18. 
Y.-Y. Chiang, C. A. Knoblock, C. Shahabi, and C.-C. Chen, “Automatic and accurate extraction of road intersections from raster map,” GeoInformatica 13(2), pp. 121–157, 2008. 

19. 
Y.-Y. Chiang and C. A. Knoblock, “Automatic extraction of road intersection position, connectivity, and orientations from raster map,” in Proceedings of the 16th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, pp. 1–10, 2008. 

20. 
Y.-Y. Chiang and C. A. Knoblock, “A method for automatically extracting road layers from raster map,” in Proceedings of the Tenth International Conference on Document Analysis and Recognition, 2009. 

21. 
D. A. Forsyth and J. Ponce, Computer Vision: A Modern Approach, Prentice Hall Professional Technical Reference, 2002. 

22. 
Y.-Y. Chiang and C. A. Knoblock, “Classiﬁcation of raster map for automatic feature extraction,” in Pro.ceedings of the 17th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, 2009. 

23. 
C.-C. Chen, C. A. Knoblock, and C. Shahabi, “Automatically conﬂating road vector data with orthoim.agery,” GeoInformatica 10(4), pp. 495–530, 2006. 

24. 
C.-C. Chen, C. A. Knoblock, and C. Shahabi, “Automatically and accurately conﬂating raster map with orthoimagery,” GeoInformatica 12(3), pp. 377–410, 2008. 

25. 
W. K. Pratt, Digital Image Processing: PIKS Scientiﬁc Inside, Wiley-Interscience, 3rd ed., 2001. 


26. 
J. Shi and C. Tomasi, “Good features to track,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 1994. 

27. 
C.-C. Chen, C. A. Knoblock, C. Shahabi, S. Thakkar, and Y.-Y. Chiang, “Automatically and accurately conﬂating orthoimagery and street map,” in Proceedings of the 12th ACM International Symposium on Advances in Geographic Information Systems (ACM-GIS’04), 2004. 

28. 
Y.-Y. Chiang and C. A. Knoblock, “An approach to automatic road vectorization of raster map,” in Proceedings of the 8th IAPR International Workshop on Graphics RECognition (GREC’09), 2009. 

29. 
K. Laws, Textured Image Segmentation. PhD thesis, University of Southern California, 1980. 

30. 
V. Vapnik, The Nature of Statistical Learning Theory, Springer-Verlag, 1995. 

31. 
Q. Tian, N. Sebe, M. Lew, E. Loupias, and T. Huang, “Image retrieval  wavelet-based salient points,” Journal of Electronic Imaging 10(4), pp. 835–849, 2001. 

32. 
L. Latecki and R. Lakamper, “Shape similarity measure based on correspondence of visual parts,” IEEE Trans. Pattern Analysis and Machine Intelligence 22(10), pp. 1185–1190, 2000. 

33. 
T. Lehmann, M. Guld, T. Deselaers, D. Keysers, H. Schubert, K. Spitzer, H. Ney, and B. Wein, “Auto.matic categorization of medical images for content-based retrieval and data mining,” Computerized Medical Imaging and Graphics 29, pp. 143–155, 2005. 

34. 
P. 	Dare and I. Dowman, “A new approach to automatic feature based registration of SAR and SPOT images,” International Archives of Photogrammetry and Remote Sensing 33(B2), pp. 125–130, 2000. 

35. 
G. Seedahmed and L. Martucci, “Automated image registration  geometrically invariant parameter space clustering (GIPSC),” International Archives of Photogrammetry, Remote Sensing and Spatial Infor.mation Sciences 34(3A), pp. 318–323, 2002. 

36. 
V. Walter and D. Fritsch, “Matching spatial data sets: 	A statistical approach,” International Journal of Geographic Information Sciences 13(5), pp. 445–473, 1999. 


GEODEC: ENABLING GEOSPATIAL DECISION MAKING 
Cyrus Shahabi, Yao-Yi Chiang, Kelvin Chung, Kai-Chen Huang, Jeff Khoshgozaran-Haghighi, Craig 
Knoblock, Sung Chun Lee, Ulrich Neumann, Ram Nevatia, Arjun Rihan, Snehal Thakkar, Suya You 

Integrated Media Systems Center – Department of Computer Science, 
University of Southern California, Los Angeles, CA 90089-2561 

ABSTRACT 

The rapid increase in the availability of geospatial data has motivated the effort to seamlessly integrate this information into an information-rich and realistic 3D environment. However, heterogeneous data sources with varying degrees of consistency and accuracy pose a challenge to such efforts. We describe the Geospatial Decision Making (GeoDec) system, which accurately integrates satellite imagery, three-dimensional models, textures and video streams, road data, map, point data and temporal data. The system also includes a glove-based user interface. 
1. INTRODUCTION 

Recently, there has been a rapid increase in the availability of geospatial data, and this growth has motivated the construction of an information-rich and realistic three-dimensional (3D) visualization of a geographical location (e.g., a city). 
The main challenge in such a task is how to accurately integrate and visualize all aspects of a geographic region, given the existence of heterogeneous data sources that have varying degrees of accuracy and consistency. Moreover, the user should be able to query the system and get information about the location that can then facilitate decision-making. Examples of information that could be used by decision-makers are: high-resolution imagery, map, road networks, and 3D models of buildings. Such a system has applications in several domains, including urban planning, emergency response, online real-estate, simulation and training, computer games and army intelligence. 
In this paper, we propose the Geospatial Decision Making (GeoDec) system, which is the result of applying relevant techniques developed independently in the fields of databases, artificial intelligence, computer graphics and computer vision to the problem described above. We present a general architecture for such a system and describe our current implementation. The current system not only allows navigation through a 3D model, but also issues queries and retrieves information as the user navigates through the area. In particular, the system seamlessly integrates satellite imagery, accurate 3D models, textures and video streams, road vector data, map, point data, and temporal data for a specific geographic location. In addition, users can intuitively interact with the system  a glove-based interface. In the following sections, we discuss the specific application of this technology to the reconstruction of the University of Southern California campus located in Los Angeles, California. 
2. RELATED WORK 
Our proposed system has geospatial data integration, image/video pre-processing and visualization components. In the area of data integration, there has been some prior work on utilizing data integration systems for integrating geospatial sources [1][2][3]. While the other systems can retrieve different types of geospatial data from sources, they cannot integrate the data  different methods, such as integrating map with imagery by conflating the map to the imagery. Moreover, our integration system supports easy addition of new sources and more flexible integration. 
In the area of image/video pre-processing in addition to providing a local map and possibly a satellite image of an area, the Google Earth project now offers 3D models of the area as well. However, unlike GeoDec, these models are typically based on LIDAR data. With LIDAR data, 3D building models are generated from high-resolution scanned range data [4][5]. The generated 3D building models are a mesh structure and do not make the structure explicit and need to be compensated for some holes in the generated models. The LIDAR data approach also requires accurate range data that may be difficult to obtain. 
In the area of visualization, the main capability that differentiates our system from Google Earth is supporting fusion of video textures with the 3D model to create a compelling and realistic visualization of a geographic location. 
3. GEODEC COMPONENTS 

 1: GeoDec System Architecture 
As depicted in  1, GeoDec system is comprised of three main components: an information mediator with a spatio-temporal database, an image and video pre-processing component, as well as a 3D visualization component that supports queries through the user interface. Our current system does not yet fully implement this architecture but nonetheless is an integrated end-to-end system comprising most parts of each major component. 

Copyright 2006 IEEE. Published in the 2006 International Conference on Multimedia and Expo (ICME 2006), scheduled for July 9-12, 2006 in Toronto, Ontario, Canada. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE. Contact: Manager, Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA. Telephone: + Intl. 908-562-3966.



The first step in our current system is to construct 3D models of buildings from (two) aerial photographs (not necessarily stereo images). In Section 3.1, we describe our system that facilitates the construction of the USC campus model rapidly and accurately with minimum human interaction. 
The second step is to enhance the model by integrating different geospatial data from both online sources as well as private databases. This data includes road vector data and map, gazetteer points, as well as temporal data, which are described in Sections 3.2.1, 3.2.2 and 3.2.3, respectively. In our proposed architecture, the system would query all the different geospatial data from a single mediator that integrates data from various sources. Our current system can obtain gazetteer point data as well as conflated vector and map data from the mediator 
The third step is to complete the 3D visualization by mapping textures to the buildings, which includes f video streams (either live or stored) of the area with the models. Section 3.3.1 discusses the details of our approach. Moreover, since intuitive interaction is an integral part of decision making, we have also implemented a glove-based user interface described in Section 
3.3.2 to enhance the user experience. 
3.1. Rapid Construction of 3D Models 
We have developed an interactive building modeling system that helps construct 3D building models from two or more aerial or satellite images. The system is designed to reduce user interactions and fatigue [6]. 
Even though the system uses two or more images, the images need not be true stereo images (i.e. they need not be taken at the same time) nor do they need to be rectified (i.e. have parallel epipolar lines); however, precise camera geometry is assumed to be given. The user interacts with only one image at a time and is not asked to fuse stereo images, reducing fatigue considerably. 
The method consists of first approximating a complex building by a rectangular seed building. After each click, indicating the approximate position of a corner, the system finds nearby corners constructed from extracted line segments. This information is sufficient to construct a 3D model by combining information from the multiple views automatically. The user can then adjust the model height or sides  efficient editing tools. 

 2: 3D Model Construction 
Complex roof shapes can be modeled by adding or removing rectangular or triangular components. These modifications do not typically require all three corners to be indicated; instead one side and one corner suffice. Multi-layered buildings are modeled from the bottom up so that new structures are extruded to the previous layer rather than the ground. By combining these operations, highly complex polygonal roof shaped buildings can be modeled. 
Our modeling system is integrated in the ERDAS Imagine GIS software. Thus, it can handle a wide variety of image formats including commercial satellite images with accompanying RPC camera models. A modeling result with texture from our system for the USC campus is shown in  2. In this case, 256 building components are generated in about two hours (user interaction time excluding image pre-processing time). 
3.2. Geospatial Data Integration 
3.2.1. Road network and map fusion 
In order to generate a useful visualization of integrated geospatial data (i.e. vector data, satellite imagery, and raster map), a simple superimposition of various geospatial sources is not sufficient to align the sources with each other for the following reasons: 
• 	
For many raster map, the geocoordinates of the map are unspecified 

• 	
For other geospatial data with specified geocoordinates, the projections and transformations used to produce the map, orthorectified imagery or vector data are unknown The current commercial tools to solve this problem require 


heavy user interventions. It is simply too slow and tedious to fully exploit the rich sources of information available in geospatial datasets. Towards this end, we have developed a set of techniques for automatically aligning map and road vector data on orthorectified imagery. In order to allow integration of the aligned data with the 3D model described in Section 3.1, the map and road vector data are aligned to one of the satellite images used to construct the 3D model. 
Our vector-to-imagery conflation technique exploits a combination of the knowledge of the road network with image processing techniques. We first find road intersection points from the road vector dataset. For each intersection point, we then perform image processing in a localized area to find the corresponding intersection point in the satellite image. Finally, we compute the transformation metrics from the two sets of intersection points to align the vector data with the imagery. The running time is dramatically lower than traditional image processing techniques due to the limited image processing. Furthermore, exploiting the road direction information improves both the accuracy and efficiency of detecting edges in the image. 
To integrate map with satellite imagery, we utilize common vector datasets as “glue” [7]. We first identify road intersections on imagery by the technique described above, and then we also detect the road intersections on map [8]. Finally, we apply geospatial point pattern matching algorithm to find matches between the two point sets. Now that we have a set of matched control point pairs, we partition the map into small triangles, and utilize the local transformation matrix to transform the map piece by piece. By doing so, we find the location (i.e. the geocoordinate) of the map and align the map with satellite imagery. 
Our proposed approach facilitates the close integration of vector datasets, imagery and map, thus allowing the creation of intelligent images that combine the visual appeal and accuracy of imagery with the detailed attribution information often contained in diverse map.  3 illustrates an area in the campus of University of Southern California where a campus tram route map   3: Conflated USC tram route map fused with 3D model 


is conflated with the aerial imagery and overlaid on the 3D model. 
3.2.2. Data Integration and Efficient Geospatial Querying 
In order to efficiently and accurately integrate a wide variety of information about a geographic location, GeoDec utilizes a geographic data integration system called Prometheus [9]. Prometheus organizes the available information sources in a domain hierarchy containing well-known domain concepts, such as, satellite image, map, or vector data. Moreover, Prometheus also models different types of integration operations and their effects. Examples of the integration operations include Overlay and Align. The Overlay operation may result in two information layers not aligning with each other while the Align operation described in Section 3.2.1 improves the alignment between two data layers. 
When GeoDec receives a request to retrieve the data for a geographic location, it can send a request to the data integration system to obtain different types of information (i.e. vector data, raster map, and satellite imagery). Prometheus determines the relevant sources for the requested data, retrieves relevant data from the sources, performs necessary alignment or integration operations, and returns the integration information to GeoDec which can then be displayed on the 3D model. 
3.2.3. Temporal Data Fusion 
One of the key aspects of GeoDec is to integrate temporal data with other available sources of information. The fusion of temporal data sources significantly enhances the ability of a user to visualize the dynamic nature of an environment over a given time period. In order to add this capability to the system, GeoDec tracks all the trams and buses that move in or around the USC campus in real-time. 
The geocoordinates of each tram are transmitted to a server every 10 seconds via modems installed on the trams. The system can thus track their location in real-time and displays the trams on the 3D model. In addition, the integration of conflated raster map of bus routes, road vector data and real time tram locations adds to the decision making capability of GeoDec. 
3.3. 3D Visualization 
3.3.1. Texture mapping and video fusion 
Textures enrich the visual complexity of a scene with elements that are not captured in the static 3D geometry of a model.  Textures capture both the static and dynamic appearance of the scene surfaces to make the 3D world appear realistic.  In particular, video textures provide spatio-temporal interpretation of real word activity, thereby aiding human comprehension and decision making.  Our method requires the building or use of existing models of scene structures.  Imagery is then projected onto these models and aligned or calibrated based on 3D and 2D landmarks and correspondences. 
Static textures are captured by still or video images. The acquisition method and texture mapping process employs a base texture buffer and image warping transformations. Dynamic textures are captured as live video streams. We dynamically fuse video textures from network cameras on a 3D model to reveal the spatial and temporal relationships between the video streams. 

 4: Video Stream Fusion with 3D Model 
The example shown in  4 depicts five real-time video images projected onto the building models of the USC campus. In this case, the video images are overlapped to seamlessly cover the entire street and provide a complete visualization of scene activity. The video fusion methods integrated in GeoDec are based on the Augmented Virtual Environment system [10]. The mappings between the model and the imagery are calibrated in advance,  point and line correspondences between multiple images and the 3D model to derive the unknown camera parameters. Projective projection techniques are based on graphics methods for shadows and lighting. Additional technical issues that must be resolved include video decoding, visibility and occlusion processing, multiple projection blending, and real-time rendering. Our solutions to these and other issues, as well as methods for leveraging the latest graphics hardware features for real-time visualization are described in [11]. 
3.3.2. Glove-based User Interface 
Gloves can provide a natural interface for various applications [12], and the GeoDec system includes a glove-based user interface for decision-makers to navigate and query this immersive and information-rich 3D environment  gestures. The goal of providing such an interface is to allow the user to intuitively interact with the integrated geospatial data on a large display. Examples of gesture-based navigation commands supported include zoom, translation and rotation. The user can also issue commands  the gloves to display different geospatial information integrated with the model such as the conflated vector data or tram map described earlier. Our system interprets user commands based on hand gestures obtained  a pair of 5DT Data Gloves and an Ascension Technology Flock of Birds Extended Range tracking device. 
4. CONCLUSION AND FUTURE WORK 
In this paper, we described a system that accurately integrates 

heterogeneous geospatial data sources to create a compelling, realistic and information-rich visualization of a geographic location. Specifically, our current system fuses satellite imagery, 3D models, textures and video streams, road data, raster map, point data, as well as temporal data. To achieve this, the system utilizes various techniques and further details on each technique, including the accuracy and performance evaluations, are described in the respective references. Our system also includes a glove-based interface to facilitate intuitive user interaction and decision making. 
In the future, we plan to continue to develop our system based on the three main architectural components described earlier. The integration of these components to fuse various modalities (e.g., geospatial data, images, video, 3D models) is what makes our system different from traditional information integration systems. However, it is part of our future work to perform a thorough evaluation of our end-to-end system and compare it with more traditional information integration systems. We plan to enhance the decision making aspects of our system through the integration of additional data sources as well as improved query capabilities  efficient spatio-temporal data structures. We are also working on a new middleware layer that offers a universal way of specifying the type of query as well as its parameters, and retrieves the result in a standard format. Therefore any visualization layer can sit on top of this universal query layer for its integrated query and access needs. In addition, we are also working on the formation of more sophisticated queries and visualization of the results in the user interface. Other potential enhancements include the modeling of more sophisticated building structures (e.g. sloping roofs), extraction of geospatial data from text documents, building image schematic representations of the dynamics and actions of objects within the GeoDec environment (e.g. cars in an intersection) and subsequently learning the object behavior, as well as exploring multimodal user interfaces such as those described in [13]. 
5. ACKNOWLEDGMENTS 

This research has been funded in part by NSF grants EEC-9529152 (IMSC ERC), IIS-0238560 (PECASE), IIS-0324955 (ITR), and unrestricted cash gifts from Google and Microsoft. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation. 
6. REFERENCES 

[1] A. Gupta, R. Marciano, I. Zaslavsky, and C. Baru, “Integrating gis and imagery through xml-based information mediation,” In Proceedings of NSF International Workshop on Integrated Spatial Databases: Digital Images and GIS, 1999. 
[2] S. Adali and R. Emery, “A uniform framework for integrating knowledge in heterogeneous knowledge systems,” In Proceedings of the Eleventh IEEE International Conference of Data Engineering, 1995. 
[3] M. Essid, O. Boucelma, F.-M. Colonna, and Y. Lassoued, “Query processing in a geographic mediation system,” In 12th ACM International Workshop on Geographic Information Systems(ACM-GIS 2004), pp. 101–108, 2004. 
[4] C. Vestri and F. Devernay, “Improving Correlation-based DEMs by Image Warping and Facade Correlation,” In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, pp. 438-443, 2000. 
[5] C. Fruch and A. Zakhor, “Constructing 3D City Models by Merging Ground-Based and Airborne Views,” Computer Graphics and Applications, pp. 52-61, 2003. 
[6] S. C. Lee, A. Huertas, and R. Nevatia, “Modeling 3-D Complex Buildings With User Assistance,” IEEE Workshop on Application of Computer Vision, pp. 170-177, 2000. 
[7] C-C. Chen, C. A. Knoblock, C. Shahabi, Y-Y. Chiang, S Thakkar, “Automatically and Accurately Conflating Orthoimagery and Street map,” The 12th ACM International Symposium on Advances in Geographic Information Systems (ACM-GIS'04), November 2004. 
[8] Y-Y. Chiang, C. A. Knoblock, C-C. Chen, “Automatic Extraction of Road Intersections From Raster map,” The 13th ACM International Symposium on Advances in Geographic Information Systems (ACM-GIS'05), November 2005. 
[9] S. Thakkar, J-L. Ambite and C. A. Knoblock, “A Data Integration Approach to Automatically Composing and Optimizing Web Services,” In Proceedings of 2004 ICAPS Workshop on Planning and Scheduling for Web and Grid Services, June 2004. 
[10] U. Neumann, S. You, J. Hu, B. Jiang, and I. O. Sebe, “Visualizing Reality in an Augmented Virtual Environment,” Presence:Teleoperators and Virtual Environments Journal, Vol. 13, No. 2, pp. 222-233, April 2004. 
[11] S. You, and U. Neumann, “V-Sentinel: a novel framework for situational awareness and surveillance,” International Conference on Sensors, Command, Control, Communications, and Intelligence (C3I) Technologies for Homeland Security and Homeland Defense III, SPIE Defense and Security Symposium, 2005. 
[12] D. J. Sturman and D. Zeltzer, “A survey of glove-based input,” IEEE Computer Graphics & Applications, pp. 30-39, January 1994. 
[13] N. Krahnstoever, S. Kettebekov, M. Yeasin and R. Sharma, “A Real-Time Framework for Natural Multimodal Interaction with Large Screen Displays,” In 4th IEEE International Conference on Multimodal Interfaces (ICMI 2002), 2002. 
31 


32 






A Survey of Digital Map Processing Techniques 
YAO-YI CHIANG, University of Southern California STEFAN LEYK, University of Colorado, Boulder CRAIG A. KNOBLOCK, University of Southern California 
map depict the natural and human-induced changes on earth at a fine resolution for large areas and over long periods of time. In addition, map – especially historical map – are often the only information source about the earth as surveyed  geodetic techniques. In order to preserve these unique documents, increasing numbers of digital map archives have been established driven by advances in software and hardware technologies. Since the early 80s, researchers from a variety of disciplines including computer science and geography have been working on computational methods for the extraction and recognition of geographic features from archived images of map (digital map processing). The typical result from map processing is geographic information that can be used in spatial and spatiotemporal analyses in a Geographic Information System environment, which benefits numerous research fields in the spatial, social, environmental, and health sciences. However, map-processing literature is spread across a broad range of disciplines in which map are included as a special type of image. This article presents an overview of existing map processing techniques with the goal of bringing together the past and current research efforts in this interdisciplinary field, to characterize the advances that have been made, and to identify future research directions and opportunities. 
Categories and Subject Descriptors: A.1 [Introduction and Survey]; H.2.8 [Database Management][Database Applications]: Spatial Databases and GIS 
General Terms: Design, Algorithms 
Additional Key Words and Phrases: map processing, geographic information system, image processing, pattern recognition, graphics recognition, color segmentation 
1. INTRODUCTION 
This article presents an overview of the techniques for “digital map processing” (or simply “map processing”), which refers to computational procedures aimed at the automatic or semi-automatic extraction and/or recognition of geographic features contained in images (usually scanned) of map. Digital map processing is a relatively young research field that grew out of image processing, document analysis, graphics recognition, and digital cartography. Over the past 40 years, researchers have become increasingly interested in the methodological aspects of computational map processing (mostly in scanned map) for the purposes of retrieval, extraction, and integration of geographic data (Freeman and Pieroni, 1982). This increasing attention results not only from the parallel advances in technologies (e.g., digital image analysis, recognition, and Geographic Information Systems (GIS)), but also can certainly be linked to the fact that computational map processing methods enable the preservation of unique (historical) map and the utilization of the contained geographic information in modern analytical environments (e.g., in a GIS). 
map can cover large areas over long periods of time for many regions in the world. This makes map unique documents witnessing places, human activities, and natural features in the past for which no or only limited alternative information sources exist. Processing map images to extract and recognize geographic information results in spatially referenced data ( map data) that can then be accessed, processed, and maintained in a GIS environment (in other words, “unlocking” geographic information from map documents). Incorporating map data into a GIS environment ( map data can be used for spatial analyses and overlaid with other spatial data) creates unprecedented opportunities for multi-temporal and multi-contextual spatial analyses such as analyzing the changes in built-up areas over large regions across long time periods and investigating how these changes interact with other geographic features such as vegetation or wetland areas. In addition, digital map processing can expedite the processes of comparing map contents from different map series/editions (e.g., contemporary map vs. historical map), to update current map series, and to create thematic map or new map series. 
The need for computational solutions with higher degrees of automation for map processing becomes evident if one considers that millions of map documents have already been scanned and stored in digital archives. For example, the U.S. Geological Survey (USGS) continues to scan and release all editions of more than 200,000 historic topographic map pages of the United States that cover the time period of 1884 – 2006. The GIS Center in Academia Sinica, Taiwan, has scanned and archived more than 160,000 historical map. Such digital archives can only be fully used after the archived map have been converted to a GIS usable format. However, manually processing these map not only generates non-reproducible data that can suffer from a high degree of inaccuracy and introduced subjectivity, but also does not scale well for handling large numbers of map. For example, manually digitizing one sounding label on a nautical chart includes two steps: drawing a minimum bounding box to label the sounding location and typing in the sounding value. If these two steps take a total of six seconds per sounding label, for a typical nautical chart that has more than 4,000 sounding labels, the digitization process takes more than six hours. In contrast,  digital map processing techniques can dramatically reduce the time (and cost) to fully utilize the geographic information locked in these map. For example, Chiang and Knoblock (2011) developed a map processing package, Strabo, which only requires one minute of the user’s time for operating the software to recognize 1,253 labels from an area in a nautical chart with 83% precision and 80% recall. For this particular area in the nautical chart, the amount of user time for verifying and correcting the recognition results from Strabo is under one hour, which is around 30% of the time that would be needed when performing the entire text recognition task manually. Without such computational solutions for map processing, large portions of the spatial data in map remain inaccessible unless they are manually converting to spatial datasets. 
General techniques for document analysis and graphics recognition (e.g., Cordella and Vento, 2000; Lladós et al., 2002) cannot be directly applied to map processing because map often pose particular difficulties for recognition due to various graphical quality issues and the complexity of the map contents. The graphical quality of scanned map can be affected by scanning or image compression processes. In addition, the stored and archived map materials suffer from aging and bleaching effects while original reproduction materials (e.g., copper plates) have often been destroyed or lost. The final scanned images inherit these graphical properties.  1 shows examples of two scanned historical map of inferior graphical quality due to the imperfections (including manually drawn features) of the original map. Furthermore, as seen in the sample map in  2, layers of geographic features, such as roads, contour lines, and labels, often overlap 

 1 Subsections of two examples of scanned (historical) map of inferior graphical quality due to aging and bleaching effects: USGS topographic map of Boulder, CO (1902) at a scale of 1:62,500 (left), and Swiss National Topographic map (Siegfried map), page 220, Brunnadern (1879) at a scale of 1:25,0001 
Various types of map have been produced manually until recently (e.g.,  copper plate or stone engraving techniques) resulting in high variations of symbol appearances and colors occurring on papers and in the final scans. Graphical guidelines for map are often map specific and rule-based but usually do not satisfy the quality requirements for machine drawings most often described and processed in document analysis and recognition work. This impedes processing map images within a general document recognition framework. For example, state-of.the-art commercial optical character recognition (OCR) tools can achieve high recognition rates in documents containing text lines of the same orientation, but recognizing map labels (text) in 
1 Reproduced by permission of swisstopo (BA13123) 
scanned map is still challenging (Nagy et al., 1997; Chiang and Knoblock, 2011). The main reason is that map labeling is based on specific underlying semantics and follows cartographic rules, which results in map labels of varying orientations that can appear curved and can often 

 2 An example map subsection that has multi-oriented labels: USGS topographic map of Jersey City, NJ (1967) at a scale of 1:24,0002 
While outcomes are of the same type, extraction and recognition techniques used in digital map processing are significantly different from the techniques used in remote sensing, document analysis, and other recognition applications. For example, road extraction from scanned map has to deal with fundamentally different input data ( manmade graphics) than road extraction from satellite imagery where photography-type or reflectance data are used. Accordingly, road extraction from scanned map aims at removing the noise from scanning, compression, and imperfection of the original material, accurately detecting and converting the pixel-based road information carefully depicted by cartographers into vector format (Bin and Cheong, 1998; Itonaga et al., 2003; Bucha et al., 2006; Chiang and Knoblock, 2013). In comparison, road extraction from satellite imagery identifies and extracts features from spectral reflectance data that could cartographically be represented as road areas and road centerlines in a GIS (Hickman et al., 1995; Steger et al., 1997; Hodgson et al., 2004). 
Due to the above described impediments and challenges in map processing, most approaches described in the literature are rather specific to a particular series of map and a lack of more generic solutions still persists. One significant consequence is that, to date, research on map processing shows slower rates of progress in developing general, automated, and robust solutions when compared to related fields in image processing and pattern recognition. Interestingly, the map processing literature appears to be abundant but highly dispersed across a range of fields such as image processing, document analysis and recognition, machine learning, data integration, and geoinformatics/digital cartography. While this is an interesting development, it also reflects an increasing need to bring together existing efforts to provide a thorough overview of what has been done in various fields and show how these efforts are related. Such an overview will help establish an objective outlook of the most promising research directions to increase the rate of progress in the field of digital map processing. As such, this survey synthesizes existing research in map processing and presents an overview that covers the diverse disciplines where this research has been published. Research on map processing greatly benefits from interdisciplinary approaches  the strengths of disciplines that naturally participate in this area,  computer science, geographic information science (GIScience), and cartography. Given the large body of research on map processing published over the last 40 years, we felt it would be most useful to focus on the current state-of-the-art instead of attempting to write a complete history of this field. As such, we 
2 Credit: U.S. Geological Survey 
have drawn on our combined 30 years of research experience on map processing to select and present the most important and promising techniques. 
The remainder of this survey is organized as follows. Section 2 outlines a brief history of map processing research and defines the scope of this survey. Section 3 describes the relevant basic techniques in document analysis and pattern recognition that support map processing. Section 4 reviews past and ongoing research on map processing for extracting and recognizing geographic information from scanned map. Section 5 concludes with a discussion and directions for future work. 
2. A BRIEF HISTORY OF MAP PROCESSING AND THE SCOPE OF THIS SURVEY 
Efforts of map processing or information extraction from map have been going on for four decades (Freeman and Pieroni, 1982) and show an increasing intensity as scanning technologies improved, storage capabilities increased, and processing speed increased. Research on map processing has been conducted on different types of map including cadastral or land register map (e.g., Boatto el al., 1992; Di Zenzo et al., 1996; Katona and Hudra, 1999; Raveaux et al., 2008), road map (e.g., Bin and Cheong, 1998; Itonaga et al., 2003; Dhar and Chanda, 2006; Bucha et al., 2007; Chiang et al., 2009; Chiang and Knoblock, 2013), hydrographic map (e.g., Trier et al., 1997), city map (e.g., Chen et al., 1999), utility map (e.g., den Hartog et al., 1996), as well as topographic or other survey map (e.g., Morse, 1969; Yamada et al., 1993; Yamamoto et al., 1993; Khotanzad and Zink, 1996; Frischknecht and Kanani, 1998; Arrighi and Soille, 1999; Ogier et al., 2001; Bessaid et al., 2003; Miyoshi et al., 2004; Chen et al., 2006; Leyk et al., 2006; Xin et al., 2006; Henderson et al., 2009). Mostly, the described systems were unable to process different types of map and so the focus was rather narrow (e.g., extraction of geographic feature layers from USGS Topographic map (Henderson et al., 2009)). Most studies focus on particular features, symbols, or map layers and thus efforts to extract map contents have been highly map specific, not applicable to a broader range of map products. 
One important question that becomes apparent is: why is the literature on map processing so dispersed over various scientific disciplines? This question can be answered by looking at the early years of map processing when this research direction was considered part of image or document analysis. At that time the main actors were computer scientists interested in technological aspects around digitization and information extraction in general; they developed specific hardware for manual map digitization (Leberl and Olson, 1982), and they considered map as just one kind of document (Cofer and Tou, 1972; Ejiri et al., 1984) without exploiting many of the cartographic principles to process map. As soon as the first GIS tools became more established and available as desktop solutions, the interest in digital map data and recognition tools increased considerably. GIS users, companies, and institutions quickly realized that map in analog (paper) format represented one important source for generating digital spatial information that could be accessed, stored, and managed in GIS environments. Naturally, the issue of recognition in map documents gained more interest in the scientific community for two main reasons. First, the large number of paper map stored in archives called for the need of higher degrees of automation since manual digitization was (and is) a highly labor-intensive, expensive process. Second, it was soon realized that such extracted spatial data would enable unprecedented GIS-based research efforts in a variety of disciplines such as landscape ecology (e.g., Kienast, 1993) or land-cover change analysis (e.g., Petit and Lambin, 2002; Kozak et al., 2007) as well as in studies on population dynamics and urbanization (e.g., Dietzel et al., 2005), potentially covering large areas and long periods of time. Thus the growing interest in map processing is driven by the increasing topical or substantive potential to use map data and also by the lack of general methods and techniques to overcome the technological challenges in map processing. 
Nowadays, map production in the Western world is mainly digital and thus map processing focuses primarily on historical map documents (historical in a sense of predating the switch to digital map production), which were originally published on paper. In addition, map processing also focuses on map documents of which the original reproduction materials do not exist or are difficult to access. Thus the motivation of map processing is twofold: on one side, the scanning, digitization, registration, and referencing of map represent an important way to preserve existing paper map in a digital format. On the other side, map processing aims at the generation of spatial data that can be used in a GIS for spatial analyses and can be combined with different spatial data 







